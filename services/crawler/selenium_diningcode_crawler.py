#!/usr/bin/env python3
"""
Selenium ê¸°ë°˜ Diningcode ë¬´í•œ ìŠ¤í¬ë¡¤ í¬ë¡¤ëŸ¬
ChromeDriver ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê°œì„ ëœ ë²„ì „
"""

import asyncio
import time
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
import httpx
from selectolax.parser import HTMLParser

from .base_crawler import BaseCrawler
from core.domain.restaurant import Restaurant
from core.domain.menu import Menu
from core.interfaces.crawler_interface import CrawlResult


class SeleniumDiningcodeCrawler(BaseCrawler):
    """Selenium ê¸°ë°˜ Diningcode í¬ë¡¤ëŸ¬ (ë¬´í•œ ìŠ¤í¬ë¡¤ ì§€ì›)"""
    
    def __init__(self, rate_limiter=None):
        super().__init__(rate_limiter)
        self.driver = None
        self.base_url = "https://www.diningcode.com"
        self.search_url = "https://www.diningcode.com/list.php"
    
    @property
    def source_name(self) -> str:
        return "diningcode"
    
    def _get_base_domain(self) -> str:
        return "diningcode.com"
    
    def get_search_url(self, keywords: List[str]) -> str:
        """ê²€ìƒ‰ URL ìƒì„±"""
        keyword = keywords[0] if keywords else "ë§›ì§‘"
        return f"{self.search_url}?query={keyword}&query_type=keyword"
    
    def is_restaurant_url(self, url: str) -> bool:
        """ë ˆìŠ¤í† ë‘ ìƒì„¸ í˜ì´ì§€ URLì¸ì§€ í™•ì¸"""
        return "profile.php?rid=" in url
    
    def extract_restaurant_info(self, html: str, url: str) -> Dict[str, Any]:
        """ë ˆìŠ¤í† ë‘ ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        from .diningcode_crawler import DiningcodeCrawler
        temp_crawler = DiningcodeCrawler()
        # ì‹¤ì œë¡œëŠ” íŒŒì‹±ë§Œ ìˆ˜í–‰í•˜ë¯€ë¡œ ì´ˆê¸°í™” ì—†ì´ ë©”ì„œë“œë§Œ í˜¸ì¶œ
        return temp_crawler._extract_restaurant_info(html, url)
    
    def extract_menu_list(self, html: str, url: str) -> List[Dict[str, Any]]:
        """ë©”ë‰´ ëª©ë¡ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        from .diningcode_crawler import DiningcodeCrawler
        temp_crawler = DiningcodeCrawler()
        # ì‹¤ì œë¡œëŠ” íŒŒì‹±ë§Œ ìˆ˜í–‰í•˜ë¯€ë¡œ ì´ˆê¸°í™” ì—†ì´ ë©”ì„œë“œë§Œ í˜¸ì¶œ
        return temp_crawler._extract_menu_list(html, url)
        
    async def initialize(self) -> None:
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        print("ğŸ”„ Selenium í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
        try:
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-webgl")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-images")
            chrome_options.add_argument("--disable-javascript")  # JS ë¹„í™œì„±í™”ë¡œ ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # ChromeDriver ìë™ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
            try:
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                print("âœ… ChromeDriver ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                print(f"âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # Edge ëŒ€ì•ˆ ì‹œë„
                try:
                    from webdriver_manager.microsoft import EdgeChromiumDriverManager
                    from selenium.webdriver.edge.options import Options as EdgeOptions
                    from selenium.webdriver.edge.service import Service as EdgeService
                    from selenium.webdriver.edge.webdriver import WebDriver as EdgeWebDriver
                    
                    edge_options = EdgeOptions()
                    edge_options.add_argument("--headless")
                    edge_options.add_argument("--no-sandbox")
                    edge_options.add_argument("--disable-dev-shm-usage")
                    edge_options.add_argument("--disable-gpu")
                    edge_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                    
                    edge_service = EdgeService(EdgeChromiumDriverManager().install())
                    self.driver = EdgeWebDriver(service=edge_service, options=edge_options)
                    print("âœ… EdgeDriver ì´ˆê¸°í™” ì„±ê³µ (Chrome ëŒ€ì•ˆ)")
                except Exception as edge_e:
                    print(f"âŒ EdgeDriver ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {edge_e}")
                    raise Exception("Chromeê³¼ Edge ëª¨ë‘ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # HTTP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë°±ì—…ìš©)
            self._client = httpx.AsyncClient(
                timeout=30.0,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )
            
            print("âœ… Selenium í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def crawl_restaurant_list(
        self, 
        keywords: List[str], 
        max_pages: int = 2,
        target_count: int = 100
    ) -> List[str]:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ì‹ë‹¹ ëª©ë¡ í¬ë¡¤ë§"""
        print(f"ğŸ”„ ë¬´í•œ ìŠ¤í¬ë¡¤ í¬ë¡¤ë§ ì‹œì‘: {keywords}, ëª©í‘œ {target_count}ê°œ")
        
        all_urls = []
        
        for keyword in keywords:
            print(f"\nğŸ” í‚¤ì›Œë“œ '{keyword}' ë¬´í•œ ìŠ¤í¬ë¡¤ ì¤‘...")
            
            try:
                # ê²€ìƒ‰ URL êµ¬ì„±
                search_params = {
                    'query': keyword,
                    'query_type': 'keyword'
                }
                
                search_url = f"{self.search_url}?" + "&".join([f"{k}={v}" for k, v in search_params.items()])
                print(f"   ğŸ“ ê²€ìƒ‰ URL: {search_url}")
                
                # Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ
                self.driver.get(search_url)
                time.sleep(3)  # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
                
                # ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜í–‰
                urls_for_keyword = await self._perform_infinite_scroll(search_url, target_count)
                print(f"   âœ… '{keyword}'ì—ì„œ {len(urls_for_keyword)}ê°œ URL ë°œê²¬")
                
                all_urls.extend(urls_for_keyword)
                
                # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
                if len(all_urls) >= target_count:
                    print(f"ğŸ‰ ëª©í‘œ {target_count}ê°œ ë‹¬ì„±! ì¤‘ë‹¨")
                    break
                    
            except Exception as e:
                print(f"   âŒ í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_urls = list(set(all_urls))
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"   ì´ ë°œê²¬ëœ URL: {len(all_urls)}ê°œ")
        print(f"   ì¤‘ë³µ ì œê±° í›„: {len(unique_urls)}ê°œ")
        
        return unique_urls[:target_count]

    async def _perform_infinite_scroll(self, url: str, target_count: int) -> List[str]:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜í–‰ ë° URL ì¶”ì¶œ"""
        print(f"   ğŸ”„ ë¬´í•œ ìŠ¤í¬ë¡¤ ì‹œì‘... (ëª©í‘œ: {target_count}ê°œ)")
        
        collected_urls = set()
        scroll_count = 0
        max_scrolls = 50  # ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì œí•œ
        
        try:
            while len(collected_urls) < target_count and scroll_count < max_scrolls:
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ì¶”ì¶œ
                current_urls = await self._extract_restaurant_urls_from_page()
                collected_urls.update(current_urls)
                
                print(f"   ğŸ“ ìŠ¤í¬ë¡¤ {scroll_count + 1}: {len(current_urls)}ê°œ ìƒˆ URL, ì´ {len(collected_urls)}ê°œ")
                
                # ë” ì´ìƒ ìƒˆ URLì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if len(current_urls) == 0:
                    print(f"   âš ï¸ ìƒˆ URL ì—†ìŒ, ìŠ¤í¬ë¡¤ ì¤‘ë‹¨")
                    break
                
                # ìŠ¤í¬ë¡¤ ë‹¤ìš´
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # ìŠ¤í¬ë¡¤ í›„ ë¡œë”© ëŒ€ê¸°
                scroll_count += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if scroll_count % 10 == 0:
                    print(f"   ğŸ“Š ì§„í–‰ë¥ : {scroll_count}ë²ˆ ìŠ¤í¬ë¡¤, {len(collected_urls)}ê°œ URL ìˆ˜ì§‘")
            
            print(f"   âœ… ë¬´í•œ ìŠ¤í¬ë¡¤ ì™„ë£Œ: {scroll_count}ë²ˆ ìŠ¤í¬ë¡¤, {len(collected_urls)}ê°œ URL")
            return list(collected_urls)
            
        except Exception as e:
            print(f"   âŒ ë¬´í•œ ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜: {e}")
            return list(collected_urls)

    async def _extract_restaurant_urls_from_page(self) -> List[str]:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ì‹ë‹¹ URL ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            
            # HTML íŒŒì‹±
            parser = HTMLParser(page_source)
            
            # ì‹ë‹¹ ë§í¬ ì°¾ê¸° (ë‹¤ì´ë‹ì½”ë“œ íŒ¨í„´)
            restaurant_urls = []
            
            # profile.php?rid= íŒ¨í„´ ì°¾ê¸°
            links = parser.css('a[href*="profile.php?rid="]')
            for link in links:
                href = link.attributes.get('href', '')
                if 'profile.php?rid=' in href:
                    full_url = urljoin(self.base_url, href)
                    restaurant_urls.append(full_url)
            
            # JavaScriptì—ì„œ rid ì¶”ì¶œ
            scripts = parser.css('script')
            for script in scripts:
                script_content = script.text()
                if script_content:
                    # rid= íŒ¨í„´ ì°¾ê¸°
                    rid_matches = re.findall(r'rid["\']?\s*[:=]\s*["\']?([a-zA-Z0-9]+)', script_content)
                    for rid in rid_matches:
                        if len(rid) > 5:  # ìœ íš¨í•œ rid ê¸¸ì´ ì²´í¬
                            url = f"{self.base_url}/profile.php?rid={rid}"
                            restaurant_urls.append(url)
            
            return list(set(restaurant_urls))  # ì¤‘ë³µ ì œê±°
            
        except Exception as e:
            print(f"   âŒ URL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    async def crawl_restaurant_detail(self, url: str) -> CrawlResult:
        """ì‹ë‹¹ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ê¸°ì¡´ httpx ë°©ì‹ ì‚¬ìš©)"""
        try:
            # httpx í´ë¼ì´ì–¸íŠ¸ë¡œ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
            response = await self._client.get(url)
            response.raise_for_status()
            
            # ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
            from .diningcode_crawler import DiningcodeCrawler
            temp_crawler = DiningcodeCrawler()
            await temp_crawler.initialize()
            
            # ìƒì„¸ ì •ë³´ íŒŒì‹±
            result = await temp_crawler._parse_restaurant_detail(response.text, url)
            
            return result
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
            return CrawlResult(success=False, error=str(e), data=None)

    async def close(self) -> None:
        """í¬ë¡¤ëŸ¬ ì •ë¦¬"""
        try:
            if self.driver:
                self.driver.quit()
                print("âœ… Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ")
            
            if hasattr(self, '_client') and self._client:
                await self._client.aclose()
                print("âœ… HTTP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ")
                
        except Exception as e:
            print(f"âš ï¸ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")