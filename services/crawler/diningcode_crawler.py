"""
ë‹¤ì´ë‹ì½”ë“œ í¬ë¡¤ëŸ¬ êµ¬í˜„
"""

import re
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from selectolax.parser import HTMLParser
import logging

from services.crawler.base_crawler import BaseCrawler

logger = logging.getLogger(__name__)

def clean_menu_name(name: str) -> str:
    """ë©”ë‰´ëª… ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    if not name:
        return name
        
    original_name = name
    
    # 1. "ë©”ë‰´ì •ë³´" ì ‘ë‘ì‚¬ ì œê±°
    name = name.replace("ë©”ë‰´ì •ë³´ ", "")
    
    # 2. "ì¶”ì²œ ìˆ«ì" íŒ¨í„´ ì œê±° (ì˜ˆ: "ì¶”ì²œ 1", "ì¶”ì²œ 2")
    name = re.sub(r'\s+ì¶”ì²œ\s+\d+$', '', name)
    
    # 3. ëì˜ ë‹¨ë… ìˆ«ì ì œê±° (ì˜ˆ: "ëˆìœ¡ì „ 1" -> "ëˆìœ¡ì „")
    # ë‹¨, ìš©ëŸ‰/ìˆ˜ëŸ‰ ì •ë³´ëŠ” ë³´ì¡´ (ì˜ˆ: "150g", "500ml")
    name = re.sub(r'\s+\d+$', '', name)
    
    # 4. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
    name = re.sub(r'\s+', ' ', name)
    
    # 5. ì•ë’¤ ê³µë°± ì œê±°
    name = name.strip()
    
    # 6. ë¹ˆ ë¬¸ìì—´ì´ ë˜ë©´ ì›ë³¸ ë°˜í™˜
    if not name:
        return original_name
        
    return name

class DiningcodeCrawler(BaseCrawler):
    """ë‹¤ì´ë‹ì½”ë“œ í¬ë¡¤ëŸ¬"""

    BASE_DOMAIN = "www.diningcode.com"
    BASE_URL = f"https://{BASE_DOMAIN}"

    @property
    def source_name(self) -> str:
        return "diningcode"

    def _get_base_domain(self) -> str:
        return self.BASE_DOMAIN

    def get_search_url(self, keyword: str, page: int = 1) -> str:
        """ê²€ìƒ‰ URL ìƒì„±"""
        # ë‹¤ì´ë‹ì½”ë“œ ê²€ìƒ‰ URL íŒ¨í„´
        return f"{self.BASE_URL}/list?keyword={keyword}&page={page}"

    def is_restaurant_url(self, url: str) -> bool:
        """ì‹ë‹¹ ìƒì„¸ í˜ì´ì§€ URL ì—¬ë¶€ í™•ì¸"""
        # ë‹¤ì´ë‹ì½”ë“œ ì‹ë‹¹ URL íŒ¨í„´: /profile.php?rid=ìˆ«ì
        return bool(re.search(r'/profile\.php\?rid=\d+', url))

    def _extract_restaurant_urls_from_search(self, html: str) -> List[str]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‹ë‹¹ URL ì¶”ì¶œ - JavaScript JSON ë°ì´í„°ì—ì„œ ì¶”ì¶œ"""
        urls = []
        
        try:
            # JavaScriptì˜ listDataì—ì„œ JSON ë°ì´í„° ì¶”ì¶œ
            import json
            
            # listData ë³€ìˆ˜ ì°¾ê¸°
            listdata_pattern = r"localStorage\.setItem\('listData',\s*'([^']+)'\)"
            listdata_match = re.search(listdata_pattern, html)
            
            if listdata_match:
                # JSON ë¬¸ìì—´ ë””ì½”ë”©
                json_str = listdata_match.group(1)
                # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ë””ì½”ë”©
                json_str = json_str.encode().decode('unicode_escape')
                
                try:
                    data = json.loads(json_str)
                    
                    # poi_section > listì—ì„œ ì‹ë‹¹ ì •ë³´ ì¶”ì¶œ
                    if 'poi_section' in data and 'list' in data['poi_section']:
                        restaurants = data['poi_section']['list']
                        
                        for restaurant in restaurants:
                            if 'v_rid' in restaurant:
                                # ì‹ë‹¹ URL ìƒì„±
                                restaurant_url = f"https://www.diningcode.com/profile.php?rid={restaurant['v_rid']}"
                                urls.append(restaurant_url)
                                
                        logger.info(f"JSON ë°ì´í„°ì—ì„œ {len(urls)}ê°œ ì‹ë‹¹ URL ì¶”ì¶œ ì™„ë£Œ")
                        
                except json.JSONDecodeError as e:
                    logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            
            # JSON ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            if not urls:
                logger.warning("JSON ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨, HTML íŒŒì‹±ìœ¼ë¡œ ì‹œë„")
                tree = HTMLParser(html)
                
                # ë‹¤ì´ë‹ì½”ë“œ íŠ¹ì • ì„ íƒì ì‚¬ìš©
                for link in tree.css('a.btxt'):
                    if link.attributes:
                        href = link.attributes.get('href', '')
                        if self.is_restaurant_url(href):
                            full_url = self._normalize_url(href)
                            urls.append(full_url)

                # ëŒ€ì²´ ì„ íƒìë“¤
                if not urls:
                    selectors = [
                        'a[href*="profile.php"]',
                        'a[href*="rid="]'
                    ]
                    
                    for selector in selectors:
                        for link in tree.css(selector):
                            if link.attributes:
                                href = link.attributes.get('href', '')
                                if self.is_restaurant_url(href):
                                    full_url = self._normalize_url(href)
                                    urls.append(full_url)
                        if urls:  # í•˜ë‚˜ë¼ë„ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                            break
            
        except Exception as e:
            logger.error(f"URL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return list(set(urls))  # ì¤‘ë³µ ì œê±°

    async def extract_restaurant_info(self, html: str, url: str) -> Dict[str, Any]:
        """HTMLì—ì„œ ì‹ë‹¹ ì •ë³´ ì¶”ì¶œ"""
        tree = HTMLParser(html)
        info = {}

        try:
            # ì‹ë‹¹ëª…
            name_element = tree.css_first('h1.tit, .tit h1, .restaurant_name h1, h1')
            if name_element:
                info['name'] = self._clean_text(name_element.text())

            # ì£¼ì†Œ - í…ìŠ¤íŠ¸ íŒ¨í„´ì—ì„œ ì¶”ì¶œ
            text_content = tree.text()
            addr_patterns = [
                r'ì„œìš¸íŠ¹ë³„ì‹œ\s+[^0-9]*?êµ¬\s+[^0-9]*?[ë™ë¡œê¸¸][^0-9]*?\d+[^0-9]*?[^\n]*',
                r'ì„œìš¸\s+[^0-9]*?êµ¬\s+[^0-9]*?[ë™ë¡œê¸¸][^0-9]*?\d+[^0-9]*?[^\n]*'
            ]
            
            for pattern in addr_patterns:
                addr_matches = re.findall(pattern, text_content)
                if addr_matches:
                    # ê°€ì¥ ì™„ì „í•œ ì£¼ì†Œ ì„ íƒ
                    full_address = max(addr_matches, key=len)
                    # ì£¼ì†Œ ì •ë¦¬: ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                    cleaned_address = self._clean_address(full_address)
                    info['address'] = cleaned_address
                    break

            # ì „í™”ë²ˆí˜¸ - ê°œì„ ëœ ì„ íƒìì™€ íŒ¨í„´ ë§¤ì¹­
            phone_element = tree.css_first('.tel, .phone, .contact')
            if phone_element:
                phone_text = self._clean_text(phone_element.text())
                # ì „í™”ë²ˆí˜¸ ì •ê·œí™”
                phone_clean = re.sub(r'[^\d-]', '', phone_text)
                if phone_clean and len(phone_clean) >= 10:
                    info['phone'] = phone_clean
            else:
                # í…ìŠ¤íŠ¸ì—ì„œ ì „í™”ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
                phone_patterns = [r'0\d{1,2}-\d{3,4}-\d{4}', r'\d{3,4}-\d{3,4}-\d{4}']
                for pattern in phone_patterns:
                    phone_matches = re.findall(pattern, text_content)
                    if phone_matches:
                        info['phone'] = phone_matches[0]
                        break

            # ì˜ì—…ì‹œê°„
            hours_element = tree.css_first('.hours, .time, .business_hours')
            if hours_element:
                info['business_hours'] = self._clean_text(hours_element.text())

            # í‰ì  - ê°œì„ ëœ ì„ íƒì
            score_element = tree.css_first('.score, .rating, .rate_point')
            if score_element:
                score_text = self._clean_text(score_element.text())
                score_match = re.search(r'(\d+\.?\d*)', score_text)
                if score_match:
                    try:
                        score_value = float(score_match.group(1))
                        # 100ì  ë§Œì ì„ 5ì  ë§Œì ìœ¼ë¡œ ë³€í™˜
                        if score_value > 10:
                            info['rating'] = round(score_value / 20, 1)
                        else:
                            info['rating'] = score_value
                    except ValueError:
                        pass

            # ë¦¬ë·° ìˆ˜
            review_element = tree.css_first('.Rating .rate_count, .rating .count, .review_count')
            if review_element:
                review_text = self._clean_text(review_element.text())
                review_count = self._extract_number(review_text)
                if review_count:
                    info['review_count'] = review_count

            # ìš”ë¦¬ ì¢…ë¥˜/ì¹´í…Œê³ ë¦¬
            category_elements = tree.css('.Restaurant_Category, .category, .food_type')
            if category_elements:
                categories = []
                for elem in category_elements:
                    cat_text = self._clean_text(elem.text())
                    if cat_text:
                        categories.append(cat_text)
                if categories:
                    info['cuisine_types'] = categories

            # ê°€ê²©ëŒ€
            price_element = tree.css_first('.Restaurant_Price, .price_range, .price')
            if price_element:
                price_text = self._clean_text(price_element.text())
                info['price_range'] = self._classify_price_range(price_text)

            # ë©”íƒ€ë°ì´í„°
            info['source_url'] = url
            info['source_name'] = self.source_name

            # ë°ì´í„° ê²€ì¦
            if not info.get('name'):
                logger.warning(f"No restaurant name found for URL: {url}")

            return info

        except Exception as e:
            logger.error(f"Error extracting restaurant info from {url}: {e}")
            return {'source_url': url, 'source_name': self.source_name}

    async def extract_menu_list(self, html: str, url: str) -> List[Dict[str, Any]]:
        """HTMLì—ì„œ ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
        tree = HTMLParser(html)
        menus = []

        try:
            # ë‹¤ì´ë‹ì½”ë“œ ë©”ë‰´ ì„¹ì…˜ ì°¾ê¸°
            menu_sections = [
                '.Menu_List',
                '.menu_list',
                '.Restaurant_Menu',
                '.menu_info',
                '.menu_section'
            ]

            menu_container = None
            for selector in menu_sections:
                menu_container = tree.css_first(selector)
                if menu_container:
                    break

            if not menu_container:
                # ëŒ€ì²´ ë°©ë²•: ë©”ë‰´ ì•„ì´í…œì„ ì§ì ‘ ì°¾ê¸°
                menu_items = tree.css('.menu_item, .Menu_Item, .menu, .food_menu')
            else:
                menu_items = menu_container.css('.menu_item, .Menu_Item, .menu, .food_menu')

            for item in menu_items:
                menu_info = self._extract_single_menu(item)
                if menu_info and menu_info.get('name'):
                    menus.append(menu_info)

            # ë©”ë‰´ê°€ ì—†ë‹¤ë©´ ë‹¤ë¥¸ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
            if not menus:
                menus = self._extract_menus_alternative_method(tree)

            logger.info(f"Extracted {len(menus)} menus from {url}")
            return menus

        except Exception as e:
            logger.error(f"Error extracting menu list from {url}: {e}")
            return []

    def _extract_single_menu(self, menu_element) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ"""
        try:
            menu_info = {}

            # ë©”ë‰´ëª…
            name_selectors = ['.menu_name', '.name', '.title', '.menu_title', 'h3', 'h4']
            for selector in name_selectors:
                name_elem = menu_element.css_first(selector)
                if name_elem:
                    name = self._clean_text(name_elem.text())
                    if name:
                        # ë©”ë‰´ëª… ì „ì²˜ë¦¬ ì ìš©
                        menu_info['name'] = clean_menu_name(name)
                        break

            # ê°€ê²©
            price_selectors = ['.menu_price', '.price', '.cost', '.amount']
            for selector in price_selectors:
                price_elem = menu_element.css_first(selector)
                if price_elem:
                    price_text = self._clean_text(price_elem.text())
                    price = self._extract_price(price_text)
                    if price:
                        menu_info['price'] = price
                        break

            # ì„¤ëª…
            desc_selectors = ['.menu_desc', '.description', '.desc', '.menu_detail']
            for selector in desc_selectors:
                desc_elem = menu_element.css_first(selector)
                if desc_elem:
                    desc = self._clean_text(desc_elem.text())
                    if desc and len(desc) > 5:  # ì˜ë¯¸ìˆëŠ” ì„¤ëª…ë§Œ
                        menu_info['description'] = desc
                        break

            return menu_info if menu_info.get('name') else None

        except Exception as e:
            logger.debug(f"Error extracting single menu: {e}")
            return None

    def _extract_menus_alternative_method(self, tree: HTMLParser) -> List[Dict[str, Any]]:
        """ëŒ€ì²´ ë©”ë‰´ ì¶”ì¶œ ë°©ë²•"""
        menus = []

        # í…ìŠ¤íŠ¸ì—ì„œ ë©”ë‰´ íŒ¨í„´ ì°¾ê¸°
        text_content = tree.text()
        menu_patterns = [
            r'([ê°€-í£\w\s]+)\s*[:\-]\s*(\d+[,\d]*)\s*ì›',
            r'([ê°€-í£\w\s]+)\s*(\d+[,\d]*)\s*ì›',
            r'â—¦\s*([ê°€-í£\w\s]+)\s*(\d+[,\d]*)'
        ]

        for pattern in menu_patterns:
            matches = re.finditer(pattern, text_content)
            for match in matches:
                name = self._clean_text(match.group(1))
                price_text = match.group(2).replace(',', '')

                if len(name) > 2 and name not in ['ì „í™”ë²ˆí˜¸', 'ì£¼ì†Œ', 'ì˜ì—…ì‹œê°„']:
                    try:
                        price = int(price_text)
                        if 1000 <= price <= 500000:  # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„
                            menus.append({
                                'name': clean_menu_name(name),  # ë©”ë‰´ëª… ì „ì²˜ë¦¬ ì ìš©
                                'price': price
                            })
                    except ValueError:
                        continue

        return menus[:20]  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ

    def _extract_price(self, price_text: str) -> Optional[int]:
        """ê°€ê²© í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        if not price_text:
            return None

        # ìˆ«ìì™€ ì½¤ë§ˆë§Œ ì¶”ì¶œ
        price_clean = re.sub(r'[^\d,]', '', price_text)
        price_clean = price_clean.replace(',', '')

        if price_clean.isdigit():
            price = int(price_clean)
            # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ ì²´í¬
            if 500 <= price <= 1000000:
                return price

        return None

    def _classify_price_range(self, price_text: str) -> Optional[str]:
        """ê°€ê²©ëŒ€ ë¶„ë¥˜"""
        if not price_text:
            return None

        price_text = price_text.lower()

        if any(keyword in price_text for keyword in ['ì €ë ´', 'ë§Œì› ì´í•˜', 'ê²½ì œì ']):
            return 'low'
        elif any(keyword in price_text for keyword in ['ê³ ê¸‰', 'ë¹„ì‹¼', 'í”„ë¦¬ë¯¸ì—„', '5ë§Œì› ì´ìƒ']):
            return 'high'
        elif any(keyword in price_text for keyword in ['ë³´í†µ', 'ì ë‹¹', '2-3ë§Œì›']):
            return 'medium'

        # ìˆ«ìì—ì„œ íŒë‹¨
        prices = re.findall(r'\d+', price_text.replace(',', ''))
        if prices:
            avg_price = sum(int(p) for p in prices) / len(prices)
            if avg_price < 15000:
                return 'low'
            elif avg_price > 50000:
                return 'high'
            else:
                return 'medium'

        return None

    def _clean_address(self, address: str) -> str:
        """ì£¼ì†Œ ì •ë¦¬ - ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°"""
        if not address:
            return ""
        
        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬
        address = self._clean_text(address)
        
        # 1ë‹¨ê³„: ì •í™•í•œ ì£¼ì†Œ íŒ¨í„´ë§Œ ì¶”ì¶œ (ê°€ì¥ ì¤‘ìš”)
        # ì„œìš¸íŠ¹ë³„ì‹œ + êµ¬ + ë™/ë¡œ/ê¸¸ + ìˆ«ì + ê±´ë¬¼ì •ë³´ ê¹Œì§€ë§Œ
        address_patterns = [
            # ë„ë¡œëª… ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ10ê¸¸ 21 1ì¸µ
            r'(ì„œìš¸íŠ¹ë³„ì‹œ\s+\S+êµ¬\s+\S*[ë¡œê¸¸]\d*\s+\d+(?:\s*-\s*\d+)?(?:\s+\d*ì¸µ)?)',
            # ì§€ë²ˆ ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™ 123-45
            r'(ì„œìš¸íŠ¹ë³„ì‹œ\s+\S+êµ¬\s+\S*ë™\s+\d+(?:\s*-\s*\d+)?)',
            # ê°„ë‹¨í•œ íŒ¨í„´: ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 147
            r'(ì„œìš¸íŠ¹ë³„ì‹œ\s+\S+êµ¬\s+\S+\s+\d+)'
        ]
        
        for pattern in address_patterns:
            match = re.search(pattern, address)
            if match:
                clean_addr = match.group(1).strip()
                # ì¶”ê°€ ì •ë¦¬
                clean_addr = re.sub(r'\s+', ' ', clean_addr)
                return clean_addr
        
        # 2ë‹¨ê³„: íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ìˆ˜ë™ ì •ë¦¬
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        unwanted_texts = [
            'ì§€ë²ˆ', 'ë„ë¡œëª…', 'ë²ˆì§€', 'ë²ˆì§€ìˆ˜', 'ìƒì„¸ì£¼ì†Œ', 'ê±´ë¬¼ëª…', 'ì¸µìˆ˜',
            'ì „í™”ë²ˆí˜¸', 'ì˜ì—…ì‹œê°„', 'íœ´ë¬´ì¼', 'ì£¼ì°¨', 'ë©”ë‰´', 'ê°€ê²©', 'ë¦¬ë·°',
            'í‰ì ', 'ë³„ì ', 'ì¶”ì²œ', 'ë§›ì§‘', 'ìŒì‹ì ', 'ë ˆìŠ¤í† ë‘', 'ì¹´í˜', 'ë°”',
            'â°', 'ğŸ“', 'ğŸª', 'ğŸ½ï¸', 'ğŸ’°', 'â­', 'ğŸ“', 'ğŸš—', 'ğŸ…¿ï¸',
            'í‰ì¼', 'ì£¼ë§', 'ê³µíœ´ì¼', 'íœ´ë¬´', 'ì˜ì—…', 'ì‹œê°„', 'ë¶„', 'ì›',
            'ê°ìíƒ•', 'ë¶€ëŒ€ì°Œê°œ', 'ëœì¥ì°Œê°œ', 'ê¹€ì¹˜ì°Œê°œ', 'ìˆœë‘ë¶€', 'ê°ˆë¹„', 'ì‚¼ê²¹ì‚´',
            'ì¹˜í‚¨', 'í”¼ì', 'í–„ë²„ê±°', 'íŒŒìŠ¤íƒ€', 'ìŠ¤í…Œì´í¬', 'ì´ˆë°¥', 'íšŒ', 'ë¼ë©´',
            'ë¼ˆ', 'ìˆ¯ë¶ˆ', 'êµ¬ì´', 'ë³¶ìŒ', 'ì°œ', 'íƒ•', 'êµ­', 'ë°¥', 'ë©´', 'ì „ê³¨'
        ]
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ë“¤ì„ ì œê±°
        for unwanted in unwanted_texts:
            address = re.sub(rf'\s*{re.escape(unwanted)}\s*', ' ', address)
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ì œê±°
        # ì‹œê°„ íŒ¨í„´ ì œê±° (ì˜ˆ: "10:00-20:30")
        address = re.sub(r'\d{1,2}:\d{2}[-~]\d{1,2}:\d{2}', '', address)
        
        # ì „í™”ë²ˆí˜¸ íŒ¨í„´ ì œê±°
        address = re.sub(r'0\d{1,2}-\d{3,4}-\d{4}', '', address)
        
        # ê°€ê²© íŒ¨í„´ ì œê±° (ì˜ˆ: "9,000ì›", "90,000ì›")
        address = re.sub(r'\d{1,3}(,\d{3})*ì›', '', address)
        
        # ìŒì‹ ê´€ë ¨ ë‹¨ì–´ë“¤ ì œê±° (ë” ê°•í™”)
        food_patterns = [
            r'[ê°€-í£]*íƒ•[ê°€-í£]*',  # ~íƒ•~
            r'[ê°€-í£]*ì°Œê°œ[ê°€-í£]*',  # ~ì°Œê°œ~
            r'[ê°€-í£]*êµ¬ì´[ê°€-í£]*',  # ~êµ¬ì´~
            r'[ê°€-í£]*ë³¶ìŒ[ê°€-í£]*',  # ~ë³¶ìŒ~
            r'[ê°€-í£]*êµ­[ê°€-í£]*',   # ~êµ­~
            r'[ê°€-í£]*ë°¥[ê°€-í£]*',   # ~ë°¥~
        ]
        
        for pattern in food_patterns:
            address = re.sub(pattern, ' ', address)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        address = re.sub(r'\s+', ' ', address).strip()
        
        # 3ë‹¨ê³„: ë§ˆì§€ë§‰ ê²€ì¦ - ì£¼ì†Œê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì´ìƒí•˜ë©´ ê¸°ë³¸ íŒ¨í„´ë§Œ
        if len(address) > 50 or not re.search(r'ì„œìš¸íŠ¹ë³„ì‹œ.*êµ¬.*\d+', address):
            # ìµœì†Œí•œì˜ ì£¼ì†Œ ì •ë³´ë§Œ ì¶”ì¶œ
            basic_match = re.search(r'(ì„œìš¸íŠ¹ë³„ì‹œ\s+\S+êµ¬\s+\S+\s+\d+)', address)
            if basic_match:
                address = basic_match.group(1).strip()
        
        return address