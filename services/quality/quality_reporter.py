from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import statistics

from core.domain.menu import Menu
from core.domain.keto_score import KetoScore, ScoreCategory
from services.scorer.keto_scorer import KetoScorer


class QualityLevel(Enum):
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    CRITICAL = "critical"


@dataclass
class QualityIssue:
    """í’ˆì§ˆ ì´ìŠˆ"""
    issue_type: str
    severity: QualityLevel
    description: str
    affected_items: List[str]
    recommendation: str
    auto_fixable: bool = False


@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ì§€í‘œ"""
    total_items: int
    scored_items: int
    high_confidence_items: int
    low_confidence_items: int
    avg_confidence: float
    keyword_coverage: float
    issues: List[QualityIssue] = field(default_factory=list)


@dataclass
class QualityReport:
    """í’ˆì§ˆ ë¦¬í¬íŠ¸"""
    report_id: str
    generated_at: datetime
    restaurant_id: Optional[str]
    overall_quality: QualityLevel
    metrics: QualityMetrics
    detailed_analysis: Dict[str, Any]
    recommendations: List[str]
    needs_review: List[str]


class QualityReporter:
    """í’ˆì§ˆ ë¦¬í¬íŠ¸ ë° ê²€ìˆ˜ ì‹œìŠ¤í…œ"""

    def __init__(self, keto_scorer: KetoScorer):
        self.keto_scorer = keto_scorer

        # í’ˆì§ˆ ê¸°ì¤€
        self.quality_thresholds = {
            'min_confidence': 0.5,
            'low_confidence_limit': 0.3,
            'keyword_coverage_min': 0.7,
            'max_no_keyword_ratio': 0.2
        }

    async def generate_quality_report(
        self,
        menus: List[Menu],
        restaurant_id: Optional[str] = None
    ) -> QualityReport:
        """í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""

        # ëª¨ë“  ë©”ë‰´ ì ìˆ˜ ê³„ì‚°
        scores = await self.keto_scorer.batch_calculate_scores(menus)

        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        metrics = self._calculate_metrics(menus, scores)

        # í’ˆì§ˆ ì´ìŠˆ ê°ì§€
        issues = self._detect_quality_issues(menus, scores)
        metrics.issues = issues

        # ì „ì²´ í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        overall_quality = self._determine_overall_quality(metrics)

        # ìƒì„¸ ë¶„ì„
        detailed_analysis = self._perform_detailed_analysis(menus, scores)

        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(metrics, issues)

        # ê²€ìˆ˜ í•„ìš” í•­ëª©
        needs_review = self._identify_review_items(menus, scores)

        return QualityReport(
            report_id=f"QR_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generated_at=datetime.now(),
            restaurant_id=restaurant_id,
            overall_quality=overall_quality,
            metrics=metrics,
            detailed_analysis=detailed_analysis,
            recommendations=recommendations,
            needs_review=needs_review
        )

    def _calculate_metrics(self, menus: List[Menu], scores: List[KetoScore]) -> QualityMetrics:
        """í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        total_items = len(menus)

        if total_items == 0:
            return QualityMetrics(
                total_items=0,
                scored_items=0,
                high_confidence_items=0,
                low_confidence_items=0,
                avg_confidence=0.0,
                keyword_coverage=0.0
            )

        scored_items = len(scores)
        confidences = [score.confidence for score in scores]
        avg_confidence = statistics.mean(confidences) if confidences else 0.0

        high_confidence_items = len([c for c in confidences if c >= self.quality_thresholds['min_confidence']])
        low_confidence_items = len([c for c in confidences if c < self.quality_thresholds['low_confidence_limit']])

        # í‚¤ì›Œë“œ ê°ì§€ìœ¨ ê³„ì‚°
        items_with_keywords = len([score for score in scores if score.detected_keywords])
        keyword_coverage = items_with_keywords / total_items if total_items > 0 else 0.0

        return QualityMetrics(
            total_items=total_items,
            scored_items=scored_items,
            high_confidence_items=high_confidence_items,
            low_confidence_items=low_confidence_items,
            avg_confidence=avg_confidence,
            keyword_coverage=keyword_coverage
        )

    def _detect_quality_issues(self, menus: List[Menu], scores: List[KetoScore]) -> List[QualityIssue]:
        """í’ˆì§ˆ ì´ìŠˆ ê°ì§€"""
        issues = []

        # 1. ë‚®ì€ ì‹ ë¢°ë„ ì´ìŠˆ
        low_confidence_items = [
            f"{menu.name} (ì‹ ë¢°ë„: {score.confidence:.2f})"
            for menu, score in zip(menus, scores)
            if score.confidence < self.quality_thresholds['low_confidence_limit']
        ]

        if low_confidence_items:
            issues.append(QualityIssue(
                issue_type="low_confidence",
                severity=QualityLevel.POOR if len(low_confidence_items) > len(menus) * 0.3 else QualityLevel.FAIR,
                description=f"ì‹ ë¢°ë„ê°€ ë‚®ì€ í•­ëª©ì´ {len(low_confidence_items)}ê°œ ë°œê²¬ë¨",
                affected_items=low_confidence_items[:10],  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                recommendation="ë©”ë‰´ëª…ì´ë‚˜ ì„¤ëª…ì„ ë” ìƒì„¸íˆ ìˆ˜ì§‘í•˜ê±°ë‚˜ í‚¤ì›Œë“œ ì‚¬ì „ì„ ë³´ì™„í•˜ì„¸ìš”",
                auto_fixable=False
            ))

        # 2. í‚¤ì›Œë“œ ë¯¸ê°ì§€ ì´ìŠˆ
        no_keyword_items = [
            menu.name for menu, score in zip(menus, scores)
            if not score.detected_keywords
        ]

        if len(no_keyword_items) > len(menus) * self.quality_thresholds['max_no_keyword_ratio']:
            issues.append(QualityIssue(
                issue_type="no_keywords",
                severity=QualityLevel.POOR,
                description=f"í‚¤ì›Œë“œê°€ ì „í˜€ ê°ì§€ë˜ì§€ ì•Šì€ í•­ëª©ì´ {len(no_keyword_items)}ê°œ",
                affected_items=no_keyword_items[:10],
                recommendation="í‚¤ì›Œë“œ ì‚¬ì „ì„ í™•ì¥í•˜ê±°ë‚˜ ë©”ë‰´ ë°ì´í„° í’ˆì§ˆì„ ê°œì„ í•˜ì„¸ìš”",
                auto_fixable=False
            ))

        # 3. ê·¹ë‹¨ì  ì ìˆ˜ ì´ìŠˆ
        extreme_scores = [
            f"{menu.name} (ì ìˆ˜: {score.final_score})"
            for menu, score in zip(menus, scores)
            if abs(score.final_score) >= 90
        ]

        if extreme_scores:
            issues.append(QualityIssue(
                issue_type="extreme_scores",
                severity=QualityLevel.FAIR,
                description=f"ê·¹ë‹¨ì  ì ìˆ˜ë¥¼ ë°›ì€ í•­ëª©ì´ {len(extreme_scores)}ê°œ",
                affected_items=extreme_scores,
                recommendation="ì ìˆ˜ê°€ ê·¹ë‹¨ì ì¸ í•­ëª©ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”",
                auto_fixable=False
            ))

        # 4. ë¹ˆ ë©”ë‰´ ì„¤ëª… ì´ìŠˆ
        empty_description_count = len([menu for menu in menus if not menu.description or len(menu.description.strip()) < 5])

        if empty_description_count > len(menus) * 0.5:
            issues.append(QualityIssue(
                issue_type="empty_descriptions",
                severity=QualityLevel.FAIR,
                description=f"ë©”ë‰´ ì„¤ëª…ì´ ë¶€ì¡±í•œ í•­ëª©ì´ {empty_description_count}ê°œ",
                affected_items=[],
                recommendation="ë©”ë‰´ ì„¤ëª… ìˆ˜ì§‘ì„ ê°œì„ í•˜ì„¸ìš”",
                auto_fixable=False
            ))

        return issues

    def _determine_overall_quality(self, metrics: QualityMetrics) -> QualityLevel:
        """ì „ì²´ í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        score = 0

        # ì‹ ë¢°ë„ ì ìˆ˜ (40ì )
        if metrics.avg_confidence >= 0.8:
            score += 40
        elif metrics.avg_confidence >= 0.6:
            score += 30
        elif metrics.avg_confidence >= 0.4:
            score += 20
        else:
            score += 10

        # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ì ìˆ˜ (30ì )
        if metrics.keyword_coverage >= 0.9:
            score += 30
        elif metrics.keyword_coverage >= 0.7:
            score += 20
        elif metrics.keyword_coverage >= 0.5:
            score += 15
        else:
            score += 5

        # ì´ìŠˆ ê°œìˆ˜ì— ë”°ë¥¸ ê°ì  (30ì ì—ì„œ ì°¨ê°)
        issue_penalty = len([issue for issue in metrics.issues if issue.severity in [QualityLevel.POOR, QualityLevel.CRITICAL]]) * 10
        score += max(0, 30 - issue_penalty)

        # ë“±ê¸‰ ê²°ì •
        if score >= 85:
            return QualityLevel.EXCELLENT
        elif score >= 70:
            return QualityLevel.GOOD
        elif score >= 50:
            return QualityLevel.FAIR
        elif score >= 30:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL

    def _perform_detailed_analysis(self, menus: List[Menu], scores: List[KetoScore]) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„"""
        if not scores:
            return {'error': 'ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}

        # ì ìˆ˜ ë¶„í¬
        score_distribution = {
            'excellent': len([s for s in scores if s.category == ScoreCategory.KETO_EXCELLENT]),
            'good': len([s for s in scores if s.category == ScoreCategory.KETO_GOOD]),
            'moderate': len([s for s in scores if s.category == ScoreCategory.KETO_MODERATE]),
            'poor': len([s for s in scores if s.category == ScoreCategory.KETO_POOR]),
            'avoid': len([s for s in scores if s.category == ScoreCategory.KETO_AVOID])
        }

        # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        all_keywords = []
        for score in scores:
            all_keywords.extend(score.detected_keywords)

        keyword_frequency = {}
        for keyword in all_keywords:
            keyword_frequency[keyword] = keyword_frequency.get(keyword, 0) + 1

        top_keywords = sorted(keyword_frequency.items(), key=lambda x: x[1], reverse=True)[:10]

        # ë£° ì ìš© í†µê³„
        all_rules = []
        for score in scores:
            all_rules.extend(score.applied_rules)

        rule_frequency = {}
        for rule in all_rules:
            rule_frequency[rule] = rule_frequency.get(rule, 0) + 1

        return {
            'score_statistics': {
                'mean': statistics.mean([s.final_score for s in scores]),
                'median': statistics.median([s.final_score for s in scores]),
                'std_dev': statistics.stdev([s.final_score for s in scores]) if len(scores) > 1 else 0,
                'min': min([s.final_score for s in scores]),
                'max': max([s.final_score for s in scores])
            },
            'confidence_statistics': {
                'mean': statistics.mean([s.confidence for s in scores]),
                'median': statistics.median([s.confidence for s in scores]),
                'min': min([s.confidence for s in scores]),
                'max': max([s.confidence for s in scores])
            },
            'category_distribution': score_distribution,
            'top_keywords': top_keywords,
            'rule_usage': rule_frequency,
            'data_completeness': {
                'menus_with_description': len([m for m in menus if m.description]),
                'menus_with_price': len([m for m in menus if m.price]),
                'avg_name_length': statistics.mean([len(m.name) for m in menus]),
                'avg_description_length': statistics.mean([len(m.description or '') for m in menus])
            }
        }

    def _generate_recommendations(self, metrics: QualityMetrics, issues: List[QualityIssue]) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ì‹ ë¢°ë„ ê°œì„ 
        if metrics.avg_confidence < 0.6:
            recommendations.append("ì „ì²´ì ì¸ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë©”ë‰´ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê°œì„ í•˜ê±°ë‚˜ í‚¤ì›Œë“œ ì‚¬ì „ì„ ë³´ì™„í•˜ì„¸ìš”.")

        # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ê°œì„ 
        if metrics.keyword_coverage < 0.7:
            recommendations.append("í‚¤ì›Œë“œ ê°ì§€ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ë” ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¥¼ ì‚¬ì „ì— ì¶”ê°€í•˜ì„¸ìš”.")

        # ì´ìŠˆë³„ ì¶”ì²œì‚¬í•­
        for issue in issues:
            if issue.severity in [QualityLevel.POOR, QualityLevel.CRITICAL]:
                recommendations.append(f"ğŸ”´ {issue.recommendation}")
            elif issue.severity == QualityLevel.FAIR:
                recommendations.append(f"ğŸŸ¡ {issue.recommendation}")

        # ë°ì´í„° ìˆ˜ì§‘ ê°œì„ 
        if metrics.total_items < 10:
            recommendations.append("ë©”ë‰´ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ë§ì€ ë©”ë‰´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")

        return recommendations

    def _identify_review_items(self, menus: List[Menu], scores: List[KetoScore]) -> List[str]:
        """ê²€ìˆ˜ê°€ í•„ìš”í•œ í•­ëª© ì‹ë³„"""
        review_items = []

        for menu, score in zip(menus, scores):
            # ë‚®ì€ ì‹ ë¢°ë„
            if score.confidence < 0.3:
                review_items.append(f"[ë‚®ì€ ì‹ ë¢°ë„] {menu.name}")

            # ê·¹ë‹¨ì  ì ìˆ˜
            if abs(score.final_score) >= 80:
                review_items.append(f"[ê·¹ë‹¨ì  ì ìˆ˜] {menu.name} ({score.final_score}ì )")

            # í‚¤ì›Œë“œ ì—†ìŒ
            if not score.detected_keywords:
                review_items.append(f"[í‚¤ì›Œë“œ ì—†ìŒ] {menu.name}")

            # ë£° ì ìš© ê°œìˆ˜ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ ì ìŒ
            if len(score.applied_rules) >= 5:
                review_items.append(f"[ë³µì¡í•œ ë£° ì ìš©] {menu.name}")
            elif len(score.applied_rules) == 0:
                review_items.append(f"[ë£° ë¯¸ì ìš©] {menu.name}")

        return review_items[:20]  # ìµœëŒ€ 20ê°œ

    async def generate_comparative_report(
        self,
        current_menus: List[Menu],
        previous_menus: List[Menu],
        restaurant_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± (ì‹œê°„ë³„ í’ˆì§ˆ ë³€í™”)"""

        current_scores = await self.keto_scorer.batch_calculate_scores(current_menus)
        previous_scores = await self.keto_scorer.batch_calculate_scores(previous_menus)

        current_metrics = self._calculate_metrics(current_menus, current_scores)
        previous_metrics = self._calculate_metrics(previous_menus, previous_scores)

        return {
            'comparison_summary': {
                'current_quality': self._determine_overall_quality(current_metrics).value,
                'previous_quality': self._determine_overall_quality(previous_metrics).value,
                'quality_change': 'improved' if current_metrics.avg_confidence > previous_metrics.avg_confidence else 'declined'
            },
            'metrics_comparison': {
                'confidence_change': current_metrics.avg_confidence - previous_metrics.avg_confidence,
                'coverage_change': current_metrics.keyword_coverage - previous_metrics.keyword_coverage,
                'item_count_change': current_metrics.total_items - previous_metrics.total_items
            },
            'new_issues': [
                issue.description for issue in current_metrics.issues
                if not any(prev_issue.issue_type == issue.issue_type for prev_issue in previous_metrics.issues)
            ],
            'resolved_issues': [
                issue.description for issue in previous_metrics.issues
                if not any(curr_issue.issue_type == issue.issue_type for curr_issue in current_metrics.issues)
            ]
        }