# ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„¤ê³„ ê°€ì´ë“œ

## 1. ê¸°ë³¸ ê°œë… (ì´ˆë³´ììš©)

### ì™œ ë°ì´í„°ë¥¼ ì¬ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?
- **ì‹œê°„ ì ˆì•½**: ë§¤ë²ˆ í¬ë¡¤ë§í•˜ì§€ ì•Šê³  ì €ì¥ëœ ë°ì´í„° í™œìš©
- **ë¹„ìš© ì ˆì•½**: API í˜¸ì¶œ íšŸìˆ˜ ì¤„ì„ (ì§€ì˜¤ì½”ë”© ë“±)
- **ì•ˆì •ì„±**: ì›¹ì‚¬ì´íŠ¸ ì°¨ë‹¨ ìœ„í—˜ ê°ì†Œ
- **ê°œë°œ íš¨ìœ¨ì„±**: í…ŒìŠ¤íŠ¸í•  ë•Œë§ˆë‹¤ í¬ë¡¤ë§í•  í•„ìš” ì—†ìŒ

### ë°ì´í„° ì €ì¥ ì „ëµ
```
í¬ë¡¤ë§ â†’ ì›ë³¸ ë°ì´í„° ì €ì¥ â†’ ì •ì œ/ê°€ê³µ â†’ ìµœì¢… DB ì €ì¥
   â†“            â†“               â†“            â†“
ì›¹ì‚¬ì´íŠ¸    JSON/CSV íŒŒì¼    Python ì²˜ë¦¬   PostgreSQL
```

## 2. ë°ì´í„° ì €ì¥ êµ¬ì¡°

### 2.1 í´ë” êµ¬ì¡°
```
final_ETL/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ 2024-01-15_diningcode_raw.json
â”‚   â”‚   â””â”€â”€ 2024-01-16_diningcode_raw.json
â”‚   â”œâ”€â”€ processed/        # ì •ì œëœ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ 2024-01-15_restaurants.csv
â”‚   â”‚   â””â”€â”€ 2024-01-15_menus.csv
â”‚   â””â”€â”€ cache/           # ì¤‘ê°„ ìºì‹œ ë°ì´í„°
â”‚       â””â”€â”€ geocoding_cache.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ crawl_diningcode.py
â”‚   â”œâ”€â”€ process_data.py
â”‚   â””â”€â”€ load_to_db.py
â””â”€â”€ config/
    â””â”€â”€ settings.json
```

### 2.2 ë°ì´í„° ì €ì¥ ë°©ì‹

#### A) ì›ë³¸ ë°ì´í„° ì €ì¥ (JSON)
```json
{
  "crawl_date": "2024-01-15T10:30:00",
  "source": "diningcode",
  "restaurants": [
    {
      "source_url": "https://www.diningcode.com/profile.php?rid=123",
      "name_raw": "ê°•ë‚¨ ë§›ì§‘",
      "addr_raw": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123",
      "phone_raw": "02-1234-5678",
      "menus_raw": [
        {"name": "ê¹€ì¹˜ì°Œê°œ", "price": "8000ì›"},
        {"name": "ëœì¥ì°Œê°œ", "price": "7000ì›"}
      ],
      "crawl_timestamp": "2024-01-15T10:31:15"
    }
  ]
}
```

#### B) ìºì‹œ ë°ì´í„° (ì§€ì˜¤ì½”ë”©)
```json
{
  "geocoding_cache": {
    "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123": {
      "addr_norm": "ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123",
      "lat": 37.4979,
      "lng": 127.0276,
      "cached_at": "2024-01-15T10:32:00"
    }
  }
}
```

## 3. ì¬ì‚¬ìš© ë¡œì§ êµ¬í˜„

### 3.1 ë°ì´í„° í™•ì¸ í•¨ìˆ˜
```python
import os
import json
from datetime import datetime, timedelta

def check_existing_data(source_name, max_age_days=7):
    """
    ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œì§€ íŒë‹¨

    Args:
        source_name: 'diningcode' ë“±
        max_age_days: ë°ì´í„° ìœ íš¨ ê¸°ê°„ (ì¼)

    Returns:
        dict: {'exists': bool, 'file_path': str, 'age_days': int}
    """
    data_dir = "data/raw"
    today = datetime.now().strftime("%Y-%m-%d")

    # ìµœê·¼ íŒŒì¼ë“¤ í™•ì¸
    for i in range(max_age_days + 1):
        check_date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
        file_path = f"{data_dir}/{check_date}_{source_name}_raw.json"

        if os.path.exists(file_path):
            return {
                'exists': True,
                'file_path': file_path,
                'age_days': i
            }

    return {'exists': False, 'file_path': None, 'age_days': None}

# ì‚¬ìš© ì˜ˆì‹œ
existing = check_existing_data('diningcode', max_age_days=3)
if existing['exists']:
    print(f"ê¸°ì¡´ ë°ì´í„° ë°œê²¬: {existing['file_path']} (ìƒì„±ëœì§€ {existing['age_days']}ì¼)")
else:
    print("ìƒˆë¡œ í¬ë¡¤ë§ í•„ìš”")
```

### 3.2 ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜
```python
def smart_crawl(force_new=False, max_age_days=3):
    """
    ê¸°ì¡´ ë°ì´í„° í™•ì¸ í›„ í•„ìš”ì‹œì—ë§Œ í¬ë¡¤ë§

    Args:
        force_new: Trueë©´ ë¬´ì¡°ê±´ ìƒˆë¡œ í¬ë¡¤ë§
        max_age_days: ë°ì´í„° ìœ íš¨ ê¸°ê°„
    """

    if not force_new:
        existing = check_existing_data('diningcode', max_age_days)

        if existing['exists']:
            print(f"âœ… ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©: {existing['file_path']}")
            return load_existing_data(existing['file_path'])

    print("ğŸ•·ï¸ ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‹œì‘...")
    new_data = crawl_diningcode()  # ì‹¤ì œ í¬ë¡¤ë§ í•¨ìˆ˜
    save_raw_data(new_data)       # ì›ë³¸ ë°ì´í„° ì €ì¥
    return new_data

def load_existing_data(file_path):
    """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_raw_data(data):
    """ì›ë³¸ ë°ì´í„° ì €ì¥"""
    today = datetime.now().strftime("%Y-%m-%d")
    file_path = f"data/raw/{today}_diningcode_raw.json"

    os.makedirs("data/raw", exist_ok=True)

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì›ë³¸ ë°ì´í„° ì €ì¥: {file_path}")
```

### 3.3 ì§€ì˜¤ì½”ë”© ìºì‹œ ì‹œìŠ¤í…œ
```python
def load_geocoding_cache():
    """ì§€ì˜¤ì½”ë”© ìºì‹œ ë¡œë“œ"""
    cache_file = "data/cache/geocoding_cache.json"

    if os.path.exists(cache_file):
        with open(cache_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    return {"geocoding_cache": {}}

def save_geocoding_cache(cache):
    """ì§€ì˜¤ì½”ë”© ìºì‹œ ì €ì¥"""
    cache_file = "data/cache/geocoding_cache.json"
    os.makedirs("data/cache", exist_ok=True)

    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def geocode_with_cache(address):
    """ìºì‹œë¥¼ í™œìš©í•œ ì§€ì˜¤ì½”ë”©"""
    cache = load_geocoding_cache()

    # ìºì‹œì— ìˆëŠ”ì§€ í™•ì¸
    if address in cache["geocoding_cache"]:
        print(f"ğŸ’¾ ìºì‹œì—ì„œ ì£¼ì†Œ ë¡œë“œ: {address}")
        return cache["geocoding_cache"][address]

    # ì—†ìœ¼ë©´ API í˜¸ì¶œ
    print(f"ğŸŒ API í˜¸ì¶œ: {address}")
    result = call_kakao_geocoding_api(address)  # ì‹¤ì œ API í˜¸ì¶œ

    # ìºì‹œì— ì €ì¥
    cache["geocoding_cache"][address] = {
        **result,
        "cached_at": datetime.now().isoformat()
    }
    save_geocoding_cache(cache)

    return result
```

## 4. ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### 4.1 ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ (main.py)
```python
#!/usr/bin/env python3
"""
ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
from datetime import datetime

def main():
    parser = argparse.ArgumentParser(description='ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ')
    parser.add_argument('--force-new', action='store_true',
                       help='ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ìƒˆë¡œ í¬ë¡¤ë§')
    parser.add_argument('--max-age', type=int, default=3,
                       help='ë°ì´í„° ìœ íš¨ ê¸°ê°„ (ì¼, ê¸°ë³¸ê°’: 3)')
    parser.add_argument('--no-geocoding', action='store_true',
                       help='ì§€ì˜¤ì½”ë”© ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ìš©)')

    args = parser.parse_args()

    print("ğŸš€ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now()}")

    # 1. ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ (í•„ìš”ì‹œì—ë§Œ)
    raw_data = smart_crawl(
        force_new=args.force_new,
        max_age_days=args.max_age
    )

    # 2. ë°ì´í„° ì •ì œ (ì§€ì˜¤ì½”ë”© ìºì‹œ í™œìš©)
    processed_data = process_restaurants(
        raw_data,
        use_geocoding=not args.no_geocoding
    )

    # 3. CSV ì €ì¥
    save_to_csv(processed_data)

    # 4. DB ì ì¬ (ì„ íƒì )
    if input("DBì— ì ì¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y':
        load_to_database(processed_data)

    print("âœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()
```

### 4.2 ì‚¬ìš©ë²•
```bash
# ê¸°ë³¸ ì‚¬ìš© (3ì¼ ì´ë‚´ ë°ì´í„° ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
python main.py

# ë¬´ì¡°ê±´ ìƒˆë¡œ í¬ë¡¤ë§
python main.py --force-new

# 7ì¼ ì´ë‚´ ë°ì´í„°ê¹Œì§€ ì¬ì‚¬ìš©
python main.py --max-age 7

# ì§€ì˜¤ì½”ë”© ì—†ì´ í…ŒìŠ¤íŠ¸
python main.py --no-geocoding
```

## 5. ì´ˆë³´ìë¥¼ ìœ„í•œ íŒ

### 5.1 ë‹¨ê³„ë³„ êµ¬í˜„ ìˆœì„œ
1. **1ë‹¨ê³„**: ì›ë³¸ ë°ì´í„° JSON ì €ì¥ë§Œ êµ¬í˜„
2. **2ë‹¨ê³„**: ê¸°ì¡´ íŒŒì¼ í™•ì¸ ë¡œì§ ì¶”ê°€
3. **3ë‹¨ê³„**: ì§€ì˜¤ì½”ë”© ìºì‹œ ì‹œìŠ¤í…œ ì¶”ê°€
4. **4ë‹¨ê³„**: ëª…ë ¹í–‰ ì˜µì…˜ ì¶”ê°€

### 5.2 ë””ë²„ê¹… íŒ
```python
# ë¡œê·¸ ì¶”ê°€
import logging

logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')

def smart_crawl_with_logs(force_new=False, max_age_days=3):
    logger = logging.getLogger(__name__)

    logger.info(f"í¬ë¡¤ë§ ì‹œì‘ - force_new: {force_new}, max_age: {max_age_days}")

    if not force_new:
        existing = check_existing_data('diningcode', max_age_days)
        logger.info(f"ê¸°ì¡´ ë°ì´í„° í™•ì¸ ê²°ê³¼: {existing}")

        if existing['exists']:
            logger.info(f"ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©: {existing['file_path']}")
            return load_existing_data(existing['file_path'])

    logger.info("ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‹œì‘")
    # ... ë‚˜ë¨¸ì§€ ë¡œì§
```

### 5.3 ì—ëŸ¬ ì²˜ë¦¬
```python
def safe_crawl():
    """ì•ˆì „í•œ í¬ë¡¤ë§ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
    try:
        return smart_crawl()
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

        # ë°±ì—… ë°ì´í„° ì°¾ê¸°
        backup = check_existing_data('diningcode', max_age_days=30)
        if backup['exists']:
            print(f"ğŸ”„ ë°±ì—… ë°ì´í„° ì‚¬ìš©: {backup['file_path']}")
            return load_existing_data(backup['file_path'])

        raise Exception("í¬ë¡¤ë§ ì‹¤íŒ¨ ë° ë°±ì—… ë°ì´í„° ì—†ìŒ")
```

## 6. ë‹¤ìŒ ë‹¨ê³„

ì´ ì‹œìŠ¤í…œì„ êµ¬í˜„í•œ í›„:
1. **ëª¨ë‹ˆí„°ë§**: ë°ì´í„° í’ˆì§ˆ ì²´í¬
2. **ìë™í™”**: ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì •ê¸° ì—…ë°ì´íŠ¸
3. **í™•ì¥**: ë‹¤ë¥¸ ì†ŒìŠ¤ ì¶”ê°€ (ì‹ì‹  ë“±)
4. **ìµœì í™”**: ì¤‘ë³µ ì œê±° ë¡œì§ ê°œì„ 

ì´ë ‡ê²Œ í•˜ë©´ í•œ ë²ˆ í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!