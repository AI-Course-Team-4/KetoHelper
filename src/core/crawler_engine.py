"""
ğŸ”„ í¬ë¡¤ë§ ì—”ì§„
- ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- íŒŒì„œì™€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
- ì§„í–‰ ìƒí™© ì¶”ì  ë° í†µê³„
"""

import asyncio
import traceback
from uuid import uuid4
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

from ..models import (
    Restaurant, RestaurantCreate, RestaurantUpdate,
    Menu, MenuCreate, MenuUpdate, 
    CrawlJob, CrawlJobCreate, CrawlJobUpdate,
    JobStatus, JobType
)
from ..database import (
    get_restaurant_repository,
    get_menu_repository, 
    get_crawl_job_repository,
    get_database
)
from ..parsers import get_parser_factory, get_parser
from ..utils.logger import get_logger
from ..utils.config_loader import get_config
from ..utils.http_client import HttpClient
from ..utils.text_utils import calculate_similarity


@dataclass
class CrawlingResult:
    """í¬ë¡¤ë§ ê²°ê³¼"""
    job_id: str
    success: bool
    restaurant_count: int
    menu_count: int
    duplicate_count: int
    error_count: int
    processing_time: float
    errors: List[str]
    metadata: Dict[str, Any]


class CrawlerEngine:
    """í¬ë¡¤ë§ ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger("crawler_engine")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.http_client = None
        self.parser_factory = None
        
        # í†µê³„
        self.stats = {
            "total_jobs": 0,
            "successful_jobs": 0,
            "failed_jobs": 0,
            "restaurants_crawled": 0,
            "menus_crawled": 0,
            "duplicates_found": 0,
            "errors_encountered": 0
        }
        
    async def initialize(self):
        """ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            self.logger.info("í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘")
            
            # HTTP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.http_client = HttpClient(self.config)
            await self.http_client.initialize()
            
            # íŒŒì„œ íŒ©í† ë¦¬ ì´ˆê¸°í™”
            self.parser_factory = get_parser_factory()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            async with get_database() as db:
                result = await db.execute_query("SELECT 1")
                if not result:
                    raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            
            self.logger.info("í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
            
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.http_client:
                await self.http_client.cleanup()
            self.logger.info("í¬ë¡¤ë§ ì—”ì§„ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def crawl_restaurant_by_name(self, restaurant_name: str, site: str = "siksin") -> CrawlingResult:
        """ì‹ë‹¹ëª…ìœ¼ë¡œ í¬ë¡¤ë§"""
        job_id = str(uuid4())
        start_time = datetime.now()
        
        self.logger.info(f"ì‹ë‹¹ í¬ë¡¤ë§ ì‹œì‘: {restaurant_name} (ì‚¬ì´íŠ¸: {site})")
        
        try:
            # í¬ë¡¤ ì‘ì—… ìƒì„±
            crawl_job_repo = get_crawl_job_repository()
            job_data = CrawlJobCreate(
                job_type=JobType.RESTAURANT_SEARCH,
                site=site,
                keyword=restaurant_name,
                status=JobStatus.RUNNING,
                metadata={"manual_trigger": True}
            )
            
            job = await crawl_job_repo.create(job_data)
            job_id = str(job.id)
            
            # íŒŒì„œ íšë“
            parser = get_parser(site, self.http_client)
            if not parser:
                raise Exception(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site}")
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_result = await parser.search(restaurant_name, page=1)
            
            if not search_result.restaurants:
                await self._update_job_status(job_id, JobStatus.COMPLETED, 
                                            f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {restaurant_name}")
                return CrawlingResult(
                    job_id=job_id,
                    success=True,
                    restaurant_count=0,
                    menu_count=0,
                    duplicate_count=0,
                    error_count=0,
                    processing_time=(datetime.now() - start_time).total_seconds(),
                    errors=[],
                    metadata={"message": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}
                )
            
            # ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
            return await self._process_restaurant_list(
                job_id, search_result.restaurants, site, start_time
            )
            
        except Exception as e:
            error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            
            await self._update_job_status(job_id, JobStatus.FAILED, error_msg)
            
            return CrawlingResult(
                job_id=job_id,
                success=False,
                restaurant_count=0,
                menu_count=0,
                duplicate_count=0,
                error_count=1,
                processing_time=(datetime.now() - start_time).total_seconds(),
                errors=[error_msg],
                metadata={"exception": str(e)}
            )
    
    async def _process_restaurant_list(self, job_id: str, restaurants: List[RestaurantCreate], 
                                     site: str, start_time: datetime) -> CrawlingResult:
        """ì‹ë‹¹ ëª©ë¡ ì²˜ë¦¬"""
        restaurant_count = 0
        menu_count = 0
        duplicate_count = 0
        error_count = 0
        errors = []
        
        parser = get_parser(site, self.http_client)
        restaurant_repo = get_restaurant_repository()
        menu_repo = get_menu_repository()
        
        for restaurant_data in restaurants:
            try:
                self.logger.info(f"ì‹ë‹¹ ì²˜ë¦¬: {restaurant_data.name}")
                
                # ì¤‘ë³µ ì²´í¬
                existing = await self._check_duplicate_restaurant(restaurant_data)
                if existing:
                    self.logger.info(f"ì¤‘ë³µ ì‹ë‹¹ ë°œê²¬: {restaurant_data.name}")
                    duplicate_count += 1
                    
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                    await self._update_existing_restaurant(existing, restaurant_data)
                    continue
                
                # ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (URLì´ ìˆëŠ” ê²½ìš°)
                menus = []
                if restaurant_data.source_url:
                    detail_result = await parser.parse_restaurant_detail(restaurant_data.source_url)
                    if detail_result.success and detail_result.data:
                        # ì‹ë‹¹ ìƒì„¸ ì •ë³´ ì—…ë°ì´íŠ¸
                        detail_restaurant = detail_result.data.get('restaurant', {})
                        restaurant_data = self._merge_restaurant_data(restaurant_data, detail_restaurant)
                        
                        # ë©”ë‰´ ì •ë³´
                        menu_data_list = detail_result.data.get('menus', [])
                        for menu_data in menu_data_list:
                            menus.append(MenuCreate(**menu_data))
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
                quality_score = self._calculate_quality_score(restaurant_data, menus)
                restaurant_data.quality_score = quality_score
                
                # ì‹ë‹¹ ì €ì¥
                restaurant = await restaurant_repo.create(restaurant_data)
                restaurant_count += 1
                self.logger.info(f"ì‹ë‹¹ ì €ì¥ ì™„ë£Œ: {restaurant.name} (ID: {restaurant.id})")
                
                # ë©”ë‰´ ì €ì¥
                for menu_data in menus:
                    menu_data.restaurant_id = restaurant.id
                    await menu_repo.create(menu_data)
                    menu_count += 1
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats["restaurants_crawled"] += 1
                self.stats["menus_crawled"] += len(menus)
                
            except Exception as e:
                error_msg = f"ì‹ë‹¹ ì²˜ë¦¬ ì‹¤íŒ¨ ({restaurant_data.name}): {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                error_count += 1
                self.stats["errors_encountered"] += 1
        
        # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
        processing_time = (datetime.now() - start_time).total_seconds()
        
        status = JobStatus.COMPLETED if error_count == 0 else JobStatus.COMPLETED_WITH_ERRORS
        status_message = f"ì™„ë£Œ - ì‹ë‹¹: {restaurant_count}, ë©”ë‰´: {menu_count}, ì¤‘ë³µ: {duplicate_count}, ì—ëŸ¬: {error_count}"
        
        await self._update_job_status(job_id, status, status_message, {
            "restaurant_count": restaurant_count,
            "menu_count": menu_count,
            "duplicate_count": duplicate_count,
            "error_count": error_count,
            "processing_time": processing_time
        })
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats["total_jobs"] += 1
        if error_count == 0:
            self.stats["successful_jobs"] += 1
        else:
            self.stats["failed_jobs"] += 1
        self.stats["duplicates_found"] += duplicate_count
        
        return CrawlingResult(
            job_id=job_id,
            success=True,
            restaurant_count=restaurant_count,
            menu_count=menu_count,
            duplicate_count=duplicate_count,
            error_count=error_count,
            processing_time=processing_time,
            errors=errors,
            metadata={
                "site": site,
                "total_processed": len(restaurants)
            }
        )
    
    async def _check_duplicate_restaurant(self, restaurant_data: RestaurantCreate) -> Optional[Restaurant]:
        """ì¤‘ë³µ ì‹ë‹¹ ì²´í¬"""
        restaurant_repo = get_restaurant_repository()
        
        # ì´ë¦„ ê¸°ë°˜ ê²€ìƒ‰
        similar = await restaurant_repo.find_similar_by_name(restaurant_data.name, threshold=0.8)
        if similar:
            for existing in similar:
                # ì¶”ê°€ ìœ ì‚¬ì„± ì²´í¬ (ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ ë“±)
                if self._is_same_restaurant(existing, restaurant_data):
                    return existing
        
        return None
    
    def _is_same_restaurant(self, existing: Restaurant, new_data: RestaurantCreate) -> bool:
        """ê°™ì€ ì‹ë‹¹ì¸ì§€ íŒë‹¨"""
        # ì´ë¦„ ìœ ì‚¬ë„
        name_similarity = calculate_similarity(existing.name, new_data.name)
        if name_similarity < 0.7:
            return False
        
        # ì£¼ì†Œ ìœ ì‚¬ë„ (ìˆëŠ” ê²½ìš°)
        if existing.address_road and new_data.address_road:
            addr_similarity = calculate_similarity(existing.address_road, new_data.address_road)
            if addr_similarity > 0.6:
                return True
        
        # ì „í™”ë²ˆí˜¸ ì¼ì¹˜ (ìˆëŠ” ê²½ìš°)
        if existing.phone and new_data.phone:
            return existing.phone == new_data.phone
        
        # ì¢Œí‘œ ê±°ë¦¬ (ìˆëŠ” ê²½ìš°)
        if (existing.lat and existing.lng and 
            new_data.lat and new_data.lng):
            distance = self._calculate_distance(
                existing.lat, existing.lng,
                new_data.lat, new_data.lng
            )
            if distance < 0.1:  # 100m ì´ë‚´
                return True
        
        return name_similarity > 0.9  # ì´ë¦„ì´ ë§¤ìš° ìœ ì‚¬í•˜ë©´ ë™ì¼ë¡œ íŒë‹¨
    
    def _calculate_distance(self, lat1: float, lng1: float, lat2: float, lng2: float) -> float:
        """ë‘ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (km)"""
        from math import radians, sin, cos, sqrt, atan2
        
        R = 6371  # ì§€êµ¬ ë°˜ì§€ë¦„ (km)
        
        lat1_rad = radians(lat1)
        lng1_rad = radians(lng1)
        lat2_rad = radians(lat2)
        lng2_rad = radians(lng2)
        
        dlat = lat2_rad - lat1_rad
        dlng = lng2_rad - lng1_rad
        
        a = sin(dlat/2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlng/2)**2
        c = 2 * atan2(sqrt(a), sqrt(1-a))
        
        return R * c
    
    async def _update_existing_restaurant(self, existing: Restaurant, new_data: RestaurantCreate):
        """ê¸°ì¡´ ì‹ë‹¹ ì •ë³´ ì—…ë°ì´íŠ¸"""
        restaurant_repo = get_restaurant_repository()
        
        # ë” ë‚˜ì€ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
        update_data = {}
        
        # í‰ì ì´ ë” ì¢‹ì€ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
        if new_data.rating and (not existing.rating or new_data.rating > existing.rating):
            update_data['rating'] = new_data.rating
        
        # ë¦¬ë·° ìˆ˜ê°€ ë” ë§ì€ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
        if new_data.review_count and (not existing.review_count or new_data.review_count > existing.review_count):
            update_data['review_count'] = new_data.review_count
        
        # ë¹ˆ í•„ë“œ ì±„ìš°ê¸°
        for field in ['phone', 'homepage_url', 'business_hours', 'lat', 'lng']:
            if not getattr(existing, field) and getattr(new_data, field):
                update_data[field] = getattr(new_data, field)
        
        if update_data:
            update_data['updated_at'] = datetime.now()
            await restaurant_repo.update(existing.id, RestaurantUpdate(**update_data))
            self.logger.info(f"ê¸°ì¡´ ì‹ë‹¹ ì •ë³´ ì—…ë°ì´íŠ¸: {existing.name}")
    
    def _merge_restaurant_data(self, base_data: RestaurantCreate, detail_data: Dict[str, Any]) -> RestaurantCreate:
        """ê¸°ë³¸ ë°ì´í„°ì™€ ìƒì„¸ ë°ì´í„° ë³‘í•©"""
        # ë” ì™„ì „í•œ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
        for field, value in detail_data.items():
            if value and (not getattr(base_data, field, None) or 
                         getattr(base_data, field, None) == ""):
                setattr(base_data, field, value)
        
        return base_data
    
    def _calculate_quality_score(self, restaurant: RestaurantCreate, menus: List[MenuCreate]) -> int:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 0
        
        # í•„ìˆ˜ ì •ë³´ (40ì )
        if restaurant.name: score += 20
        if restaurant.address_road: score += 10
        if restaurant.phone: score += 10
        
        # ìœ„ì¹˜ ì •ë³´ (20ì )
        if restaurant.lat and restaurant.lng: score += 20
        
        # í‰ì /ë¦¬ë·° ì •ë³´ (20ì )
        if restaurant.rating: score += 10
        if restaurant.review_count: score += 10
        
        # ë©”ë‰´ ì •ë³´ (20ì )
        if menus:
            score += min(len(menus) * 2, 15)  # ë©”ë‰´ ê°œìˆ˜
            if any(menu.price for menu in menus): score += 5  # ê°€ê²© ì •ë³´
        
        return min(score, 100)
    
    async def _update_job_status(self, job_id: str, status: JobStatus, 
                               message: str, metadata: Optional[Dict[str, Any]] = None):
        """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            crawl_job_repo = get_crawl_job_repository()
            update_data = CrawlJobUpdate(
                status=status,
                progress_message=message,
                completed_at=datetime.now() if status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.COMPLETED_WITH_ERRORS] else None
            )
            
            if metadata:
                update_data.metadata = metadata
            
            await crawl_job_repo.update(job_id, update_data)
            
        except Exception as e:
            self.logger.error(f"ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ì—”ì§„ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            "parser_stats": self.parser_factory.get_parser_info() if self.parser_factory else {},
            "http_client_stats": self.http_client.get_stats() if self.http_client else {}
        }
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.stats = {k: 0 for k in self.stats.keys()}


# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_restaurant(restaurant_name: str, site: str = "siksin") -> CrawlingResult:
    """ì‹ë‹¹ í¬ë¡¤ë§ í¸ì˜ í•¨ìˆ˜"""
    engine = CrawlerEngine()
    try:
        await engine.initialize()
        return await engine.crawl_restaurant_by_name(restaurant_name, site)
    finally:
        await engine.cleanup()


if __name__ == "__main__":
    import asyncio
    
    async def test_crawler():
        """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
        print("=== í¬ë¡¤ëŸ¬ ì—”ì§„ í…ŒìŠ¤íŠ¸ ===")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = await crawl_restaurant("ê°•ë‚¨ ë§›ì§‘")
        
        print(f"ê²°ê³¼:")
        print(f"  ì„±ê³µ: {result.success}")
        print(f"  ì‹ë‹¹ ìˆ˜: {result.restaurant_count}")
        print(f"  ë©”ë‰´ ìˆ˜: {result.menu_count}")
        print(f"  ì¤‘ë³µ ìˆ˜: {result.duplicate_count}")
        print(f"  ì—ëŸ¬ ìˆ˜: {result.error_count}")
        print(f"  ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
        
        if result.errors:
            print(f"  ì—ëŸ¬:")
            for error in result.errors[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                print(f"    - {error}")
    
    asyncio.run(test_crawler())