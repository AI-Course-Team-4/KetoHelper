"""
ğŸ•·ï¸ CrawlJob ë°ì´í„° ëª¨ë¸
- í¬ë¡¤ë§ ì‘ì—… ê´€ë¦¬ ë° ìƒíƒœ ì¶”ì 
- ë¹„ë™ê¸° ì‘ì—… í ì§€ì›
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
"""

from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from uuid import UUID, uuid4
from enum import Enum

from pydantic import BaseModel, Field, validator, root_validator
from pydantic.types import conint, constr


class JobStatus(str, Enum):
    """ì‘ì—… ìƒíƒœ"""
    QUEUED = "queued"      # ëŒ€ê¸°ì¤‘
    RUNNING = "running"    # ì‹¤í–‰ì¤‘
    COMPLETED = "completed"  # ì™„ë£Œ
    FAILED = "failed"      # ì‹¤íŒ¨
    CANCELLED = "cancelled"  # ì·¨ì†Œë¨


class JobType(str, Enum):
    """ì‘ì—… ìœ í˜•"""
    SEARCH = "search"      # ê²€ìƒ‰
    DETAIL = "detail"      # ìƒì„¸ ì •ë³´
    BATCH = "batch"        # ë°°ì¹˜ ì‘ì—…


class ErrorCode(str, Enum):
    """ì—ëŸ¬ ì½”ë“œ"""
    NETWORK_ERROR = "network_error"
    TIMEOUT_ERROR = "timeout_error"
    PARSE_ERROR = "parse_error"
    BLOCKED_ERROR = "blocked_error"
    RATE_LIMIT_ERROR = "rate_limit_error"
    VALIDATION_ERROR = "validation_error"
    SYSTEM_ERROR = "system_error"


class CrawlJobBase(BaseModel):
    """CrawlJob ê¸°ë³¸ ëª¨ë¸"""
    
    # ğŸ¯ ì‘ì—… ì •ë³´
    job_type: JobType = Field(..., description="ì‘ì—… ìœ í˜•")
    site: str = Field(..., max_length=20, description="ëŒ€ìƒ ì‚¬ì´íŠ¸")
    url: str = Field(..., max_length=1000, description="í¬ë¡¤ë§ ëŒ€ìƒ URL")
    keyword: Optional[str] = Field(None, max_length=100, description="ê²€ìƒ‰ í‚¤ì›Œë“œ")
    
    # ğŸ“Š ì‘ì—… ìƒíƒœ
    status: JobStatus = Field(JobStatus.QUEUED, description="ì‘ì—… ìƒíƒœ")
    priority: int = Field(0, description="ìš°ì„ ìˆœìœ„ (ë†’ì„ìˆ˜ë¡ ë¨¼ì € ì²˜ë¦¬)")
    attempts: int = Field(0, description="ì‹œë„ íšŸìˆ˜")
    max_attempts: int = Field(3, description="ìµœëŒ€ ì‹œë„ íšŸìˆ˜")
    
    # âŒ ì˜¤ë¥˜ ì •ë³´
    last_error_code: Optional[ErrorCode] = Field(None, description="ë§ˆì§€ë§‰ ì˜¤ë¥˜ ì½”ë“œ")
    last_error_message: Optional[str] = Field(None, max_length=1000, description="ë§ˆì§€ë§‰ ì˜¤ë¥˜ ë©”ì‹œì§€")
    last_error_at: Optional[datetime] = Field(None, description="ë§ˆì§€ë§‰ ì˜¤ë¥˜ ë°œìƒ ì‹œê°„")
    
    # â±ï¸ ì‹œê°„ ì •ë³´
    scheduled_at: datetime = Field(default_factory=datetime.utcnow, description="ì˜ˆì•½ ì‹œê°„")
    started_at: Optional[datetime] = Field(None, description="ì‹œì‘ ì‹œê°„")
    completed_at: Optional[datetime] = Field(None, description="ì™„ë£Œ ì‹œê°„")
    
    # ğŸ”§ ì¶”ê°€ ì„¤ì •
    config: Optional[Dict[str, Any]] = Field(None, description="ì‘ì—…ë³„ ì„¤ì •")
    metadata: Optional[Dict[str, Any]] = Field(None, description="ë©”íƒ€ë°ì´í„°")
    
    class Config:
        use_enum_values = True
        validate_assignment = True
        str_strip_whitespace = True


class CrawlJobCreate(CrawlJobBase):
    """CrawlJob ìƒì„± ëª¨ë¸"""
    pass


class CrawlJobUpdate(BaseModel):
    """CrawlJob ì—…ë°ì´íŠ¸ ëª¨ë¸"""
    status: Optional[JobStatus] = None
    priority: Optional[int] = None
    attempts: Optional[int] = None
    last_error_code: Optional[ErrorCode] = None
    last_error_message: Optional[str] = Field(None, max_length=1000)
    last_error_at: Optional[datetime] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    config: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


class CrawlJob(CrawlJobBase):
    """CrawlJob ì™„ì „ ëª¨ë¸ (DB í¬í•¨)"""
    
    # ğŸ”‘ ì‹œìŠ¤í…œ í•„ë“œ
    id: UUID = Field(default_factory=uuid4, description="ê³ ìœ  ì‹ë³„ì")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="ìƒì„± ì‹œê°„")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="ìˆ˜ì • ì‹œê°„")
    
    class Config(CrawlJobBase.Config):
        orm_mode = True
    
    @validator('site')
    def validate_site(cls, v):
        """ì‚¬ì´íŠ¸ëª… ê²€ì¦"""
        allowed_sites = ['siksin', 'diningcode', 'mangoplate']
        if v not in allowed_sites:
            raise ValueError(f'ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤: {v}')
        return v
    
    @validator('url')
    def validate_url(cls, v):
        """URL ê²€ì¦"""
        import re
        url_pattern = r'^https?://.+'
        if not re.match(url_pattern, v):
            raise ValueError('ì˜¬ë°”ë¥¸ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤')
        return v
    
    @validator('priority')
    def validate_priority(cls, v):
        """ìš°ì„ ìˆœìœ„ ê²€ì¦"""
        if v < -100 or v > 100:
            raise ValueError('ìš°ì„ ìˆœìœ„ëŠ” -100~100 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤')
        return v
    
    @validator('attempts')
    def validate_attempts(cls, v, values):
        """ì‹œë„ íšŸìˆ˜ ê²€ì¦"""
        max_attempts = values.get('max_attempts', 3)
        if v > max_attempts:
            raise ValueError(f'ì‹œë„ íšŸìˆ˜ê°€ ìµœëŒ€ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {v} > {max_attempts}')
        return v
    
    @root_validator
    def validate_times(cls, values):
        """ì‹œê°„ ê´€ë ¨ ê²€ì¦"""
        scheduled_at = values.get('scheduled_at')
        started_at = values.get('started_at')
        completed_at = values.get('completed_at')
        
        if started_at and scheduled_at and started_at < scheduled_at:
            raise ValueError('ì‹œì‘ ì‹œê°„ì´ ì˜ˆì•½ ì‹œê°„ë³´ë‹¤ ë¹ ë¥¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
        
        if completed_at and started_at and completed_at < started_at:
            raise ValueError('ì™„ë£Œ ì‹œê°„ì´ ì‹œì‘ ì‹œê°„ë³´ë‹¤ ë¹ ë¥¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
        
        return values
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return self.dict(exclude_none=True)
    
    def is_pending(self) -> bool:
        """ëŒ€ê¸°ì¤‘ ì—¬ë¶€"""
        return self.status == JobStatus.QUEUED
    
    def is_running(self) -> bool:
        """ì‹¤í–‰ì¤‘ ì—¬ë¶€"""
        return self.status == JobStatus.RUNNING
    
    def is_completed(self) -> bool:
        """ì™„ë£Œ ì—¬ë¶€"""
        return self.status == JobStatus.COMPLETED
    
    def is_failed(self) -> bool:
        """ì‹¤íŒ¨ ì—¬ë¶€"""
        return self.status == JobStatus.FAILED
    
    def can_retry(self) -> bool:
        """ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€"""
        return (self.is_failed() and 
                self.attempts < self.max_attempts and
                self.last_error_code != ErrorCode.VALIDATION_ERROR)
    
    def get_duration(self) -> Optional[timedelta]:
        """ì‹¤í–‰ ì‹œê°„ ê³„ì‚°"""
        if self.started_at and self.completed_at:
            return self.completed_at - self.started_at
        elif self.started_at:
            return datetime.utcnow() - self.started_at
        return None
    
    def get_wait_time(self) -> Optional[timedelta]:
        """ëŒ€ê¸° ì‹œê°„ ê³„ì‚°"""
        if self.started_at:
            return self.started_at - self.scheduled_at
        else:
            return datetime.utcnow() - self.scheduled_at
    
    def calculate_next_retry_time(self) -> datetime:
        """ë‹¤ìŒ ì¬ì‹œë„ ì‹œê°„ ê³„ì‚° (ì§€ìˆ˜ ë°±ì˜¤í”„)"""
        base_delay = 60  # 1ë¶„
        max_delay = 3600  # 1ì‹œê°„
        
        delay = min(base_delay * (2 ** self.attempts), max_delay)
        return datetime.utcnow() + timedelta(seconds=delay)
    
    def get_display_status(self) -> str:
        """ìƒíƒœ í‘œì‹œ ë¬¸ìì—´"""
        status_display = {
            JobStatus.QUEUED: "â³ ëŒ€ê¸°ì¤‘",
            JobStatus.RUNNING: "ğŸ”„ ì‹¤í–‰ì¤‘",
            JobStatus.COMPLETED: "âœ… ì™„ë£Œ",
            JobStatus.FAILED: "âŒ ì‹¤íŒ¨",
            JobStatus.CANCELLED: "ğŸš« ì·¨ì†Œë¨"
        }
        return status_display.get(self.status, str(self.status))
    
    def get_progress_info(self) -> Dict[str, Any]:
        """ì§„í–‰ ì •ë³´ ë°˜í™˜"""
        duration = self.get_duration()
        wait_time = self.get_wait_time()
        
        return {
            "id": str(self.id),
            "status": self.get_display_status(),
            "job_type": self.job_type.value,
            "site": self.site,
            "keyword": self.keyword,
            "attempts": f"{self.attempts}/{self.max_attempts}",
            "duration": str(duration) if duration else None,
            "wait_time": str(wait_time) if wait_time else None,
            "last_error": self.last_error_message,
            "scheduled_at": self.scheduled_at.isoformat(),
            "can_retry": self.can_retry()
        }


class CrawlJobSearch(BaseModel):
    """CrawlJob ê²€ìƒ‰ ëª¨ë¸"""
    
    job_type: Optional[JobType] = Field(None, description="ì‘ì—… ìœ í˜• í•„í„°")
    site: Optional[str] = Field(None, description="ì‚¬ì´íŠ¸ í•„í„°")
    status: Optional[JobStatus] = Field(None, description="ìƒíƒœ í•„í„°")
    keyword: Optional[str] = Field(None, description="í‚¤ì›Œë“œ í•„í„°")
    
    # ì‹œê°„ ë²”ìœ„
    scheduled_after: Optional[datetime] = Field(None, description="ì˜ˆì•½ ì‹œê°„ ì´í›„")
    scheduled_before: Optional[datetime] = Field(None, description="ì˜ˆì•½ ì‹œê°„ ì´ì „")
    
    # íŠ¹ì„± í•„í„°
    has_errors: Optional[bool] = Field(None, description="ì—ëŸ¬ ë³´ìœ  ì—¬ë¶€")
    can_retry: Optional[bool] = Field(None, description="ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€")
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    page: int = Field(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸")
    size: int = Field(20, ge=1, le=100, description="í˜ì´ì§€ í¬ê¸°")
    
    # ì •ë ¬
    sort_by: str = Field("scheduled_at", description="ì •ë ¬ ê¸°ì¤€")
    sort_desc: bool = Field(True, description="ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬")


class CrawlJobStats(BaseModel):
    """CrawlJob í†µê³„ ëª¨ë¸"""
    
    total_count: int = Field(0, description="ì „ì²´ ì‘ì—… ìˆ˜")
    by_status: Dict[str, int] = Field(default_factory=dict, description="ìƒíƒœë³„ ê°œìˆ˜")
    by_site: Dict[str, int] = Field(default_factory=dict, description="ì‚¬ì´íŠ¸ë³„ ê°œìˆ˜")
    by_job_type: Dict[str, int] = Field(default_factory=dict, description="ì‘ì—… ìœ í˜•ë³„ ê°œìˆ˜")
    by_error_code: Dict[str, int] = Field(default_factory=dict, description="ì—ëŸ¬ ì½”ë“œë³„ ê°œìˆ˜")
    
    success_rate: float = Field(0.0, description="ì„±ê³µë¥ ")
    avg_duration: Optional[float] = Field(None, description="í‰ê·  ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    avg_wait_time: Optional[float] = Field(None, description="í‰ê·  ëŒ€ê¸° ì‹œê°„ (ì´ˆ)")
    
    pending_count: int = Field(0, description="ëŒ€ê¸°ì¤‘ ì‘ì—… ìˆ˜")
    running_count: int = Field(0, description="ì‹¤í–‰ì¤‘ ì‘ì—… ìˆ˜")
    failed_count: int = Field(0, description="ì‹¤íŒ¨ ì‘ì—… ìˆ˜")
    retryable_count: int = Field(0, description="ì¬ì‹œë„ ê°€ëŠ¥ ì‘ì—… ìˆ˜")
    
    last_updated: datetime = Field(default_factory=datetime.utcnow, description="ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸")


class CrawlResult(BaseModel):
    """í¬ë¡¤ë§ ê²°ê³¼ ëª¨ë¸"""
    
    job_id: UUID = Field(..., description="ì‘ì—… ID")
    success: bool = Field(..., description="ì„±ê³µ ì—¬ë¶€")
    
    # ìˆ˜ì§‘ ê²°ê³¼
    restaurants_found: int = Field(0, description="ë°œê²¬ëœ ì‹ë‹¹ ìˆ˜")
    restaurants_saved: int = Field(0, description="ì €ì¥ëœ ì‹ë‹¹ ìˆ˜")
    menus_found: int = Field(0, description="ë°œê²¬ëœ ë©”ë‰´ ìˆ˜")
    menus_saved: int = Field(0, description="ì €ì¥ëœ ë©”ë‰´ ìˆ˜")
    
    # ì²˜ë¦¬ ì •ë³´
    pages_crawled: int = Field(0, description="í¬ë¡¤ë§í•œ í˜ì´ì§€ ìˆ˜")
    duration: float = Field(0.0, description="ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    
    # ì—ëŸ¬ ì •ë³´
    errors: List[str] = Field(default_factory=list, description="ë°œìƒí•œ ì—ëŸ¬ ëª©ë¡")
    warnings: List[str] = Field(default_factory=list, description="ê²½ê³  ëª©ë¡")
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ì •ë³´")
    
    def get_summary(self) -> str:
        """ê²°ê³¼ ìš”ì•½"""
        if self.success:
            return (f"âœ… ì„±ê³µ: ì‹ë‹¹ {self.restaurants_saved}ê°œ, "
                   f"ë©”ë‰´ {self.menus_saved}ê°œ ì €ì¥ "
                   f"({self.duration:.1f}ì´ˆ)")
        else:
            error_count = len(self.errors)
            return f"âŒ ì‹¤íŒ¨: {error_count}ê°œ ì—ëŸ¬ ({self.duration:.1f}ì´ˆ)"


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_search_job(site: str, keyword: str, priority: int = 0) -> CrawlJobCreate:
    """ê²€ìƒ‰ ì‘ì—… ìƒì„±"""
    # URLì€ ì‚¬ì´íŠ¸ë³„ë¡œ ë‹¤ë¥´ê²Œ ìƒì„±í•´ì•¼ í•¨ (ì‹¤ì œ êµ¬í˜„ì—ì„œ)
    url = f"https://{site}.com/search?q={keyword}"
    
    return CrawlJobCreate(
        job_type=JobType.SEARCH,
        site=site,
        url=url,
        keyword=keyword,
        priority=priority
    )


def create_detail_job(site: str, url: str, priority: int = 0) -> CrawlJobCreate:
    """ìƒì„¸ ì •ë³´ ì‘ì—… ìƒì„±"""
    return CrawlJobCreate(
        job_type=JobType.DETAIL,
        site=site,
        url=url,
        priority=priority
    )


def create_batch_job(site: str, urls: List[str], priority: int = 0) -> List[CrawlJobCreate]:
    """ë°°ì¹˜ ì‘ì—… ìƒì„±"""
    jobs = []
    for i, url in enumerate(urls):
        job = CrawlJobCreate(
            job_type=JobType.BATCH,
            site=site,
            url=url,
            priority=priority - i  # ìˆœì„œëŒ€ë¡œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        )
        jobs.append(job)
    
    return jobs


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    
    # ê²€ìƒ‰ ì‘ì—… ìƒì„±
    search_job_data = {
        "job_type": JobType.SEARCH,
        "site": "siksin",
        "url": "https://siksin.com/search?q=ê°•ë‚¨ë§›ì§‘",
        "keyword": "ê°•ë‚¨ë§›ì§‘",
        "priority": 10
    }
    
    try:
        job = CrawlJob(**search_job_data)
        print(f"âœ… CrawlJob ìƒì„± ì„±ê³µ: {job.id}")
        print(f"   í‘œì‹œ ìƒíƒœ: {job.get_display_status()}")
        print(f"   ëŒ€ê¸°ì¤‘ ì—¬ë¶€: {job.is_pending()}")
        print(f"   ì¬ì‹œë„ ê°€ëŠ¥: {job.can_retry()}")
        print(f"   ëŒ€ê¸° ì‹œê°„: {job.get_wait_time()}")
        
        # ì§„í–‰ ì •ë³´ í…ŒìŠ¤íŠ¸
        progress = job.get_progress_info()
        print(f"   ì§„í–‰ ì •ë³´: {progress}")
        
        # í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        search_job = create_search_job("siksin", "ê°•ë‚¨ë§›ì§‘", 5)
        print(f"   ê²€ìƒ‰ ì‘ì—… ìƒì„±: {search_job.keyword}")
        
        # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        json_data = job.json()
        print(f"   JSON í¬ê¸°: {len(json_data)}ë°”ì´íŠ¸")
        
    except Exception as e:
        print(f"âŒ CrawlJob ìƒì„± ì‹¤íŒ¨: {e}")