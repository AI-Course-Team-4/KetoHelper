"""
ğŸ“ ë¡œê¹… ì„¤ì • ëª¨ë“ˆ
- loguru ê¸°ë°˜ ê³ ê¸‰ ë¡œê¹…
- íŒŒì¼ë³„, ë ˆë²¨ë³„ ë¡œê¹…
- ìë™ ë¡œí…Œì´ì…˜ ë° ì••ì¶•
"""

import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any
from loguru import logger

from .config_loader import get_config


class CrawlerLogger:
    """í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì „ìš© ë¡œê±°"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = get_config(config_path)
        self.is_initialized = False
        self._setup_logger()
    
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        if self.is_initialized:
            return
            
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        logger.remove()
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self._ensure_log_directories()
        
        # ì½˜ì†” ë¡œê±° ì„¤ì •
        self._setup_console_logger()
        
        # íŒŒì¼ ë¡œê±° ì„¤ì •
        self._setup_file_loggers()
        
        self.is_initialized = True
        logger.info("ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _ensure_log_directories(self):
        """ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
        log_files = self.config.logging.files
        
        for log_file in log_files.values():
            log_path = Path(log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
    
    def _setup_console_logger(self):
        """ì½˜ì†” ë¡œê±° ì„¤ì •"""
        console_format = (
            "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
            "<level>{message}</level>"
        )
        
        logger.add(
            sys.stdout,
            format=console_format,
            level=self.config.logging.level,
            colorize=True,
            backtrace=True,
            diagnose=True,
            filter=self._filter_console_logs
        )
    
    def _setup_file_loggers(self):
        """íŒŒì¼ ë¡œê±° ì„¤ì •"""
        log_config = self.config.logging
        
        # ë©”ì¸ ë¡œê·¸ íŒŒì¼ (ëª¨ë“  ë ˆë²¨)
        logger.add(
            log_config.files["main"],
            format=log_config.format,
            level="DEBUG",
            rotation=log_config.rotation["size"],
            retention=log_config.rotation["retention"],
            compression="zip",
            encoding="utf-8",
            backtrace=True,
            diagnose=True
        )
        
        # ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ (ERROR ì´ìƒë§Œ)
        logger.add(
            log_config.files["error"],
            format=log_config.format,
            level="ERROR",
            rotation=log_config.rotation["size"],
            retention=log_config.rotation["retention"],
            compression="zip",
            encoding="utf-8",
            backtrace=True,
            diagnose=True
        )
        
        # ì•¡ì„¸ìŠ¤ ë¡œê·¸ íŒŒì¼ (HTTP ìš”ì²­ ê´€ë ¨)
        logger.add(
            log_config.files["access"],
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
            level="INFO",
            rotation="1 day",
            retention="30 days",
            compression="zip",
            encoding="utf-8",
            filter=lambda record: record["extra"].get("type") == "access"
        )
    
    def _filter_console_logs(self, record) -> bool:
        """ì½˜ì†” ë¡œê·¸ í•„í„°ë§"""
        # ì•¡ì„¸ìŠ¤ ë¡œê·¸ëŠ” ì½˜ì†”ì— ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        if record["extra"].get("type") == "access":
            return False
        
        # ë””ë²„ê·¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ DEBUG ë¡œê·¸ ìˆ¨ê¹€
        if not self.config.is_debug() and record["level"].name == "DEBUG":
            return False
            
        return True
    
    def get_logger(self, name: str = "crawler") -> Any:
        """ì´ë¦„ì´ ì§€ì •ëœ ë¡œê±° ë°˜í™˜"""
        return logger.bind(name=name)
    
    def log_request(self, method: str, url: str, status_code: int, 
                   response_time: float, site: str = "unknown"):
        """HTTP ìš”ì²­ ë¡œê¹…"""
        logger.bind(type="access").info(
            f"{method} {url} - {status_code} - {response_time:.2f}s - {site}"
        )
    
    def log_crawl_start(self, site: str, keyword: str, job_id: str):
        """í¬ë¡¤ë§ ì‹œì‘ ë¡œê¹…"""
        logger.bind(name="crawler").info(
            f"í¬ë¡¤ë§ ì‹œì‘: {site} | í‚¤ì›Œë“œ: {keyword} | Job ID: {job_id}"
        )
    
    def log_crawl_complete(self, site: str, restaurants_found: int, 
                          menus_found: int, duration: float, job_id: str):
        """í¬ë¡¤ë§ ì™„ë£Œ ë¡œê¹…"""
        logger.bind(name="crawler").info(
            f"í¬ë¡¤ë§ ì™„ë£Œ: {site} | ì‹ë‹¹: {restaurants_found}ê°œ | "
            f"ë©”ë‰´: {menus_found}ê°œ | ì†Œìš”ì‹œê°„: {duration:.2f}s | Job ID: {job_id}"
        )
    
    def log_crawl_error(self, site: str, error: str, url: str = "", job_id: str = ""):
        """í¬ë¡¤ë§ ì—ëŸ¬ ë¡œê¹…"""
        logger.bind(name="crawler").error(
            f"í¬ë¡¤ë§ ì—ëŸ¬: {site} | URL: {url} | ì—ëŸ¬: {error} | Job ID: {job_id}"
        )
    
    def log_parser_error(self, site: str, selector: str, url: str, error: str):
        """íŒŒì‹± ì—ëŸ¬ ë¡œê¹…"""
        logger.bind(name="parser").error(
            f"íŒŒì‹± ì—ëŸ¬: {site} | ì…€ë ‰í„°: {selector} | URL: {url} | ì—ëŸ¬: {error}"
        )
    
    def log_blocking_detected(self, site: str, url: str, indicator: str):
        """ì°¨ë‹¨ ê°ì§€ ë¡œê¹…"""
        logger.bind(name="security").warning(
            f"ì°¨ë‹¨ ê°ì§€: {site} | URL: {url} | ì§€í‘œ: {indicator}"
        )
    
    def log_rate_limit(self, site: str, current_qps: float, new_qps: float):
        """ì†ë„ ì œí•œ ë¡œê¹…"""
        logger.bind(name="rate_limit").warning(
            f"ì†ë„ ì œí•œ ì ìš©: {site} | {current_qps:.2f} -> {new_qps:.2f} QPS"
        )
    
    def log_database_operation(self, operation: str, table: str, 
                              record_count: int = 1, duration: float = 0):
        """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ë¡œê¹…"""
        logger.bind(name="database").debug(
            f"DB ì‘ì—…: {operation} | í…Œì´ë¸”: {table} | "
            f"ë ˆì½”ë“œ: {record_count}ê°œ | ì†Œìš”ì‹œê°„: {duration:.3f}s"
        )
    
    def log_validation_error(self, data_type: str, field: str, value: Any, error: str):
        """ë°ì´í„° ê²€ì¦ ì—ëŸ¬ ë¡œê¹…"""
        logger.bind(name="validator").warning(
            f"ê²€ì¦ ì‹¤íŒ¨: {data_type}.{field} = {value} | ì—ëŸ¬: {error}"
        )
    
    def log_duplicate_found(self, data_type: str, key: str, source1: str, source2: str):
        """ì¤‘ë³µ ë°ì´í„° ë°œê²¬ ë¡œê¹…"""
        logger.bind(name="deduplicator").info(
            f"ì¤‘ë³µ ë°œê²¬: {data_type} | í‚¤: {key} | ì†ŒìŠ¤: {source1} vs {source2}"
        )
    
    def log_performance(self, operation: str, duration: float, 
                       throughput: float = 0, unit: str = "ops/s"):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        logger.bind(name="performance").info(
            f"ì„±ëŠ¥: {operation} | ì†Œìš”ì‹œê°„: {duration:.2f}s | "
            f"ì²˜ë¦¬ëŸ‰: {throughput:.2f} {unit}"
        )


# ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
_crawler_logger = None


def get_logger(name: str = "crawler") -> Any:
    """ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _crawler_logger
    
    if _crawler_logger is None:
        _crawler_logger = CrawlerLogger()
    
    return _crawler_logger.get_logger(name)


def setup_logging(config_path: Optional[str] = None):
    """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    global _crawler_logger
    _crawler_logger = CrawlerLogger(config_path)


# í¸ì˜ í•¨ìˆ˜ë“¤
def log_request(method: str, url: str, status_code: int, 
                response_time: float, site: str = "unknown"):
    """HTTP ìš”ì²­ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_request(method, url, status_code, response_time, site)


def log_crawl_start(site: str, keyword: str, job_id: str):
    """í¬ë¡¤ë§ ì‹œì‘ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_crawl_start(site, keyword, job_id)


def log_crawl_complete(site: str, restaurants_found: int, 
                      menus_found: int, duration: float, job_id: str):
    """í¬ë¡¤ë§ ì™„ë£Œ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_crawl_complete(
            site, restaurants_found, menus_found, duration, job_id
        )


def log_crawl_error(site: str, error: str, url: str = "", job_id: str = ""):
    """í¬ë¡¤ë§ ì—ëŸ¬ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_crawl_error(site, error, url, job_id)


def log_parser_error(site: str, selector: str, url: str, error: str):
    """íŒŒì‹± ì—ëŸ¬ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_parser_error(site, selector, url, error)


def log_blocking_detected(site: str, url: str, indicator: str):
    """ì°¨ë‹¨ ê°ì§€ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_blocking_detected(site, url, indicator)


def log_rate_limit(site: str, current_qps: float, new_qps: float):
    """ì†ë„ ì œí•œ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_rate_limit(site, current_qps, new_qps)


def log_database_operation(operation: str, table: str, 
                          record_count: int = 1, duration: float = 0):
    """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_database_operation(operation, table, record_count, duration)


def log_validation_error(data_type: str, field: str, value: Any, error: str):
    """ë°ì´í„° ê²€ì¦ ì—ëŸ¬ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_validation_error(data_type, field, value, error)


def log_duplicate_found(data_type: str, key: str, source1: str, source2: str):
    """ì¤‘ë³µ ë°ì´í„° ë°œê²¬ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_duplicate_found(data_type, key, source1, source2)


def log_performance(operation: str, duration: float, 
                   throughput: float = 0, unit: str = "ops/s"):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…"""
    global _crawler_logger
    if _crawler_logger:
        _crawler_logger.log_performance(operation, duration, throughput, unit)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    setup_logging()
    
    logger = get_logger("test")
    
    logger.info("í…ŒìŠ¤íŠ¸ ë¡œê·¸ ë©”ì‹œì§€")
    logger.debug("ë””ë²„ê·¸ ë©”ì‹œì§€")
    logger.warning("ê²½ê³  ë©”ì‹œì§€")
    logger.error("ì—ëŸ¬ ë©”ì‹œì§€")
    
    # ì „ìš© ë¡œê¹… í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    log_request("GET", "https://example.com", 200, 1.5, "test_site")
    log_crawl_start("test_site", "í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ", "job-123")
    log_crawl_complete("test_site", 5, 25, 10.5, "job-123")
    
    print("ë¡œê¹… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")