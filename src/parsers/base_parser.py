"""
ğŸ”Œ ê¸°ë³¸ íŒŒì„œ ì¸í„°í˜ì´ìŠ¤
- ì¶”ìƒ í´ë˜ìŠ¤ë¡œ íŒŒì„œ í‘œì¤€ ì •ì˜
- í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜ ì§€ì›
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
"""

import re
import json
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass

from ..models import Restaurant, Menu, RestaurantCreate, MenuCreate
from ..utils.config_loader import get_config
from ..utils.logger import get_logger, log_parser_error
from ..utils.text_utils import (
    normalize_text, normalize_restaurant_name, normalize_address,
    extract_phone_number, extract_price, extract_coordinates, extract_rating
)
from ..utils.http_client import HttpClient


@dataclass
class ParseResult:
    """íŒŒì‹± ê²°ê³¼"""
    success: bool
    data: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None
    error: Optional[str] = None
    warnings: List[str] = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []
        if self.metadata is None:
            self.metadata = {}


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    restaurants: List[RestaurantCreate]
    total_found: int
    page: int
    has_next_page: bool
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class BaseParser(ABC):
    """ê¸°ë³¸ íŒŒì„œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, site: str, http_client: Optional[HttpClient] = None):
        self.site = site
        self.config = get_config()
        self.logger = get_logger(f"parser_{site}")
        
        # HTTP í´ë¼ì´ì–¸íŠ¸
        self.http_client = http_client
        
        # íŒŒì„œ ì„¤ì • ë¡œë“œ
        try:
            self.parser_config = self.config.get_parser_config(site)
            self.site_config = self.parser_config.get('site', {})
            self.search_config = self.parser_config.get('search', {})
            self.detail_config = self.parser_config.get('restaurant_detail', {})
            self.extraction_config = self.parser_config.get('extraction', {})
            self.validation_config = self.parser_config.get('validation', {})
            
        except Exception as e:
            self.logger.error(f"íŒŒì„œ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        # í†µê³„
        self.stats = {
            "requests_made": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "restaurants_parsed": 0,
            "menus_parsed": 0,
            "parsing_errors": 0
        }
    
    @abstractmethod
    async def search(self, keyword: str, page: int = 1) -> SearchResult:
        """ì‹ë‹¹ ê²€ìƒ‰"""
        pass
    
    @abstractmethod
    async def parse_restaurant_detail(self, url: str) -> ParseResult:
        """ì‹ë‹¹ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        pass
    
    @abstractmethod
    def extract_restaurant_data(self, content: str, url: str) -> Dict[str, Any]:
        """HTMLì—ì„œ ì‹ë‹¹ ë°ì´í„° ì¶”ì¶œ"""
        pass
    
    @abstractmethod
    def extract_menu_data(self, content: str, url: str) -> List[Dict[str, Any]]:
        """HTMLì—ì„œ ë©”ë‰´ ë°ì´í„° ì¶”ì¶œ"""
        pass
    
    # ê³µí†µ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    
    def extract_text_by_selector(self, content: str, selectors: Union[str, List[str]], 
                                default: str = "") -> str:
        """CSS ì…€ë ‰í„°ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
        if isinstance(selectors, str):
            selectors = [selectors]
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    text = elements[0].get_text(strip=True)
                    if text:
                        return normalize_text(text)
            except Exception as e:
                self.logger.debug(f"ì…€ë ‰í„° ì‹¤íŒ¨: {selector} - {e}")
                continue
        
        return default
    
    def extract_attribute_by_selector(self, content: str, selector: str, 
                                     attribute: str, default: str = "") -> str:
        """CSS ì…€ë ‰í„°ë¡œ ì†ì„±ê°’ ì¶”ì¶œ"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(content, 'html.parser')
        
        try:
            elements = soup.select(selector)
            if elements:
                attr_value = elements[0].get(attribute, "")
                if attr_value:
                    return normalize_text(str(attr_value))
        except Exception as e:
            self.logger.debug(f"ì†ì„± ì¶”ì¶œ ì‹¤íŒ¨: {selector}[{attribute}] - {e}")
        
        return default
    
    def extract_multiple_by_selector(self, content: str, container_selector: str, 
                                   item_selector: str) -> List[str]:
        """ì—¬ëŸ¬ ìš”ì†Œ ì¶”ì¶œ (ëª©ë¡)"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(content, 'html.parser')
        results = []
        
        try:
            containers = soup.select(container_selector)
            for container in containers:
                items = container.select(item_selector)
                for item in items:
                    text = item.get_text(strip=True)
                    if text:
                        results.append(normalize_text(text))
        except Exception as e:
            self.logger.debug(f"ë‹¤ì¤‘ ì¶”ì¶œ ì‹¤íŒ¨: {container_selector} > {item_selector} - {e}")
        
        return results
    
    def extract_coordinates_from_content(self, content: str) -> tuple[Optional[float], Optional[float]]:
        """ì½˜í…ì¸ ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ"""
        # ì„¤ì •ì—ì„œ ì¢Œí‘œ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
        patterns = self.extraction_config.get('coordinate_patterns', [])
        
        lat, lng = extract_coordinates(content)
        
        # ì¶”ê°€ íŒ¨í„´ ì‹œë„
        for pattern in patterns:
            if lat and lng:
                break
                
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                try:
                    value = float(match)
                    if 'lat' in pattern.lower():
                        lat = value
                    elif 'lng' in pattern.lower() or 'lon' in pattern.lower():
                        lng = value
                except ValueError:
                    continue
        
        return lat, lng
    
    def normalize_restaurant_data(self, raw_data: Dict[str, Any], url: str) -> Dict[str, Any]:
        """ì‹ë‹¹ ë°ì´í„° ì •ê·œí™”"""
        normalized = {
            'source': self.site,
            'source_url': url,
        }
        
        # í•„ìˆ˜ í•„ë“œ ì²˜ë¦¬
        if 'name' in raw_data:
            normalized['name'] = normalize_restaurant_name(raw_data['name'])
        
        # ì£¼ì†Œ ì •ê·œí™”
        if 'address_road' in raw_data:
            normalized['address_road'] = normalize_address(raw_data['address_road'])
        
        if 'address_jibun' in raw_data:
            normalized['address_jibun'] = normalize_address(raw_data['address_jibun'])
        
        # ì „í™”ë²ˆí˜¸ ì •ê·œí™”
        if 'phone' in raw_data:
            phone = extract_phone_number(raw_data['phone'])
            if phone:
                normalized['phone'] = phone
        
        # ì¢Œí‘œ ì²˜ë¦¬
        if 'lat' in raw_data and 'lng' in raw_data:
            try:
                lat = float(raw_data['lat']) if raw_data['lat'] else None
                lng = float(raw_data['lng']) if raw_data['lng'] else None
                
                if lat and lng and self.config.validation.geo_bounds:
                    bounds = self.config.validation.geo_bounds
                    if (bounds['lat_min'] <= lat <= bounds['lat_max'] and
                        bounds['lng_min'] <= lng <= bounds['lng_max']):
                        normalized['lat'] = lat
                        normalized['lng'] = lng
            except (ValueError, TypeError):
                pass
        
        # í‰ì  ì²˜ë¦¬
        if 'rating' in raw_data:
            rating = extract_rating(str(raw_data['rating']))
            if rating and 0 <= rating <= 5:
                normalized['rating'] = rating
        
        # ê¸°íƒ€ í•„ë“œ ë³µì‚¬
        for field in ['category', 'cuisine_type', 'homepage_url', 'business_hours']:
            if field in raw_data and raw_data[field]:
                normalized[field] = normalize_text(str(raw_data[field]))
        
        # ìˆ«ì í•„ë“œ ì²˜ë¦¬
        for field in ['review_count']:
            if field in raw_data:
                try:
                    value = int(raw_data[field])
                    if value >= 0:
                        normalized[field] = value
                except (ValueError, TypeError):
                    pass
        
        return normalized
    
    def normalize_menu_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”ë‰´ ë°ì´í„° ì •ê·œí™”"""
        normalized = {}
        
        # í•„ìˆ˜ í•„ë“œ
        if 'name' in raw_data:
            normalized['name'] = normalize_text(raw_data['name'])
        
        # ê°€ê²© ì²˜ë¦¬
        if 'price' in raw_data:
            price = extract_price(str(raw_data['price']))
            if price and self.config.validation.price_bounds:
                bounds = self.config.validation.price_bounds
                if bounds['min'] <= price <= bounds['max']:
                    normalized['price'] = price
        
        # ê¸°íƒ€ í•„ë“œ
        for field in ['description', 'category', 'image_url']:
            if field in raw_data and raw_data[field]:
                normalized[field] = normalize_text(str(raw_data[field]))
        
        # ë¶ˆë¦° í•„ë“œ
        for field in ['is_signature', 'is_recommended']:
            if field in raw_data:
                normalized[field] = bool(raw_data[field])
        
        return normalized
    
    def validate_restaurant_data(self, data: Dict[str, Any]) -> List[str]:
        """ì‹ë‹¹ ë°ì´í„° ê²€ì¦"""
        errors = []
        required_fields = self.validation_config.get('required_fields', {}).get('restaurant', [])
        
        for field in required_fields:
            if field not in data or not data[field]:
                errors.append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        # í˜•ì‹ ê²€ì¦
        formats = self.validation_config.get('formats', {})
        
        if 'phone' in data and data['phone'] and 'phone' in formats:
            pattern = formats['phone']
            if not re.match(pattern, data['phone']):
                errors.append(f"ì „í™”ë²ˆí˜¸ í˜•ì‹ ì˜¤ë¥˜: {data['phone']}")
        
        return errors
    
    def validate_menu_data(self, data: Dict[str, Any]) -> List[str]:
        """ë©”ë‰´ ë°ì´í„° ê²€ì¦"""
        errors = []
        required_fields = self.validation_config.get('required_fields', {}).get('menu', [])
        
        for field in required_fields:
            if field not in data or not data[field]:
                errors.append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        return errors
    
    def detect_blocking(self, content: str, url: str) -> Optional[str]:
        """ì°¨ë‹¨ ê°ì§€"""
        indicators = self.parser_config.get('blocking_detection', {}).get('indicators', [])
        
        content_lower = content.lower()
        for indicator in indicators:
            if indicator.lower() in content_lower:
                return indicator
        
        return None
    
    def is_empty_page(self, content: str) -> bool:
        """ë¹ˆ í˜ì´ì§€ ê°ì§€"""
        if len(content.strip()) < 100:
            return True
        
        empty_selectors = self.parser_config.get('blocking_detection', {}).get(
            'empty_page_selectors', []
        )
        
        for selector in empty_selectors:
            if selector in content:
                return True
        
        return False
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ í¬ë¡¤ë§"""
        if not self.http_client:
            self.logger.error("HTTP í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
        
        try:
            self.stats["requests_made"] += 1
            
            result = await self.http_client.fetch(
                url, 
                self.site, 
                self.parser_config
            )
            
            content = result.get('content', '')
            
            # ì°¨ë‹¨ ê°ì§€
            blocking_indicator = self.detect_blocking(content, url)
            if blocking_indicator:
                self.stats["failed_requests"] += 1
                raise Exception(f"ì°¨ë‹¨ ê°ì§€: {blocking_indicator}")
            
            # ë¹ˆ í˜ì´ì§€ ê°ì§€
            if self.is_empty_page(content):
                self.stats["failed_requests"] += 1
                raise Exception("ë¹ˆ í˜ì´ì§€ ê°ì§€")
            
            self.stats["successful_requests"] += 1
            return content
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            log_parser_error(self.site, "fetch", url, str(e))
            return None
    
    def get_stats(self) -> Dict[str, Any]:
        """íŒŒì„œ í†µê³„ ë°˜í™˜"""
        success_rate = 0
        if self.stats["requests_made"] > 0:
            success_rate = self.stats["successful_requests"] / self.stats["requests_made"]
        
        return {
            **self.stats,
            "success_rate": success_rate,
            "site": self.site
        }
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.stats = {k: 0 for k in self.stats.keys()}


if __name__ == "__main__":
    # BaseParser í…ŒìŠ¤íŠ¸ëŠ” ì¶”ìƒ í´ë˜ìŠ¤ì´ë¯€ë¡œ ì§ì ‘ ë¶ˆê°€ëŠ¥
    print("BaseParserëŠ” ì¶”ìƒ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.")
    print("êµ¬ì²´ì ì¸ íŒŒì„œ êµ¬í˜„ì²´(ì˜ˆ: SiksinParser)ë¥¼ í†µí•´ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.")