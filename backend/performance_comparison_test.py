"""
GPT-4o-mini vs Gemini 2.5 Flash ì„±ëŠ¥ ë° ë¹„ìš© ë¹„êµ í…ŒìŠ¤íŠ¸
ì‘ë‹µ ì†ë„, í† í° ì‚¬ìš©ëŸ‰, ì˜ˆìƒ ë¹„ìš©ì„ ìˆ˜ì¹˜í™”í•˜ì—¬ ë¹„êµ
"""

import asyncio
import time
import json
from typing import Dict, Any, List
from datetime import datetime

# í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ë“¤
TEST_PROMPTS = [
    "ì•ˆë…•í•˜ì„¸ìš”",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ì— ì¢‹ì€ ìŒì‹ ì¶”ì²œí•´ì£¼ì„¸ìš”",
    "ê°•ë‚¨ì—­ ê·¼ì²˜ì—ì„œ í‚¤í†  ì‹ë‹¹ ì¶”ì²œí•´ì£¼ì„¸ìš”",
    "ì•„ì¹¨ì— ë¨¹ì„ í‚¤í†  ì¹œí™”ì ì¸ ê°„ë‹¨í•œ ìš”ë¦¬ ì•Œë ¤ì£¼ì„¸ìš”",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ ì¤‘ì¸ë° ì™¸ì‹í•  ë•Œ ì£¼ì˜í•  ì ì´ ë­ê°€ ìˆë‚˜ìš”?",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ì—ì„œ íƒ„ìˆ˜í™”ë¬¼ í•˜ë£¨ ê¶Œì¥ëŸ‰ì€ ì–¼ë§ˆì¸ê°€ìš”?",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ë¡œ ì²´ì¤‘ ê°ëŸ‰ íš¨ê³¼ë¥¼ ë³´ë ¤ë©´ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ ì¤‘ ìš´ë™ì€ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ì—ì„œ ìƒë¦¬ ë¶ˆìˆœì´ ìƒê²¼ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
    "í‚¤í†  ë‹¤ì´ì–´íŠ¸ ì¤‘ ìˆ ì„ ë§ˆì…”ë„ ë˜ë‚˜ìš”?"
]

class PerformanceTester:
    def __init__(self):
        self.results = {
            "gpt-4o-mini": [],
            "gemini-2.5-flash": []
        }
    
    async def test_model(self, provider: str, model: str, test_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ” {test_name} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            # LLM ì´ˆê¸°í™”
            from app.core.llm_factory import create_chat_llm
            from app.core.config import settings
            
            if provider == "openai":
                llm = create_chat_llm(
                    provider="openai",
                    model="gpt-4o-mini",
                    temperature=0.2,
                    max_tokens=512,
                    timeout=10
                )
            else:  # gemini
                llm = create_chat_llm(
                    provider="gemini",
                    model="gemini-2.5-flash",
                    temperature=0.2,
                    max_tokens=8192,
                    timeout=60
                )
            
            print(f"âœ… {test_name} LLM ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
            prompt_results = []
            total_tokens = 0
            total_cost = 0.0
            
            for i, prompt in enumerate(TEST_PROMPTS, 1):
                print(f"  ğŸ“ í…ŒìŠ¤íŠ¸ {i}/{len(TEST_PROMPTS)}: {prompt[:30]}...")
                
                try:
                    start_time = time.time()
                    
                    # LLM í˜¸ì¶œ
                    from langchain.schema import HumanMessage
                    response = await llm.ainvoke([HumanMessage(content=prompt)])
                    
                    end_time = time.time()
                    response_time = end_time - start_time
                    
                    # í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )
                    input_tokens = len(prompt.split()) * 1.3  # í•œêµ­ì–´ í† í° ì¶”ì •
                    output_tokens = len(response.content.split()) * 1.3
                    total_prompt_tokens = input_tokens + output_tokens
                    
                    # ë¹„ìš© ê³„ì‚°
                    if provider == "openai":
                        # GPT-4o-mini ê°€ê²©: ì…ë ¥ $0.15/1M, ì¶œë ¥ $0.60/1M
                        input_cost = (input_tokens / 1_000_000) * 0.15
                        output_cost = (output_tokens / 1_000_000) * 0.60
                        prompt_cost = input_cost + output_cost
                    else:
                        # Gemini 2.5 Flash ê°€ê²©: ì…ë ¥ $0.25/1M, ì¶œë ¥ $1.00/1M
                        input_cost = (input_tokens / 1_000_000) * 0.25
                        output_cost = (output_tokens / 1_000_000) * 1.00
                        prompt_cost = input_cost + output_cost
                    
                    total_tokens += total_prompt_tokens
                    total_cost += prompt_cost
                    
                    prompt_result = {
                        "prompt": prompt,
                        "response_time": response_time,
                        "input_tokens": int(input_tokens),
                        "output_tokens": int(output_tokens),
                        "total_tokens": int(total_prompt_tokens),
                        "cost": prompt_cost,
                        "response_length": len(response.content),
                        "success": True
                    }
                    
                    print(f"    â±ï¸ {response_time:.2f}ì´ˆ, ğŸ’° ${prompt_cost:.6f}, ğŸ“Š {int(total_prompt_tokens)}í† í°")
                    
                except Exception as e:
                    print(f"    âŒ ì˜¤ë¥˜: {e}")
                    prompt_result = {
                        "prompt": prompt,
                        "error": str(e),
                        "success": False
                    }
                
                prompt_results.append(prompt_result)
                await asyncio.sleep(0.5)  # API ì œí•œ ë°©ì§€
            
            # ì „ì²´ ê²°ê³¼ ì§‘ê³„
            successful_tests = [r for r in prompt_results if r.get("success", False)]
            avg_response_time = sum(r["response_time"] for r in successful_tests) / len(successful_tests) if successful_tests else 0
            avg_tokens = sum(r["total_tokens"] for r in successful_tests) / len(successful_tests) if successful_tests else 0
            total_successful = len(successful_tests)
            
            model_result = {
                "model": test_name,
                "provider": provider,
                "total_tests": len(TEST_PROMPTS),
                "successful_tests": total_successful,
                "success_rate": (total_successful / len(TEST_PROMPTS)) * 100,
                "avg_response_time": avg_response_time,
                "total_tokens": int(total_tokens),
                "avg_tokens_per_request": avg_tokens,
                "total_cost": total_cost,
                "avg_cost_per_request": total_cost / len(TEST_PROMPTS),
                "detailed_results": prompt_results,
                "tested_at": datetime.now().isoformat()
            }
            
            print(f"\nğŸ“Š {test_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
            print(f"  âœ… ì„±ê³µë¥ : {model_result['success_rate']:.1f}%")
            print(f"  â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.2f}ì´ˆ")
            print(f"  ğŸ“Š ì´ í† í°: {int(total_tokens):,}")
            print(f"  ğŸ’° ì´ ë¹„ìš©: ${total_cost:.6f}")
            print(f"  ğŸ’° í‰ê·  ë¹„ìš©/ìš”ì²­: ${total_cost/len(TEST_PROMPTS):.6f}")
            
            return model_result
            
        except Exception as e:
            print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                "model": test_name,
                "provider": provider,
                "error": str(e),
                "success": False,
                "tested_at": datetime.now().isoformat()
            }
    
    async def run_comparison(self):
        """ì „ì²´ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ GPT-4o-mini vs Gemini 2.5 Flash ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {len(TEST_PROMPTS)}ê°œ")
        print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # GPT-4o-mini í…ŒìŠ¤íŠ¸
        gpt_result = await self.test_model("openai", "gpt-4o-mini", "GPT-4o-mini")
        self.results["gpt-4o-mini"] = gpt_result
        
        print("\n" + "="*60)
        
        # Gemini í…ŒìŠ¤íŠ¸
        gemini_result = await self.test_model("gemini", "gemini-2.5-flash", "Gemini 2.5 Flash")
        self.results["gemini-2.5-flash"] = gemini_result
        
        # ê²°ê³¼ ë¹„êµ ë° ë¶„ì„
        self.print_comparison()
        
        # ê²°ê³¼ ì €ì¥
        self.save_results()
    
    def print_comparison(self):
        """ê²°ê³¼ ë¹„êµ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*80)
        
        gpt = self.results["gpt-4o-mini"]
        gemini = self.results["gemini-2.5-flash"]
        
        if not gpt.get("success", True) or not gemini.get("success", True):
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        # ì‘ë‹µ ì‹œê°„ ë¹„êµ
        gpt_time = gpt["avg_response_time"]
        gemini_time = gemini["avg_response_time"]
        time_improvement = ((gemini_time - gpt_time) / gemini_time) * 100
        
        print(f"\nâ±ï¸ ì‘ë‹µ ì†ë„ ë¹„êµ:")
        print(f"  GPT-4o-mini:    {gpt_time:.2f}ì´ˆ")
        print(f"  Gemini 2.5:     {gemini_time:.2f}ì´ˆ")
        print(f"  ê°œì„ ìœ¨:         {time_improvement:+.1f}% ({'ë¹ ë¦„' if time_improvement > 0 else 'ëŠë¦¼'})")
        
        # ë¹„ìš© ë¹„êµ
        gpt_cost = gpt["total_cost"]
        gemini_cost = gemini["total_cost"]
        cost_savings = ((gemini_cost - gpt_cost) / gemini_cost) * 100
        
        print(f"\nğŸ’° ë¹„ìš© ë¹„êµ (10ê°œ ìš”ì²­ ê¸°ì¤€):")
        print(f"  GPT-4o-mini:    ${gpt_cost:.6f}")
        print(f"  Gemini 2.5:     ${gemini_cost:.6f}")
        print(f"  ì ˆì•½ì•¡:         ${gemini_cost - gpt_cost:.6f}")
        print(f"  ì ˆì•½ìœ¨:         {cost_savings:+.1f}%")
        
        # í† í° íš¨ìœ¨ì„± ë¹„êµ
        gpt_tokens = gpt["total_tokens"]
        gemini_tokens = gemini["total_tokens"]
        token_efficiency = (gpt_tokens / gemini_tokens) * 100 if gemini_tokens > 0 else 0
        
        print(f"\nğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ë¹„êµ:")
        print(f"  GPT-4o-mini:    {gpt_tokens:,} í† í°")
        print(f"  Gemini 2.5:     {gemini_tokens:,} í† í°")
        print(f"  íš¨ìœ¨ì„±:         {token_efficiency:.1f}% (100% = Geminiì™€ ë™ì¼)")
        
        # ì„±ê³µë¥  ë¹„êµ
        gpt_success = gpt["success_rate"]
        gemini_success = gemini["success_rate"]
        
        print(f"\nâœ… ì•ˆì •ì„± ë¹„êµ:")
        print(f"  GPT-4o-mini:    {gpt_success:.1f}%")
        print(f"  Gemini 2.5:     {gemini_success:.1f}%")
        
        # ì›”ê°„ ì˜ˆìƒ ë¹„ìš© (1000íšŒ ìš”ì²­ ê¸°ì¤€)
        monthly_gpt = gpt_cost * 100
        monthly_gemini = gemini_cost * 100
        monthly_savings = monthly_gemini - monthly_gpt
        
        print(f"\nğŸ“ˆ ì›”ê°„ ì˜ˆìƒ ë¹„ìš© (1000íšŒ ìš”ì²­):")
        print(f"  GPT-4o-mini:    ${monthly_gpt:.2f}")
        print(f"  Gemini 2.5:     ${monthly_gemini:.2f}")
        print(f"  ì›”ê°„ ì ˆì•½ì•¡:    ${monthly_savings:.2f}")
        
        # ì¢…í•© í‰ê°€
        print(f"\nğŸ¯ ì¢…í•© í‰ê°€:")
        if time_improvement > 0 and cost_savings > 0:
            print(f"  ğŸš€ GPT-4o-miniê°€ ìš°ìˆ˜: {time_improvement:.1f}% ë¹ ë¥´ê³  {cost_savings:.1f}% ì €ë ´")
        elif time_improvement > 0:
            print(f"  âš¡ GPT-4o-miniê°€ ë¹ ë¦„: {time_improvement:.1f}% ë¹ ë¥¸ ì‘ë‹µ")
        elif cost_savings > 0:
            print(f"  ğŸ’° GPT-4o-miniê°€ ì €ë ´: {cost_savings:.1f}% ë¹„ìš© ì ˆì•½")
        else:
            print(f"  âš–ï¸  ì–‘ìª½ ëª¨ë‘ ì¥ë‹¨ì  ìˆìŒ")
    
    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"performance_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = PerformanceTester()
    await tester.run_comparison()

if __name__ == "__main__":
    asyncio.run(main())
