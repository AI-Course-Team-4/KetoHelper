"""
ì¼ë°˜ ì±„íŒ… ì „ìš© í‚¤í†  ì½”ì¹˜ ì—ì´ì „íŠ¸
ë ˆì‹œí”¼/ì‹ë‹¹ ê²€ìƒ‰ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ í‚¤í†  ì‹ë‹¨ ìƒë‹´ ì²˜ë¦¬

íŒ€ì› ê°œì¸í™” ê°€ì´ë“œ:
1. config/personal_config.pyì—ì„œ CHAT_AGENT_CONFIG ìˆ˜ì •
2. ê°œì¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ chat/prompts/ í´ë”ì— ìƒì„±
3. USE_PERSONAL_CONFIGë¥¼ Trueë¡œ ì„¤ì •í•˜ì—¬ í™œì„±í™”
"""

from typing import Dict, Any, Optional
from langchain.schema import HumanMessage
import importlib

from app.core.llm_factory import create_chat_llm
from app.core.redis_cache import redis_cache
from app.core.semantic_cache import semantic_cache_service
from app.core.config import settings
from config import get_personal_configs, get_agent_config

class SimpleKetoCoachAgent:
    """ì¼ë°˜ ì±„íŒ… ì „ìš© í‚¤í†  ì½”ì¹˜ ì—ì´ì „íŠ¸"""
    
    # ê¸°ë³¸ ì„¤ì • (ê°œì¸ ì„¤ì •ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
    DEFAULT_AGENT_NAME = "Simple Keto Coach"
    DEFAULT_PROMPT_FILE_NAME = "general_chat"  # chat/prompts/ í´ë”ì˜ íŒŒì¼ëª…
    
    def __init__(self, prompt_file_name: str = None, agent_name: str = None):
        # ê°œì¸ ì„¤ì • ë¡œë“œ
        personal_configs = get_personal_configs()
        agent_config = get_agent_config("chat_agent", personal_configs)
        
        # ê°œì¸í™”ëœ ì„¤ì • ì ìš© (ìš°ì„ ìˆœìœ„: ë§¤ê°œë³€ìˆ˜ > ê°œì¸ì„¤ì • > ê¸°ë³¸ì„¤ì •)
        self.prompt_file_name = prompt_file_name or agent_config.get("prompt_file_name", self.DEFAULT_PROMPT_FILE_NAME)
        self.agent_name = agent_name or agent_config.get("agent_name", self.DEFAULT_AGENT_NAME)
        
        # ë™ì  í”„ë¡¬í”„íŠ¸ ë¡œë”©
        self.prompt_template = self._load_prompt_template()
        
        try:
            # ChatAgent ì „ìš© LLM ì„¤ì • ì‚¬ìš©
            from app.core.config import settings
            self.llm = create_chat_llm(
                provider=settings.chat_agent_provider,
                model=settings.chat_agent_model,
                temperature=settings.chat_agent_temperature,
                max_tokens=settings.chat_agent_max_tokens,
                timeout=settings.chat_agent_timeout
            )
            print(f"âœ… ChatAgent LLM ì´ˆê¸°í™”: {settings.chat_agent_provider}/{settings.chat_agent_model}")
            print(f"ğŸ”§ ChatAgent ì„¤ì •: max_tokens={settings.chat_agent_max_tokens}, timeout={settings.chat_agent_timeout}s")
        except Exception as e:
            print(f"âŒ ChatAgent LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm = None
    
    def _load_prompt_template(self) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë™ì  ë¡œë”©"""
        try:
            # í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ë™ì  import
            module_path = f"app.prompts.chat.{self.prompt_file_name}"
            prompt_module = importlib.import_module(module_path)
            
            # GENERAL_CHAT_PROMPT ë˜ëŠ” PROMPT ì†ì„± ì°¾ê¸°
            if hasattr(prompt_module, 'GENERAL_CHAT_PROMPT'):
                return prompt_module.GENERAL_CHAT_PROMPT
            elif hasattr(prompt_module, 'PROMPT'):
                return prompt_module.PROMPT
            else:
                print(f"ê²½ê³ : {self.prompt_file_name}ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.")
                return self._get_default_prompt()
            
        except ImportError:
            print(f"ê²½ê³ : {self.prompt_file_name} í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í”„ë¡¬í”„íŠ¸ íŒŒì¼ì—ì„œ ë¡œë“œ)"""
        try:
            from app.prompts.chat.general_chat import DEFAULT_CHAT_PROMPT
            return DEFAULT_CHAT_PROMPT
        except ImportError:
            # í´ë°± í”„ë¡¬í”„íŠ¸ íŒŒì¼ì—ì„œ ë¡œë“œ
            try:
                from app.prompts.chat.fallback import FALLBACK_GENERAL_CHAT_PROMPT
                return FALLBACK_GENERAL_CHAT_PROMPT
            except ImportError:
                # ì •ë§ ë§ˆì§€ë§‰ í´ë°±
                return "í‚¤í†  ì‹ë‹¨ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. ì§ˆë¬¸: {message}, í”„ë¡œí•„: {profile_context}"
    
    async def process_message(
        self,
        message: str,
        location: Optional[Dict[str, float]] = None,
        radius_km: float = 5.0,
        profile: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ì¼ë°˜ ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬"""
        
        try:
            if not self.llm:
                return {
                    "response": "AI ì„œë¹„ìŠ¤ê°€ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Google API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "intent": "general_chat",
                    "results": [],
                    "tool_calls": []
                }
            
            # ğŸš€ ì¼ë°˜ ì±„íŒ… ìºì‹± ë¡œì§ ì¶”ê°€ (meal_plannerì™€ ë™ì¼í•œ ë°©ì‹)
            user_id = profile.get("user_id", "") if profile else ""
            allergies = profile.get("allergies", []) if profile else []
            dislikes = profile.get("dislikes", []) if profile else []
            
            cache_key = f"general_chat_{hash(message)}_{user_id}_{hash(tuple(sorted(allergies)))}_{hash(tuple(sorted(dislikes)))}"
            
            # ğŸ” ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            print(f"ğŸ” ì¼ë°˜ ì±„íŒ… ìºì‹œ í‚¤: {cache_key}")
            print(f"ğŸ” ì‚¬ìš©ì ID: {user_id}")
            print(f"ğŸ” ì•Œë ˆë¥´ê¸° í•´ì‹œ: {hash(tuple(sorted(allergies)))}")
            print(f"ğŸ” ê¸°í”¼ì‹í’ˆ í•´ì‹œ: {hash(tuple(sorted(dislikes)))}")
            print(f"ğŸ” ë©”ì‹œì§€ í•´ì‹œ: {hash(message)}")
            
            # Redis ìºì‹œ í™•ì¸
            print(f"    ğŸ” ìºì‹œ í™•ì¸ ì‹œì‘: {cache_key}")
            print(f"    ğŸ” Redis í™œì„±í™” ìƒíƒœ: {redis_cache.is_enabled}")
            print(f"    ğŸ” Redis ê°ì²´: {redis_cache}")
            print(f"    ğŸ” Redis íƒ€ì…: {type(redis_cache)}")
            
            try:
                cached_result = redis_cache.get(cache_key)
                print(f"    ğŸ” Redis get ê²°ê³¼: {cached_result is not None}")
            except Exception as e:
                print(f"    âŒ Redis get ì˜¤ë¥˜: {e}")
                cached_result = None
            
            if cached_result:
                print(f"    âœ… Redis ì¼ë°˜ ì±„íŒ… ìºì‹œ íˆíŠ¸: {message[:30]}...")
                print(f"    âœ… ìºì‹œëœ ì‘ë‹µ ê¸¸ì´: {len(str(cached_result))} ë¬¸ì")
                return cached_result
            
            # ì‹œë§¨í‹± ìºì‹œ í™•ì¸ (ì •í™• ìºì‹œ ë¯¸ìŠ¤ ì‹œ)
            if settings.semantic_cache_enabled:
                try:
                    model_ver = f"chat_agent_{settings.llm_model}"
                    opts_hash = f"{user_id}_{hash(tuple(sorted(allergies)))}_{hash(tuple(sorted(dislikes)))}"
                    
                    semantic_result = await semantic_cache_service.semantic_lookup(
                        message, user_id, model_ver, opts_hash
                    )
                    
                    if semantic_result:
                        print(f"    ğŸ§  ì‹œë§¨í‹± ìºì‹œ íˆíŠ¸: ì¼ë°˜ ì±„íŒ…")
                        return semantic_result
                except Exception as e:
                    print(f"    âš ï¸ ì‹œë§¨í‹± ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            
            # ìºì‹œ ë¯¸ìŠ¤ ì‹œ
                print(f"    âŒ Redis ì¼ë°˜ ì±„íŒ… ìºì‹œ ë¯¸ìŠ¤: {message[:30]}...")
                print(f"    âŒ ìºì‹œ í‚¤: {cache_key}")
            
            # ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìƒì„±
            response = await self._generate_general_response(message, profile)
            
            result_data = {
                "response": response,
                "intent": "general_chat",
                "results": [],
                "tool_calls": [{"tool": "general_chat_agent", "message": message}]
            }
            
            # ğŸš€ ì¼ë°˜ ì±„íŒ… ê²°ê³¼ ìºì‹± (TTL: 30ë¶„)
            print(f"    ğŸ’¾ ìºì‹œ ì €ì¥ ì‹œì‘: {cache_key}")
            redis_cache.set(cache_key, result_data, ttl=1800)
            print(f"    âœ… ì¼ë°˜ ì±„íŒ… ê²°ê³¼ ìºì‹œ ì €ì¥ ì™„ë£Œ: {message[:30]}...")
            print(f"    âœ… ì €ì¥ëœ ì‘ë‹µ ê¸¸ì´: {len(str(result_data))} ë¬¸ì")
            
            # ğŸ§  ì‹œë§¨í‹± ìºì‹œ ì €ì¥
            if settings.semantic_cache_enabled:
                try:
                    model_ver = f"chat_agent_{settings.llm_model}"
                    opts_hash = f"{user_id}_{hash(tuple(sorted(allergies)))}_{hash(tuple(sorted(dislikes)))}"
                    
                    meta = {
                        "route": "general_chat",
                        "allergies": allergies,
                        "dislikes": dislikes
                    }
                    
                    await semantic_cache_service.save_semantic_cache(
                        message, user_id, model_ver, opts_hash, 
                        result_data.get("response", ""), meta
                    )
                except Exception as e:
                    print(f"    âš ï¸ ì‹œë§¨í‹± ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
            
            return result_data
            
        except Exception as e:
            return {
                "response": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "intent": "error",
                "results": [],
                "tool_calls": []
            }
    
    async def _generate_general_response(self, message: str, profile: Optional[Dict[str, Any]]) -> str:
        """ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìƒì„± (ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        
        try:
            # ğŸš€ ì¼ë°˜ ì±„íŒ… ìºì‹± ë¡œì§ ì¶”ê°€ (meal_plannerì™€ ë™ì¼í•œ ë°©ì‹)
            user_id = profile.get("user_id", "") if profile else ""
            allergies = profile.get("allergies", []) if profile else []
            dislikes = profile.get("dislikes", []) if profile else []
            
            cache_key = f"general_chat_response_{hash(message)}_{user_id}_{hash(tuple(sorted(allergies)))}_{hash(tuple(sorted(dislikes)))}"
            
            # ğŸ” ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            print(f"ğŸ” _generate_general_response ìºì‹œ í‚¤: {cache_key}")
            print(f"ğŸ” ì‚¬ìš©ì ID: {user_id}")
            print(f"ğŸ” ì•Œë ˆë¥´ê¸° í•´ì‹œ: {hash(tuple(sorted(allergies)))}")
            print(f"ğŸ” ê¸°í”¼ì‹í’ˆ í•´ì‹œ: {hash(tuple(sorted(dislikes)))}")
            print(f"ğŸ” ë©”ì‹œì§€ í•´ì‹œ: {hash(message)}")
            
            # Redis ìºì‹œ í™•ì¸
            print(f"    ğŸ” ìºì‹œ í™•ì¸ ì‹œì‘: {cache_key}")
            print(f"    ğŸ” Redis í™œì„±í™” ìƒíƒœ: {redis_cache.is_enabled}")
            print(f"    ğŸ” Redis ê°ì²´: {redis_cache}")
            print(f"    ğŸ” Redis íƒ€ì…: {type(redis_cache)}")
            
            try:
                cached_result = redis_cache.get(cache_key)
                print(f"    ğŸ” Redis get ê²°ê³¼: {cached_result is not None}")
            except Exception as e:
                print(f"    âŒ Redis get ì˜¤ë¥˜: {e}")
                cached_result = None
            
            if cached_result:
                print(f"    âœ… Redis ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìºì‹œ íˆíŠ¸: {message[:30]}...")
                print(f"    âœ… ìºì‹œëœ ì‘ë‹µ ê¸¸ì´: {len(str(cached_result))} ë¬¸ì")
                return cached_result
            else:
                print(f"    âŒ Redis ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìºì‹œ ë¯¸ìŠ¤: {message[:30]}...")
                print(f"    âŒ ìºì‹œ í‚¤: {cache_key}")
            
            # í”„ë¡œí•„ ì •ë³´ ì»¨í…ìŠ¤íŠ¸
            profile_context = ""
            if profile:
                if profile.get("allergies"):
                    profile_context += f"ì•Œë ˆë¥´ê¸°: {', '.join(profile['allergies'])}. "
                if profile.get("dislikes"):
                    profile_context += f"ë¹„ì„ í˜¸ ìŒì‹: {', '.join(profile['dislikes'])}. "
                if profile.get("goals_carbs_g"):
                    profile_context += f"ëª©í‘œ íƒ„ìˆ˜í™”ë¬¼: {profile['goals_carbs_g']}g/ì¼. "
            
            # ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            prompt = self.prompt_template.format(
                message=message,
                profile_context=profile_context if profile_context else 'íŠ¹ë³„í•œ ì œì•½ì‚¬í•­ ì—†ìŒ'
            )
            
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            response_content = response.content
            
            # ğŸš€ ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìºì‹± (TTL: 30ë¶„)
            print(f"    ğŸ’¾ ìºì‹œ ì €ì¥ ì‹œì‘: {cache_key}")
            redis_cache.set(cache_key, response_content, ttl=1800)
            print(f"    âœ… ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìºì‹œ ì €ì¥ ì™„ë£Œ: {message[:30]}...")
            print(f"    âœ… ì €ì¥ëœ ì‘ë‹µ ê¸¸ì´: {len(str(response_content))} ë¬¸ì")
            
            return response_content
            
        except Exception as e:
            return f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def stream_response(self, *args, **kwargs):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        result = await self.process_message(*args, **kwargs)
        yield {"event": "complete", **result}
    
