"""
ì‹œë§¨í‹± ìºì‹œ ì„œë¹„ìŠ¤
ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìºì‹œ ì‹œìŠ¤í…œ
"""

import hashlib
import json
import time
from typing import Optional, List, Dict, Any
import openai
from app.core.config import settings
from app.core.database import supabase


class SemanticCacheService:
    """ì‹œë§¨í‹± ìºì‹œ ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.supabase = supabase
        self.openai_client = openai.OpenAI(api_key=settings.openai_api_key)
        self.threshold = 0.90  # ìœ ì‚¬ë„ ì„ê³„ê°’
        self.window_seconds = 24 * 3600  # 24ì‹œê°„
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ê³µë°± ì •ë¦¬ ë° ì†Œë¬¸ì ë³€í™˜
        normalized = " ".join(text.strip().split()).lower()
        
        # ë™ì˜ì–´ ì •ê·œí™”
        synonyms = {
            'ì‹ë‹¨í‘œ': ['ì‹ë‹¨', 'ë©”ë‰´', 'ê³„íš', 'í‘œ'],
            'ë§Œë“¤ì–´ì¤˜': ['ë§Œë“¤ì–´', 'ìƒì„±í•´ì¤˜', 'ì¶”ì²œí•´ì¤˜', 'ê³„íší•´ì¤˜'],
            'í‚¤í† ': ['ì¼€í† ', 'ì €íƒ„ìˆ˜', 'ì €íƒ„ìˆ˜í™”ë¬¼'],
            'ì¼ì£¼ì¼': ['7ì¼', '1ì£¼ì¼'],
            'ì´ëŸ¬ë©´': ['ì´ëŸ°', 'ê°™ì€', 'ë¹„ìŠ·í•œ']
        }
        
        for standard, variants in synonyms.items():
            for variant in variants:
                normalized = normalized.replace(variant, standard)
        
        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        stop_words = ['ì´ëŸ¬ë©´', 'ì´ëŸ°', 'ê°™ì€', 'ë¹„ìŠ·í•œ', 'ì´ëŸ°ê±°']
        for word in stop_words:
            normalized = normalized.replace(word, '')
        
        return normalized.strip()
    
    def _sha256(self, text: str) -> str:
        """SHA256 í•´ì‹œ ìƒì„±"""
        return hashlib.sha256(text.encode("utf-8")).hexdigest()
    
    async def _embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            print(f"ğŸ“Š ì‹œë§¨í‹± ìºì‹œ ì„ë² ë”© ìƒì„±: {text[:50]}...")
            response = self.openai_client.embeddings.create(
                model=settings.embedding_model,
                input=text
            )
            embedding = response.data[0].embedding
            print(f"âœ… ì‹œë§¨í‹± ìºì‹œ ì„ë² ë”© ì™„ë£Œ: {len(embedding)}ì°¨ì›")
            return embedding
        except Exception as e:
            print(f"âŒ ì‹œë§¨í‹± ìºì‹œ ì„ë² ë”© ì˜¤ë¥˜: {e}")
            return []
    
    def _passes_rules(self, meta: Dict[str, Any], query_text: str) -> bool:
        """ë©”íƒ€ë°ì´í„° ê·œì¹™ ê²€ì‚¬"""
        # ì‹ë‹¨ ìƒì„±ì˜ ê²½ìš° ì¼ìˆ˜ ì •ë³´ë„ ê³ ë ¤
        if meta.get("route") == "meal_plan":
            cached_days = meta.get("days")
            if cached_days:
                # ìš”ì²­ëœ ì¼ìˆ˜ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­)
                import re
                days_match = re.search(r'(\d+)ì¼', query_text)
                if days_match:
                    requested_days = int(days_match.group(1))
                    # ì¼ìˆ˜ê°€ ë‹¤ë¥´ë©´ ë§¤ì¹­ ì‹¤íŒ¨
                    if cached_days != requested_days:
                        print(f"   ğŸ“… ì¼ìˆ˜ ë¶ˆì¼ì¹˜: ìºì‹œ {cached_days}ì¼ vs ìš”ì²­ {requested_days}ì¼")
                        return False
        return True
    
    async def semantic_lookup(
        self, 
        text: str, 
        user_id: str, 
        model_ver: str, 
        opts_hash: str
    ) -> Optional[str]:
        """ì‹œë§¨í‹± ìºì‹œì—ì„œ ìœ ì‚¬í•œ ê²°ê³¼ ê²€ìƒ‰"""
        try:
            # í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_text = self._normalize_text(text)
            
            # ì„ë² ë”© ìƒì„±
            query_vec = await self._embed_text(normalized_text)
            if not query_vec:
                return None
            
            # Supabase RPC í˜¸ì¶œ
            response = self.supabase.rpc("sc_match", {
                "query_vec": query_vec,
                "p_user": user_id,
                "p_model_ver": model_ver,
                "p_opts_hash": opts_hash,
                "p_window_seconds": self.window_seconds,
                "p_limit": 1
            }).execute()
            
            rows = getattr(response, "data", []) or []
            if not rows:
                print(f"ğŸ” ì‹œë§¨í‹± ìºì‹œ ë¯¸ìŠ¤: ìœ ì‚¬í•œ ê²°ê³¼ ì—†ìŒ")
                return None
            
            top_result = rows[0]
            score = float(top_result["score"])
            
            print(f"ğŸ” ì‹œë§¨í‹± ìºì‹œ ê²€ìƒ‰ ê²°ê³¼: ì ìˆ˜ {score:.3f} (ì„ê³„ê°’ {self.threshold})")
            
            if score >= self.threshold and self._passes_rules(top_result["meta"], text):
                print(f"âœ… ì‹œë§¨í‹± ìºì‹œ íˆíŠ¸: {score:.3f} (ì„ê³„ê°’ {self.threshold})")
                print(f"   ğŸ“ ì›ë³¸ ë©”ì‹œì§€: '{text[:50]}...'")
                print(f"   ğŸ¯ ë§¤ì¹­ëœ ë©”ì‹œì§€: '{top_result.get('meta', {}).get('original_message', 'N/A')[:50]}...'")
                return top_result["answer"]
            else:
                print(f"âŒ ì‹œë§¨í‹± ìºì‹œ ë¯¸ìŠ¤: ì ìˆ˜ {score:.3f} < ì„ê³„ê°’ {self.threshold}")
                return None
                
        except Exception as e:
            print(f"âŒ ì‹œë§¨í‹± ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    async def save_semantic_cache(
        self,
        text: str,
        user_id: str,
        model_ver: str,
        opts_hash: str,
        answer: str,
        meta: Dict[str, Any]
    ) -> bool:
        """ì‹œë§¨í‹± ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        try:
            # í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_text = self._normalize_text(text)
            
            # ì„ë² ë”© ìƒì„±
            embedding = await self._embed_text(normalized_text)
            if not embedding:
                return False
            
            # í•´ì‹œ ìƒì„±
            prompt_hash = self._sha256(normalized_text)
            
            # ë°ì´í„° ì €ì¥
            row = {
                "user_id": user_id,
                "model_ver": model_ver,
                "opts_hash": opts_hash,
                "prompt_hash": prompt_hash,
                "answer": answer,
                "meta": meta,
                "embedding": embedding
            }
            
            result = self.supabase.table("semantic_cache").insert(row).execute()
            
            if result.data:
                print(f"âœ… ì‹œë§¨í‹± ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(answer)}ì")
                print(f"   ğŸ“ ë©”ì‹œì§€: '{normalized_text[:50]}...'")
                print(f"   ğŸ·ï¸ ëª¨ë¸: {model_ver}")
                print(f"   ğŸ”‘ ì˜µì…˜ í•´ì‹œ: {opts_hash[:20]}...")
                return True
            else:
                print(f"âŒ ì‹œë§¨í‹± ìºì‹œ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì‹œë§¨í‹± ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
semantic_cache_service = SemanticCacheService()
