"""
í•œê¸€ ìµœì í™” ê²€ìƒ‰ ë„êµ¬
PostgreSQL Full-Text Search + pg_trgm + ë²¡í„° ê²€ìƒ‰ í†µí•©
"""

import re
import openai
import asyncio
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from app.core.database import supabase
from app.core.config import settings

class KoreanSearchTool:
    """í•œê¸€ ìµœì í™” ê²€ìƒ‰ ë„êµ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supabase = supabase
        self.openai_client = openai.OpenAI(api_key=settings.openai_api_key)
        
        # ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ
        synonym_file = Path(__file__).parent.parent.parent / 'data' / 'ingredient_synonyms.json'
        try:
            with open(synonym_file, 'r', encoding='utf-8') as f:
                self.synonym_data = json.load(f)
                print(f"âœ… ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {synonym_file}")
        except Exception as e:
            print(f"âš ï¸ ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.synonym_data = {"ì•Œë ˆë¥´ê¸°": {}, "ë¹„ì„ í˜¸": {}}
    
    def _expand_with_synonyms(self, words: List[str], category: str) -> List[str]:
        """ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë™ì˜ì–´ë¡œ í™•ì¥"""
        expanded = []
        synonym_dict = self.synonym_data.get(category, {})
        
        for word in words:
            expanded.append(word)  # ì›ë˜ ë‹¨ì–´ ì¶”ê°€
            if word in synonym_dict:
                expanded.extend(synonym_dict[word])  # ë™ì˜ì–´ ì¶”ê°€
        
        return expanded
    
    async def _create_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_korean_keywords(self, query: str) -> List[str]:
        """í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ê·œí™”"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]+', query)
        
        # 2ê¸€ì ì´ìƒë§Œ í•„í„°ë§
        keywords = [kw for kw in keywords if len(kw) >= 2]
        
        # í•œê¸€ í‚¤ì›Œë“œ ì •ê·œí™” (ì¡°ì‚¬ ì œê±° ë“±)
        normalized_keywords = []
        for keyword in keywords:
            # í•œê¸€ì¸ ê²½ìš° ì¡°ì‚¬ ì œê±°
            if re.match(r'[ê°€-í£]+', keyword):
                # ê°„ë‹¨í•œ ì¡°ì‚¬ ì œê±° (ë” ì •êµí•œ í˜•íƒœì†Œ ë¶„ì„ í•„ìš”ì‹œ KoNLPy ì‚¬ìš©)
                normalized = re.sub(r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ì™€|ê³¼|ì˜|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ë¶€í„°|í•œí…Œ|ì—ê²Œ)$', '', keyword)
                if len(normalized) >= 2:
                    normalized_keywords.append(normalized)
            else:
                normalized_keywords.append(keyword)
        
        return normalized_keywords
    
    def _generate_query_variants(self, query: str) -> List[str]:
        """ì‚¬ìš©ì ê²€ìƒ‰ì–´ë¥¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì •ê·œí™”í•´ ë³€í˜• ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±.
        - ë¶ˆìš©ì–´ ì œê±°: ë ˆì‹œí”¼/ë§Œë“œëŠ”ë²•/ë§Œë“œëŠ” ë²•/ìš”ë¦¬ ë“±
        - ê³µë°± ì œê±°/í† í° ë¶„ë¦¬/OR í† í°
        """
        q = (query or '').strip()
        if not q:
            return []

        stopwords = ['ë ˆì‹œí”¼', 'ë§Œë“œëŠ”ë²•', 'ë§Œë“œëŠ” ë²•', 'ìš”ë¦¬', 'ë°©ë²•']
        base = q
        for sw in stopwords:
            base = base.replace(sw, '').strip()

        # í† í°í™”(ê³µë°± ê¸°ì¤€)
        tokens = [t for t in base.split() if t]

        variants = []
        variants.append(q)            # ì›ë¬¸
        if base and base != q:
            variants.append(base)     # ë¶ˆìš©ì–´ ì œê±°
        if tokens:
            joined = ' '.join(tokens)
            if joined not in variants:
                variants.append(joined)
            nospace = ''.join(tokens)
            if nospace and nospace not in variants:
                variants.append(nospace)
            # OR í† í°(ë‹¹ê·¼|ë¼í˜|ê¹€ë°¥)
            if len(tokens) > 1:
                or_tokens = '|'.join(tokens)
                variants.append(or_tokens)

        # ì¤‘ë³µ ì œê±° ìœ ì§€ ìˆœì„œ
        seen = set()
        uniq = []
        for v in variants:
            if v and v not in seen:
                uniq.append(v)
                seen.add(v)
        return uniq[:5]

    async def _exact_ilike_search(self, query: str, k: int) -> List[Dict]:
        """ì •í™• ë§¤ì¹­ì— ê°€ê¹Œìš´ ILIKE ê¸°ë°˜ ê²€ìƒ‰(RPC ì‚¬ìš©).
        ë³€í˜• ì¿¼ë¦¬(ë¶ˆìš©ì–´ ì œê±°/ê³µë°± ì œê±°/OR í† í°)ë¥¼ ìˆœì°¨ ì‹œë„í•˜ì—¬
        ìµœì´ˆë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.
        """
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []

            for vq in self._generate_query_variants(query):
                try:
                    res = self.supabase.rpc('ilike_search', {'query_text': vq, 'match_count': k}).execute()
                    rows = res.data or []
                    if rows:
                        formatted = []
                        for row in rows:
                            formatted.append({
                                'id': str(row.get('id', '')),
                                'title': row.get('title', 'ì œëª© ì—†ìŒ'),
                                'content': row.get('content', ''),
                                'allergens': row.get('allergens', []),
                                'ingredients': row.get('ingredients', []),
                                'search_score': row.get('search_score', 1.0),
                                'search_type': 'ilike_exact',
                                'metadata': {kk: vv for kk, vv in row.items() if kk not in ['id','title','content','search_score','allergens','ingredients']}
                            })
                        return formatted
                except Exception as e:
                    print(f"ILIKE ì •í™• ë§¤ì¹­ RPC ì˜¤ë¥˜({vq}): {e}")
                    continue
            return []
        except Exception as e:
            print(f"ILIKE ì •í™• ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return []
    async def _groonga_search(self, query: str, k: int) -> List[Dict]:
        """PGroonga ê²€ìƒ‰ (ì œëª©/ë³¸ë¬¸ ìš°ì„  ì •í™• ë§¤ì¹­)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            results = self.supabase.rpc('groonga_search', {
                'query_text': query,
                'match_count': k
            }).execute()

            formatted_results = []
            for result in results.data or []:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', 1.0),
                    'search_type': 'pgroonga',
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'search_score', 'allergens', 'ingredients']}
                })

            return formatted_results
        except Exception as e:
            print(f"PGroonga ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _full_text_search(self, query: str, k: int) -> List[Dict]:
        """PostgreSQL Full-Text Search (í•œê¸€ ìµœì í™”)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            # Full-Text Search ì‹¤í–‰ (RPC í•¨ìˆ˜ ì‚¬ìš©)
            results = self.supabase.rpc('fts_search', {
                'query_text': query,
                'match_count': k
            }).execute()
            
            formatted_results = []
            for result in results.data or []:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', result.get('fts_score', 0.0)),
                    'search_type': 'fts',
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'fts_score', 'allergens', 'ingredients']}
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"Full-Text Search ì˜¤ë¥˜: {e}")
            return []
    
    async def _trigram_similarity_search(self, query: str, k: int) -> List[Dict]:
        """Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ (í•œê¸€ ìœ ì‚¬ë„)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            # Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ (RPC í•¨ìˆ˜ ì‚¬ìš©)
            results = self.supabase.rpc('trgm_search', {
                'query_text': query,
                'match_count': k
            }).execute()
            
            formatted_results = []
            for result in results.data or []:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', result.get('similarity_score', 0.0)),
                    'search_type': 'trigram',
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'similarity_score', 'allergens', 'ingredients']}
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _vector_search(self, query: str, query_embedding: List[float], k: int, user_id: Optional[str] = None, meal_type: Optional[str] = None) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ (ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ í•„í„°ë§)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            # ì‚¬ìš©ì í”„ë¡œí•„ì—ì„œ ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ ê°€ì ¸ì˜¤ê¸°
            exclude_allergens_embeddings = None
            exclude_dislikes_embeddings = None
            exclude_allergens_names = None
            exclude_dislikes_names = None
            
            if user_id:
                from app.tools.shared.profile_tool import user_profile_tool
                user_preferences = await user_profile_tool.get_user_preferences(user_id)
                
                if user_preferences.get("success"):
                    prefs = user_preferences["preferences"]
                    user_allergies = prefs.get("allergies", [])
                    user_dislikes = prefs.get("dislikes", [])
                    
                    # ì•Œë ˆë¥´ê¸° í‚¤ì›Œë“œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ)
                    if user_allergies:
                        allergy_text = ' '.join(user_allergies)
                        allergy_embedding = await self._create_embedding(allergy_text)
                        exclude_allergens_embeddings = [allergy_embedding]  # ë°°ì—´ë¡œ ê°ì‹¸ê¸°
                        exclude_allergens_names = user_allergies
                        print(f"ğŸ” ì•Œë ˆë¥´ê¸° ì„ë² ë”© ìƒì„± (1ê°œ): {user_allergies}")
                    
                    # ë¹„ì„ í˜¸ í‚¤ì›Œë“œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ)
                    if user_dislikes:
                        dislike_text = ' '.join(user_dislikes)
                        dislike_embedding = await self._create_embedding(dislike_text)
                        exclude_dislikes_embeddings = [dislike_embedding]  # ë°°ì—´ë¡œ ê°ì‹¸ê¸°
                        exclude_dislikes_names = user_dislikes
                        print(f"ğŸ” ë¹„ì„ í˜¸ ì„ë² ë”© ìƒì„± (1ê°œ): {user_dislikes}")
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ (RPC í•¨ìˆ˜ ì‚¬ìš©)
            rpc_params = {
                'query_embedding': query_embedding,
                'match_count': k,
                'similarity_threshold': 0.0
            }
            
            # ë‹¨ì¼ ë²¡í„°ë¡œ ì „ë‹¬ (ë°°ì—´ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ)
            if exclude_allergens_embeddings:
                rpc_params['exclude_allergens_embedding'] = exclude_allergens_embeddings[0]
            if exclude_dislikes_embeddings:
                rpc_params['exclude_dislikes_embedding'] = exclude_dislikes_embeddings[0]
            if exclude_allergens_names:
                rpc_params['exclude_allergens_names'] = exclude_allergens_names
            if exclude_dislikes_names:
                rpc_params['exclude_dislikes_names'] = exclude_dislikes_names
            
            # ğŸ†• meal_type í•„í„° ì¶”ê°€
            if meal_type:
                rpc_params['meal_type_filter'] = meal_type
                print(f"ğŸ½ï¸ meal_type í•„í„° ì ìš©: {meal_type}")
            
            print(f"ğŸ” RPC íŒŒë¼ë¯¸í„°: allergens={len(exclude_allergens_names) if exclude_allergens_names else 0}, dislikes={len(exclude_dislikes_names) if exclude_dislikes_names else 0}")
            
            results = self.supabase.rpc('vector_search', rpc_params).execute()
            
            formatted_results = []
            filtered_count = 0
            
            # ë™ì˜ì–´ í™•ì¥
            expanded_allergens = self._expand_with_synonyms(exclude_allergens_names, 'ì•Œë ˆë¥´ê¸°') if exclude_allergens_names else []
            expanded_dislikes = self._expand_with_synonyms(exclude_dislikes_names, 'ë¹„ì„ í˜¸') if exclude_dislikes_names else []
            
            for result in results.data or []:
                # ğŸš¨ Python ë ˆë²¨ í•„í„°ë§: title, ingredientsì—ì„œ ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ ì²´í¬ (ë™ì˜ì–´ í¬í•¨)
                title = result.get('title', '').lower()
                ingredients = result.get('ingredients', [])
                ingredients_lower = [ing.lower() for ing in ingredients] if ingredients else []
                
                # ì•Œë ˆë¥´ê¸° ì²´í¬ (ë™ì˜ì–´ í¬í•¨)
                if expanded_allergens:
                    allergy_found = False
                    for allergy in expanded_allergens:
                        allergy_lower = allergy.lower()
                        # titleì— ìˆëŠ”ì§€ ì²´í¬
                        if allergy_lower in title:
                            print(f"    âš ï¸ ì•Œë ˆë¥´ê¸° ì œì™¸: '{result.get('title')}' (ì œëª©ì— '{allergy}' í¬í•¨)")
                            allergy_found = True
                            break
                        # ingredientsì— ìˆëŠ”ì§€ ì²´í¬ (ë¶€ë¶„ ì¼ì¹˜)
                        for ing in ingredients_lower:
                            if allergy_lower in ing:
                                print(f"    âš ï¸ ì•Œë ˆë¥´ê¸° ì œì™¸: '{result.get('title')}' (ì¬ë£Œ '{ing}'ì— '{allergy}' í¬í•¨)")
                                allergy_found = True
                                break
                        if allergy_found:
                            break
                    if allergy_found:
                        filtered_count += 1
                        continue
                
                # ë¹„ì„ í˜¸ ì²´í¬ (ë™ì˜ì–´ í¬í•¨)
                if expanded_dislikes:
                    dislike_found = False
                    for dislike in expanded_dislikes:
                        dislike_lower = dislike.lower()
                        # titleì— ìˆëŠ”ì§€ ì²´í¬
                        if dislike_lower in title:
                            print(f"    âš ï¸ ë¹„ì„ í˜¸ ì œì™¸: '{result.get('title')}' (ì œëª©ì— '{dislike}' í¬í•¨)")
                            dislike_found = True
                            break
                        # ingredientsì— ìˆëŠ”ì§€ ì²´í¬ (ë¶€ë¶„ ì¼ì¹˜)
                        for ing in ingredients_lower:
                            if dislike_lower in ing:
                                print(f"    âš ï¸ ë¹„ì„ í˜¸ ì œì™¸: '{result.get('title')}' (ì¬ë£Œ '{ing}'ì— '{dislike}' í¬í•¨)")
                                dislike_found = True
                                break
                        if dislike_found:
                            break
                    if dislike_found:
                        filtered_count += 1
                        continue
                
                # í†µê³¼!
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', result.get('similarity_score', 0.0)),
                    'search_type': 'vector',
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'similarity_score', 'allergens', 'ingredients']}
                })
            
            if filtered_count > 0:
                print(f"    ğŸ” Python í•„í„°ë§: {filtered_count}ê°œ ì œì™¸ë¨")
            
            return formatted_results
            
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _fallback_ilike_search(self, query: str, k: int) -> List[Dict]:
        """í´ë°± ILIKE ê²€ìƒ‰ (ê¸°ì¡´)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            keywords = self._extract_korean_keywords(query)
            if not keywords:
                return []
            
            all_results = []
            
            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                try:
                    # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš© (JSONB ê²€ìƒ‰ ì œê±°)
                    title_results = self.supabase.table('recipe_blob_emb').select('*').ilike('title', f'%{keyword}%').limit(k).execute()
                    
                    all_results.extend(title_results.data or [])
                    
                except Exception as e:
                    print(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜ for '{keyword}': {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            seen_ids = set()
            unique_results = []
            for result in all_results:
                result_id = result.get('id')
                if result_id and result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in unique_results:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': 0.5,  # ILIKE ê²€ìƒ‰ ê¸°ë³¸ ì ìˆ˜
                    'search_type': 'ilike',
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'allergens', 'ingredients']}
                })
            
            return formatted_results[:k]
            
        except Exception as e:
            print(f"í´ë°± ILIKE ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def korean_hybrid_search(self, query: str, k: int = 5, user_id: Optional[str] = None, meal_type: Optional[str] = None) -> List[Dict]:
        """í•œê¸€ ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë³‘ë ¬ ì‹¤í–‰ ë°©ì‹)"""
        try:
            print(f"ğŸ” í•œê¸€ ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            all_results = []
            search_strategy = "hybrid"
            search_message = "ì¢…í•© ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."
            
            # ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            print("  ğŸš€ ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ ë³‘ë ¬ ì‹¤í–‰...")
            
            # 1. ë²¡í„° ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ 40% - ê°€ì¥ ë†’ìŒ)
            print("    ğŸ“Š ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰...")
            query_embedding = await self._create_embedding(query)
            vector_results = []
            if query_embedding:
                vector_results = await self._vector_search(query, query_embedding, k, user_id, meal_type)
                for result in vector_results:
                    result['final_score'] = result['search_score'] * 0.4
                    result['search_type'] = 'vector'
                all_results.extend(vector_results)
                print(f"    âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(vector_results)}ê°œ")
            else:
                print("    âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ë²¡í„° ê²€ìƒ‰ ê±´ë„ˆëœ€")
            
            # ğŸš¨ user_idê°€ ìˆìœ¼ë©´ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš© (ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ í•„í„°ë§ ì ìš©)
            # ë‹¤ë¥¸ ê²€ìƒ‰ ë°©ë²•ì€ í•„í„°ë§ì„ ìš°íšŒí•˜ë¯€ë¡œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
            if user_id:
                print("    âš ï¸ ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ í•„í„°ë§ ì ìš© - ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©")
                ilike_exact = []
                fts_results = []
                trigram_results = []
            else:
                # 2. ì •í™•í•œ ILIKE ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 35%)
                print("    ğŸ” ILIKE ì •í™• ë§¤ì¹­ ê²€ìƒ‰...")
                ilike_exact = await self._exact_ilike_search(query, k)
                for result in ilike_exact:
                    result['final_score'] = result['search_score'] * 0.35
                    result['search_type'] = 'exact_ilike'
                all_results.extend(ilike_exact)
                print(f"    âœ… ILIKE ì •í™• ë§¤ì¹­ ì™„ë£Œ: {len(ilike_exact)}ê°œ")
                
                # 3. Full-Text Search (ê°€ì¤‘ì¹˜ 30%)
                print("    ğŸ“ Full-Text Search ì‹¤í–‰...")
                fts_results = await self._full_text_search(query, k)
                for result in fts_results:
                    result['final_score'] = result['search_score'] * 0.3
                    result['search_type'] = 'fts'
                all_results.extend(fts_results)
                print(f"    âœ… FTS ê²€ìƒ‰ ì™„ë£Œ: {len(fts_results)}ê°œ")
                
                # 4. Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ 20%)
                print("    ğŸ”¤ Trigram ê²€ìƒ‰ ì‹¤í–‰...")
                trigram_results = await self._trigram_similarity_search(query, k)
                for result in trigram_results:
                    result['final_score'] = result['search_score'] * 0.2
                    result['search_type'] = 'trigram'
                all_results.extend(trigram_results)
                print(f"    âœ… Trigram ê²€ìƒ‰ ì™„ë£Œ: {len(trigram_results)}ê°œ")
            
            # ê²€ìƒ‰ ì „ëµ ê²°ì • (ê²°ê³¼ ì¢…ë¥˜ì— ë”°ë¼)
            if vector_results and len(vector_results) >= 2:
                search_strategy = "vector_strong"
                search_message = "AI ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            elif ilike_exact and len(ilike_exact) >= 2:
                search_strategy = "exact"
                search_message = "ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            elif fts_results and len(fts_results) >= 2:
                search_strategy = "fts_strong"
                search_message = "ì „ë¬¸ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            elif any([vector_results, ilike_exact, fts_results, trigram_results]):
                search_strategy = "partial"
                search_message = "ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
            
            # ê²°ê³¼ í†µí•© ë° ì •ë ¬
            if not all_results:
                print("    âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
            seen_ids = set()
            unique_results = []
            for result in all_results:
                result_id = result.get('id')
                if result_id and result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
                elif result_id in seen_ids:
                    # ì¤‘ë³µëœ ê²½ìš° ë” ë†’ì€ ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                    for i, existing in enumerate(unique_results):
                        if existing.get('id') == result_id and result['final_score'] > existing['final_score']:
                            unique_results[i] = result
                            break
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
            unique_results.sort(key=lambda x: x['final_score'], reverse=True)
            
            # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
            final_results = unique_results[:k]
            
            # ê²€ìƒ‰ ì „ëµê³¼ ë©”ì‹œì§€ ì¶”ê°€
            for result in final_results:
                result['search_strategy'] = search_strategy
                result['search_message'] = search_message
            
            print(f"  âœ… ìµœì¢… ê²°ê³¼: {len(final_results)}ê°œ (ì „ëµ: {search_strategy})")
            print(f"  ğŸ’¬ {search_message}")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            for i, result in enumerate(final_results[:3], 1):
                print(f"    {i}. {result['title']} (ì ìˆ˜: {result['final_score']:.3f}, íƒ€ì…: {result['search_type']})")
            
            return final_results
            
        except Exception as e:
            print(f"âŒ í•œê¸€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def search(self, query: str, profile: str = "", max_results: int = 5) -> List[Dict]:
        """ê°„ë‹¨í•œ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ (í•œê¸€ ìµœì í™” + ìŠ¤ë§ˆíŠ¸ ê°œì„ )"""
        try:
            # í”„ë¡œí•„ì—ì„œ í•„í„° ì¶”ì¶œ
            filters = {}
            if profile:
                if "ì•„ì¹¨" in profile or "morning" in profile.lower():
                    filters['category'] = 'ì•„ì¹¨'
                if "ì‰¬ìš´" in profile or "easy" in profile.lower():
                    filters['difficulty'] = 'ì‰¬ì›€'
            
            # ë©”ì‹œì§€ì—ì„œ ì‹ì‚¬-ì‹œê°„ í‚¤ì›Œë“œ ê°ì§€ â†’ ë³´ì¡° í‚¤ì›Œë“œë¡œ ê°•í™”
            adjusted_query = query
            meal_hint = None
            if any(k in query for k in ["ì•„ì¹¨", "ë¸Œë ‰í¼ìŠ¤íŠ¸", "ì•„ì¹¨ì‹ì‚¬", "morning", "breakfast"]):
                meal_hint = 'ì•„ì¹¨'
                adjusted_query = f"{query} ì˜¤ë¯ˆë › ê³„ë€ ìƒëŸ¬ë“œ ìš”ê±°íŠ¸"
            elif any(k in query for k in ["ì ì‹¬", "ëŸ°ì¹˜", "lunch"]):
                meal_hint = 'ì ì‹¬'
                adjusted_query = f"{query} ìƒëŸ¬ë“œ ìŠ¤í…Œì´í¬ ë³¶ìŒ êµ¬ì´"
            elif any(k in query for k in ["ì €ë…", "ë””ë„ˆ", "dinner"]):
                meal_hint = 'ì €ë…'
                adjusted_query = f"{query} ìŠ¤í…Œì´í¬ êµ¬ì´ ì°œ ë³¶ìŒ"

            # ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰(ê°•í™” ì¿¼ë¦¬ ìš°ì„ )
            results = await self.korean_hybrid_search(adjusted_query, max_results)
            if not results and adjusted_query != query:
                results = await self.korean_hybrid_search(query, max_results)
            
            # ê²°ê³¼ í¬ë§·íŒ… (ê²€ìƒ‰ ì „ëµê³¼ ë©”ì‹œì§€ í¬í•¨)
            formatted_results = []
            search_message = ""
            search_strategy = "unknown"
            
            for result in results:
                # ì²« ë²ˆì§¸ ê²°ê³¼ì—ì„œ ê²€ìƒ‰ ì „ëµê³¼ ë©”ì‹œì§€ ì¶”ì¶œ
                if not search_message:
                    search_message = result.get('search_message', '')
                    search_strategy = result.get('search_strategy', 'unknown')
                    if meal_hint and not search_message:
                        search_message = f"'{meal_hint}' í‚¤ì›Œë“œë¥¼ ë°˜ì˜í•´ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤."
                
                formatted_results.append({
                    'id': result.get('id', ''),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'similarity': result.get('final_score', 0.0),
                    'metadata': result.get('metadata', {}),
                    'search_types': [result.get('search_type', 'hybrid')],
                    'search_strategy': search_strategy
                })
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ì¶”ê°€
            if not formatted_results:
                formatted_results.append({
                    'title': 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ',
                    'content': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.',
                    'similarity': 0.0,
                    'metadata': {'search_message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'},
                    'search_types': ['none'],
                    'search_strategy': 'none'
                })
            
            # ê²€ìƒ‰ ë©”ì‹œì§€ ì¶œë ¥
            if search_message:
                print(f"ğŸ’¬ ì‚¬ìš©ì ì•ˆë‚´: {search_message}")
            
            return formatted_results
            
        except Exception as e:
            print(f"Search error: {e}")
            return [{
                'title': 'ê²€ìƒ‰ ì˜¤ë¥˜',
                'content': f'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'similarity': 0.0,
                'metadata': {'error': str(e)},
                'search_types': ['error'],
                'search_strategy': 'error'
            }]

    async def smart_search(self, query: str, k: int = 5) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ (ì‚¬ìš©ì í”¼ë“œë°± í¬í•¨)"""
        try:
            print(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
            print("  ğŸ¯ 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ê²€ìƒ‰...")
            fts_results = await self._full_text_search(query, k)
            
            if fts_results and any(result['search_score'] > 0.1 for result in fts_results):
                print(f"    âœ… ì •í™•í•œ ë§¤ì¹­ ë°œê²¬: {len(fts_results)}ê°œ")
                return {
                    'results': fts_results,
                    'search_strategy': 'exact',
                    'message': 'ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.',
                    'total_count': len(fts_results)
                }
            
            # 2ë‹¨ê³„: ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            print("  ğŸ” 2ë‹¨ê³„: ë¶€ë¶„ ë§¤ì¹­ ê²€ìƒ‰...")
            trigram_results = await self._trigram_similarity_search(query, k)
            ilike_results = await self._fallback_ilike_search(query, k)
            
            if trigram_results or ilike_results:
                print(f"    âœ… ë¶€ë¶„ ë§¤ì¹­ ë°œê²¬: Trigram {len(trigram_results)}ê°œ, ILIKE {len(ilike_results)}ê°œ")
                
                # ê²°ê³¼ í†µí•©
                all_partial_results = []
                all_partial_results.extend(trigram_results)
                all_partial_results.extend(ilike_results)
                
                # ì¤‘ë³µ ì œê±°
                seen_ids = set()
                unique_results = []
                for result in all_partial_results:
                    result_id = result.get('id')
                    if result_id and result_id not in seen_ids:
                        seen_ids.add(result_id)
                        unique_results.append(result)
                
                return {
                    'results': unique_results[:k],
                    'search_strategy': 'partial',
                    'message': 'ì •í™•í•œ ê²€ìƒ‰ì–´ê°€ ì—†ì–´ì„œ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.',
                    'total_count': len(unique_results)
                }
            
            # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            print("  ğŸ”„ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰...")
            hybrid_results = await self.korean_hybrid_search(query, k)
            
            if hybrid_results:
                return {
                    'results': hybrid_results,
                    'search_strategy': 'hybrid',
                    'message': 'ì¢…í•© ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'total_count': len(hybrid_results)
                }
            
            # 4ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            print("    âŒ ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ì—ì„œ ê²°ê³¼ ì—†ìŒ")
            return {
                'results': [],
                'search_strategy': 'none',
                'message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.',
                'total_count': 0
            }
            
        except Exception as e:
            print(f"âŒ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'results': [],
                'search_strategy': 'error',
                'message': f'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'total_count': 0
            }

# ì „ì—­ í•œê¸€ ê²€ìƒ‰ ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤
korean_search_tool = KoreanSearchTool()
