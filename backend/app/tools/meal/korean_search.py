"""
í•œê¸€ ìµœì í™” ê²€ìƒ‰ ë„êµ¬
PostgreSQL Full-Text Search + pg_trgm + ë²¡í„° ê²€ìƒ‰ í†µí•©
"""

import re
import openai
import asyncio
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from app.core.database import supabase
from app.core.config import settings
from app.core.redis_cache import redis_cache

class KoreanSearchTool:
    """í•œê¸€ ìµœì í™” ê²€ìƒ‰ ë„êµ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supabase = supabase
        self.openai_client = openai.OpenAI(api_key=settings.openai_api_key)
        
        # ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ
        synonym_file = Path(__file__).parent.parent.parent / 'data' / 'ingredient_synonyms.json'
        try:
            with open(synonym_file, 'r', encoding='utf-8') as f:
                self.synonym_data = json.load(f)
                print(f"âœ… ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {synonym_file}")
        except Exception as e:
            print(f"âš ï¸ ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.synonym_data = {"ì•Œë ˆë¥´ê¸°": {}, "ë¹„ì„ í˜¸": {}}
        
        # ì„ë² ë”© ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)
        self._embedding_cache = {}
        self._normalization_cache = {}
        self._expansion_cache = {}
        self._query_embedding_cache = {}  # ì¿¼ë¦¬ ì„ë² ë”© ìºì‹œ ì¶”ê°€
        self._search_results_cache = {}   # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì¶”ê°€
        
        # ìºì‹œ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        self._max_cache_size = 100  # ìµœëŒ€ 100ê°œ í•­ëª©
    
    def _manage_cache_size(self, cache_dict: dict):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ (LRU ë°©ì‹)"""
        if len(cache_dict) > self._max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (FIFO)
            oldest_key = next(iter(cache_dict))
            del cache_dict[oldest_key]
            print(f"    ğŸ“Š ìºì‹œ í¬ê¸° ê´€ë¦¬: {oldest_key[:30]}... ì œê±°")
    
    def _expand_with_synonyms(self, words: List[str], category: str) -> List[str]:
        """ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë™ì˜ì–´ë¡œ í™•ì¥ (ìºì‹± ì ìš©)"""
        if not words:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_key = f"expand_{category}_{hash(tuple(sorted(words)))}"
        if cache_key in self._expansion_cache:
            print(f"ğŸ“Š í™•ì¥ ìºì‹œ íˆíŠ¸: {words}")
            return self._expansion_cache[cache_key]
        
        expanded = []
        synonym_dict = self.synonym_data.get(category, {})
        
        for word in words:
            expanded.append(word)  # ì›ë˜ ë‹¨ì–´ ì¶”ê°€
            if word in synonym_dict:
                # ë™ì˜ì–´ ì¶”ê°€ (ìµœëŒ€ 5ê°œë¡œ ì œí•œ)
                synonyms = synonym_dict[word][:5]  # ì²˜ìŒ 5ê°œë§Œ ì‚¬ìš©
                expanded.extend(synonyms)
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        seen = set()
        unique_expanded = []
        for item in expanded:
            if item not in seen:
                seen.add(item)
                unique_expanded.append(item)
        
        # ìºì‹œ ì €ì¥
        self._expansion_cache[cache_key] = unique_expanded
        self._manage_cache_size(self._expansion_cache)
        print(f"âœ… ë™ì˜ì–´ í™•ì¥ ì™„ë£Œ: {words} â†’ {len(unique_expanded)}ê°œ (ìºì‹œ ì €ì¥)")
        print(f"    ğŸ” í™•ì¥ëœ í•­ëª©ë“¤: {unique_expanded[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        return unique_expanded
    
    def _normalize_to_canonical(self, words: List[str], category: str) -> List[str]:
        """ì…ë ¥ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ í‘œì¤€ëª…(canonical)ìœ¼ë¡œ ì •ê·œí™” (ìºì‹± ì ìš©)
        
        Args:
            words: ì…ë ¥ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸/ê²€ìƒ‰ í‚¤ì›Œë“œ)
            category: ì¹´í…Œê³ ë¦¬ ("ì•Œë ˆë¥´ê¸°" ë˜ëŠ” "ë¹„ì„ í˜¸")
        
        Returns:
            í‘œì¤€ëª… ë¦¬ìŠ¤íŠ¸ (ì •í™• ë¹„êµìš©)
        """
        if not words:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_key = f"normalize_{category}_{hash(tuple(sorted(words)))}"
        if cache_key in self._normalization_cache:
            print(f"ğŸ“Š ì •ê·œí™” ìºì‹œ íˆíŠ¸: {words}")
            return self._normalization_cache[cache_key]
        
        synonym_dict = self.synonym_data.get(category, {})
        canonicals = []
        
        for word in words:
            if not word:
                continue
            
            # ì†Œë¬¸ìí™” ë° ê³µë°± íŠ¸ë¦¼
            normalized = word.strip().lower()
            
            # ì—­ë§¤í•‘: ë™ì˜ì–´ â†’ í‘œì¤€ëª…
            found_canonical = None
            for canonical, synonyms in synonym_dict.items():
                # í‘œì¤€ëª… ìì²´ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
                if normalized == canonical.lower():
                    found_canonical = canonical
                    break
                # ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
                for syn in synonyms:
                    if normalized == syn.lower():
                        found_canonical = canonical
                        break
                if found_canonical:
                    break
            
            if found_canonical:
                canonicals.append(found_canonical)
            else:
                # í‘œì¤€ëª…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì›ë³¸ ì¶”ê°€
                canonicals.append(word.strip())
        
        # ì¤‘ë³µ ì œê±°
        result = list(set(canonicals))
        
        # ìºì‹œ ì €ì¥
        self._normalization_cache[cache_key] = result
        self._manage_cache_size(self._normalization_cache)
        print(f"âœ… ì •ê·œí™” ì™„ë£Œ: {words} â†’ {result} (ìºì‹œ ì €ì¥)")
        return result
    
    def _tokenize_ingredients(self, text: str) -> List[str]:
        """ì¬ë£Œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
        
        Args:
            text: ì¬ë£Œ ë¬¸ìì—´
        
        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸ (ê³µë°±, ì‰¼í‘œ, íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€ ë¶„ë¦¬)
        """
        if not text:
            return []
        
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ (ê³µë°±ê³¼ êµ¬ë¶„ì ê¸°ì¤€)
        tokens = re.split(r'[,\s\(\)\[\]\{\}/]+', text.lower())
        
        # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ì •ê·œí™”
        return [t.strip() for t in tokens if t.strip()]
    
    def _exact_match_filter(self, text: str, banned_terms: List[str]) -> bool:
        """ì •í™• ë§¤ì¹­ í•„í„° (ë¶€ë¶„ë¬¸ìì—´ ê¸ˆì§€)
        
        Args:
            text: ê²€ì‚¬í•  í…ìŠ¤íŠ¸ (ì œëª© ë˜ëŠ” ì¬ë£Œ)
            banned_terms: ê¸ˆì§€ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (í‘œì¤€ëª…)
        
        Returns:
            True if ê¸ˆì§€ ë‹¨ì–´ê°€ ë°œê²¬ë¨, False otherwise
        """
        if not text or not banned_terms:
            return False
        
        # í…ìŠ¤íŠ¸ í† í°í™”
        tokens = self._tokenize_ingredients(text)
        
        # í‘œì¤€ëª…ì„ í‘œì¤€ëª…ìœ¼ë¡œ ì •ê·œí™” (ë™ì˜ì–´ í™•ì¥)
        for banned in banned_terms:
            banned_lower = banned.lower()
            
            # í•œê¸€ í† í° ì •í™• ì¼ì¹˜
            for token in tokens:
                if token == banned_lower:
                    return True
            
            # ì˜ë¬¸ ë‹¨ì–´ ê²½ê³„ ì •ê·œì‹ (ê³µë°±ì´ ìˆëŠ” ê²½ìš° ëŒ€ë¹„)
            # ì˜ˆ: "bell pepper", "sesame oil" ë“±
            if ' ' in banned_lower:
                # ë³µí•©ì–´ëŠ” ì›ë¬¸ì—ì„œ ì§ì ‘ ê²€ìƒ‰
                if banned_lower in text.lower():
                    return True
        
        return False
    
    async def _create_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ìºì‹± ì ìš©)"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"embedding_{hash(text)}"
            if cache_key in self._embedding_cache:
                print(f"ğŸ“Š ì„ë² ë”© ìºì‹œ íˆíŠ¸: {text[:50]}...")
                return self._embedding_cache[cache_key]
            
            print(f"ğŸ“Š ì„ë² ë”© ìƒì„± ì¤‘: {text[:50]}...")
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            embedding = response.data[0].embedding
            
            # ìºì‹œ ì €ì¥
            self._embedding_cache[cache_key] = embedding
            self._manage_cache_size(self._embedding_cache)
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding)}ì°¨ì› (ìºì‹œ ì €ì¥)")
            return embedding
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_korean_keywords(self, query: str) -> List[str]:
        """í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ê·œí™”"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]+', query)
        
        # 2ê¸€ì ì´ìƒë§Œ í•„í„°ë§
        keywords = [kw for kw in keywords if len(kw) >= 2]
        
        # í•œê¸€ í‚¤ì›Œë“œ ì •ê·œí™” (ì¡°ì‚¬ ì œê±° ë“±)
        normalized_keywords = []
        for keyword in keywords:
            # í•œê¸€ì¸ ê²½ìš° ì¡°ì‚¬ ì œê±°
            if re.match(r'[ê°€-í£]+', keyword):
                # ê°„ë‹¨í•œ ì¡°ì‚¬ ì œê±° (ë” ì •êµí•œ í˜•íƒœì†Œ ë¶„ì„ í•„ìš”ì‹œ KoNLPy ì‚¬ìš©)
                normalized = re.sub(r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì—|ì—ì„œ|ë¡œ|ìœ¼ë¡œ|ì™€|ê³¼|ì˜|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ë¶€í„°|í•œí…Œ|ì—ê²Œ)$', '', keyword)
                if len(normalized) >= 2:
                    normalized_keywords.append(normalized)
            else:
                normalized_keywords.append(keyword)
        
        return normalized_keywords
    
    def _generate_query_variants(self, query: str) -> List[str]:
        """ì‚¬ìš©ì ê²€ìƒ‰ì–´ë¥¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì •ê·œí™”í•´ ë³€í˜• ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±.
        - ë¶ˆìš©ì–´ ì œê±°: ë ˆì‹œí”¼/ë§Œë“œëŠ”ë²•/ë§Œë“œëŠ” ë²•/ìš”ë¦¬ ë“±
        - ê³µë°± ì œê±°/í† í° ë¶„ë¦¬/OR í† í°
        """
        q = (query or '').strip()
        if not q:
            return []

        stopwords = ['ë ˆì‹œí”¼', 'ë§Œë“œëŠ”ë²•', 'ë§Œë“œëŠ” ë²•', 'ìš”ë¦¬', 'ë°©ë²•']
        base = q
        for sw in stopwords:
            base = base.replace(sw, '').strip()

        # í† í°í™”(ê³µë°± ê¸°ì¤€)
        tokens = [t for t in base.split() if t]

        variants = []
        variants.append(q)            # ì›ë¬¸
        if base and base != q:
            variants.append(base)     # ë¶ˆìš©ì–´ ì œê±°
        if tokens:
            joined = ' '.join(tokens)
            if joined not in variants:
                variants.append(joined)
            nospace = ''.join(tokens)
            if nospace and nospace not in variants:
                variants.append(nospace)
            # OR í† í°(ë‹¹ê·¼|ë¼í˜|ê¹€ë°¥)
            if len(tokens) > 1:
                or_tokens = '|'.join(tokens)
                variants.append(or_tokens)

        # ì¤‘ë³µ ì œê±° ìœ ì§€ ìˆœì„œ
        seen = set()
        uniq = []
        for v in variants:
            if v and v not in seen:
                uniq.append(v)
                seen.add(v)
        return uniq[:5]

    async def _exact_ilike_search(self, query: str, k: int) -> List[Dict]:
        """ì •í™• ë§¤ì¹­ì— ê°€ê¹Œìš´ ILIKE ê¸°ë°˜ ê²€ìƒ‰(RPC ì‚¬ìš©).
        ë³€í˜• ì¿¼ë¦¬(ë¶ˆìš©ì–´ ì œê±°/ê³µë°± ì œê±°/OR í† í°)ë¥¼ ìˆœì°¨ ì‹œë„í•˜ì—¬
        ìµœì´ˆë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.
        """
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []

            for vq in self._generate_query_variants(query):
                try:
                    res = self.supabase.rpc('ilike_search', {'query_text': vq, 'match_count': k}).execute()
                    rows = res.data or []
                    if rows:
                        formatted = []
                        for row in rows:
                            formatted.append({
                                'id': str(row.get('id', '')),
                                'title': row.get('title', 'ì œëª© ì—†ìŒ'),
                                'content': row.get('content', ''),
                                'allergens': row.get('allergens', []),
                                'ingredients': row.get('ingredients', []),
                                'search_score': row.get('search_score', 1.0),
                                'search_type': 'ilike_exact',
                                'url': row.get('url'),  # URL ì¶”ê°€
                                'metadata': {kk: vv for kk, vv in row.items() if kk not in ['id','title','content','search_score','allergens','ingredients','url']}
                            })
                        return formatted
                except Exception as e:
                    print(f"ILIKE ì •í™• ë§¤ì¹­ RPC ì˜¤ë¥˜({vq}): {e}")
                    continue
            return []
        except Exception as e:
            print(f"ILIKE ì •í™• ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return []
    async def _groonga_search(self, query: str, k: int) -> List[Dict]:
        """PGroonga ê²€ìƒ‰ (ì œëª©/ë³¸ë¬¸ ìš°ì„  ì •í™• ë§¤ì¹­)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            results = self.supabase.rpc('groonga_search', {
                'query_text': query,
                'match_count': k
            }).execute()

            formatted_results = []
            for result in results.data or []:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', 1.0),
                    'search_type': 'pgroonga',
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'search_score', 'allergens', 'ingredients']}
                })

            return formatted_results
        except Exception as e:
            print(f"PGroonga ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _full_text_search(self, query: str, k: int) -> List[Dict]:
        """PostgreSQL Full-Text Search (í•œê¸€ ìµœì í™”)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            # Full-Text Search ì‹¤í–‰ (RPC í•¨ìˆ˜ ì‚¬ìš©)
            results = self.supabase.rpc('fts_search', {
                'query_text': query,
                'match_count': k
            }).execute()
            
            formatted_results = []
            for result in results.data or []:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', result.get('fts_score', 0.0)),
                    'search_type': 'fts',
                    'url': result.get('url'),  # URL ì¶”ê°€
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'fts_score', 'allergens', 'ingredients', 'url']}
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"Full-Text Search ì˜¤ë¥˜: {e}")
            return []
    
    async def _trigram_similarity_search(self, query: str, k: int) -> List[Dict]:
        """Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ (í•œê¸€ ìœ ì‚¬ë„)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            # Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ (RPC í•¨ìˆ˜ ì‚¬ìš©)
            results = self.supabase.rpc('trgm_search', {
                'query_text': query,
                'match_count': k
            }).execute()
            
            formatted_results = []
            for result in results.data or []:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', result.get('similarity_score', 0.0)),
                    'search_type': 'trigram',
                    'url': result.get('url'),  # URL ì¶”ê°€
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'similarity_score', 'allergens', 'ingredients', 'url']}
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _vector_search(self, query: str, query_embedding: List[float], k: int, user_id: Optional[str] = None, meal_type: Optional[str] = None,
                            allergies: Optional[List[str]] = None, dislikes: Optional[List[str]] = None) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ (ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ í•„í„°ë§ + ì„ì‹œ ì œì•½ì¡°ê±´)"""
        try:
            # print(f"ğŸ” DEBUG: _vector_search í•¨ìˆ˜ ì‹œì‘ - query='{query}', k={k}, user_id={user_id}")  # ì„ì‹œ ë¹„í™œì„±í™”
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                # print(f"ğŸ” DEBUG: Supabaseê°€ Noneì´ê±°ë‚˜ DummySupabase - ë¹ˆ ê²°ê³¼ ë°˜í™˜")  # ì„ì‹œ ë¹„í™œì„±í™”
                return []
            
            # ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ ì •ë³´ ì¤€ë¹„
            exclude_allergens_embeddings = None
            exclude_dislikes_embeddings = None
            exclude_allergens_names = None
            exclude_dislikes_names = None
            
            # 1. íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ëœ allergies/dislikesê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            user_allergies = allergies if allergies is not None else []
            user_dislikes = dislikes if dislikes is not None else []
            # print(f"    ğŸ” ì „ë‹¬ë°›ì€ ì•Œë ˆë¥´ê¸°: {user_allergies}")  # ì„ì‹œ ë¹„í™œì„±í™”
            # print(f"    ğŸ” ì „ë‹¬ë°›ì€ ë¹„ì„ í˜¸: {user_dislikes}")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            # 2. íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ í”„ë¡œí•„ì—ì„œ ì¡°íšŒ
            if not user_allergies and not user_dislikes and user_id:
            # print(f"    ğŸ” í”„ë¡œí•„ì—ì„œ ì•Œë ˆë¥´ê¸° ì •ë³´ ì¡°íšŒ: user_id={user_id}")  # ì„ì‹œ ë¹„í™œì„±í™”
                from app.tools.shared.profile_tool import user_profile_tool
                user_preferences = await user_profile_tool.get_user_preferences(user_id)
                
                if user_preferences.get("success"):
                    prefs = user_preferences["preferences"]
                    user_allergies = prefs.get("allergies", [])
                    user_dislikes = prefs.get("dislikes", [])
                    # print(f"    ğŸ” í”„ë¡œí•„ì—ì„œ ì¡°íšŒëœ ì•Œë ˆë¥´ê¸°: {user_allergies}")  # ì„ì‹œ ë¹„í™œì„±í™”
                    # print(f"    ğŸ” í”„ë¡œí•„ì—ì„œ ì¡°íšŒëœ ë¹„ì„ í˜¸: {user_dislikes}")  # ì„ì‹œ ë¹„í™œì„±í™”
                else:
                    print(f"    âš ï¸ í”„ë¡œí•„ ì¡°íšŒ ì‹¤íŒ¨: {user_preferences}")
            
            # ì•Œë ˆë¥´ê¸° ì„ë² ë”© ìƒì„±
            if user_allergies:
                allergy_text = ' '.join(user_allergies)
                allergy_embedding = await self._create_embedding(allergy_text)
                exclude_allergens_embeddings = [allergy_embedding]
                exclude_allergens_names = user_allergies
                # print(f"ğŸ” ì•Œë ˆë¥´ê¸° ì„ë² ë”© ìƒì„± (1ê°œ): {user_allergies}")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            # ë¹„ì„ í˜¸ ì„ë² ë”© ìƒì„±
            if user_dislikes:
                dislike_text = ' '.join(user_dislikes)
                dislike_embedding = await self._create_embedding(dislike_text)
                exclude_dislikes_embeddings = [dislike_embedding]
                exclude_dislikes_names = user_dislikes
                print(f"ğŸ” ë¹„ì„ í˜¸ ì„ë² ë”© ìƒì„± (1ê°œ): {user_dislikes}")
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ (RPC í•¨ìˆ˜ ì‚¬ìš©) - ìµœëŒ€í•œ ë§ì€ ë°ì´í„° ê²€ìƒ‰
            # ì•Œë ˆë¥´ê¸° í•„í„°ë§ì„ ê³ ë ¤í•˜ì—¬ ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´
            max_search_count = 1000  # ìµœëŒ€ 1000ê°œ ê²€ìƒ‰ (DBì˜ ëª¨ë“  ë°ì´í„°)
            
            rpc_params = {
                'query_embedding': query_embedding,
                'match_count': max_search_count,
                'similarity_threshold': 0.0
            }
            # print(f"    ğŸ” ìµœëŒ€ ê²€ìƒ‰ ìˆ˜: {max_search_count}ê°œ (ì•Œë ˆë¥´ê¸° í•„í„°ë§ ê³ ë ¤)")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            # ë‹¨ì¼ ë²¡í„°ë¡œ ì „ë‹¬ (ë°°ì—´ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ)
            if exclude_allergens_embeddings:
                rpc_params['exclude_allergens_embedding'] = exclude_allergens_embeddings[0]
            if exclude_dislikes_embeddings:
                rpc_params['exclude_dislikes_embedding'] = exclude_dislikes_embeddings[0]
            if exclude_allergens_names:
                rpc_params['exclude_allergens_names'] = exclude_allergens_names
            if exclude_dislikes_names:
                rpc_params['exclude_dislikes_names'] = exclude_dislikes_names
            
            # ğŸ†• meal_type í•„í„° ì¶”ê°€ (í•­ìƒ ì „ë‹¬í•˜ì—¬ í•¨ìˆ˜ ì˜¤ë²„ë¡œë”© ëª¨í˜¸ì„± ì œê±°)
            rpc_params['meal_type_filter'] = meal_type if meal_type else None
            if meal_type:
                print(f"ğŸ½ï¸ meal_type í•„í„° ì ìš©: {meal_type}")
            
            print(f"ğŸ” RPC íŒŒë¼ë¯¸í„°: allergens={len(exclude_allergens_names) if exclude_allergens_names else 0}, dislikes={len(exclude_dislikes_names) if exclude_dislikes_names else 0}")
            
            results = self.supabase.rpc('vector_search', rpc_params).execute()
            
            formatted_results = []
            filtered_count = 0
            
            # í‘œì¤€ëª…ìœ¼ë¡œ ì •ê·œí™” (ë™ì˜ì–´ í™•ì¥ ëŒ€ì‹ )
            canonical_allergens = self._normalize_to_canonical(exclude_allergens_names, 'ì•Œë ˆë¥´ê¸°') if exclude_allergens_names else []
            canonical_dislikes = self._normalize_to_canonical(exclude_dislikes_names, 'ë¹„ì„ í˜¸') if exclude_dislikes_names else []
            
            # ë™ì˜ì–´ í™•ì¥ (ìºì‹±ëœ í•¨ìˆ˜ ì‚¬ìš©)
            expanded_allergens = self._expand_with_synonyms(canonical_allergens, 'ì•Œë ˆë¥´ê¸°') if canonical_allergens else []
            expanded_dislikes = self._expand_with_synonyms(canonical_dislikes, 'ë¹„ì„ í˜¸') if canonical_dislikes else []
            
            # print(f"    ğŸ” ì •ê·œí™”ëœ ì•Œë ˆë¥´ê¸°: {canonical_allergens}")  # ì„ì‹œ ë¹„í™œì„±í™”
            # print(f"    ğŸ” í™•ì¥ëœ ì•Œë ˆë¥´ê¸°: {len(expanded_allergens)}ê°œ - {expanded_allergens[:10]}...")  # ì„ì‹œ ë¹„í™œì„±í™”
            # print(f"    ğŸ” ì •ê·œí™”ëœ ë¹„ì„ í˜¸: {canonical_dislikes}")  # ì„ì‹œ ë¹„í™œì„±í™”
            # print(f"    ğŸ” í™•ì¥ëœ ë¹„ì„ í˜¸: {len(expanded_dislikes)}ê°œ - {expanded_dislikes[:10]}...")  # ì„ì‹œ ë¹„í™œì„±í™”
            # print(f"    ğŸš¨ ì•Œë ˆë¥´ê¸° í•„í„°ë§ ì‹œì‘ - ì´ {len(results.data or [])}ê°œ ê²°ê³¼ ê²€ì‚¬")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            for result in results.data or []:
                # ğŸš¨ Python ë ˆë²¨ í•„í„°ë§: title, ingredientsì—ì„œ ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ ì²´í¬
                title = result.get('title', '')
                ingredients = result.get('ingredients', [])
                should_skip = False
                # print(f"    ğŸ” ê²€ì‚¬ ì¤‘: '{title}' (ì¬ë£Œ: {ingredients[:3]}...)")  # ì„ì‹œ ë¹„í™œì„±í™”
                
                # ì•Œë ˆë¥´ê¸° ì²´í¬ (í† í° ë§¤ì¹­ + ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­)
                # print(f"    ğŸ” ì•Œë ˆë¥´ê¸° ì²´í¬ ì¡°ê±´: expanded_allergens={len(expanded_allergens) if expanded_allergens else 0}, should_skip={should_skip}")  # ì„ì‹œ ë¹„í™œì„±í™”
                if expanded_allergens and not should_skip:
                    # ì œëª© ì²´í¬ (í† í° ë§¤ì¹­)
                    if self._exact_match_filter(title, expanded_allergens):
                        print(f"    âš ï¸ ì•Œë ˆë¥´ê¸° ì œì™¸: '{title}' (ì œëª©ì— ì•Œë ˆë¥´ê¸° ì¬ë£Œ í¬í•¨)")
                        filtered_count += 1
                        should_skip = True
                    
                    # ì œëª© ì²´í¬ (ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ - "ê³„ë€ìƒëŸ¬ë“œ" ê°™ì€ ê²½ìš°)
                    if not should_skip:
                        title_lower = title.lower()
                        for allergen in expanded_allergens:
                            if allergen in title_lower:
                                print(f"    âš ï¸ ì•Œë ˆë¥´ê¸° ì œì™¸: '{title}' (ì œëª©ì— '{allergen}' í¬í•¨)")
                                print(f"        ğŸ” ë§¤ì¹­ëœ ì•Œë ˆë¥´ê¸°: '{allergen}' in '{title_lower}'")
                                filtered_count += 1
                                should_skip = True
                                break
                    
                    # ì¬ë£Œ ì²´í¬
                    if not should_skip:
                        for ing in ingredients:
                            if self._exact_match_filter(ing, expanded_allergens):
                                print(f"    âš ï¸ ì•Œë ˆë¥´ê¸° ì œì™¸: '{title}' (ì¬ë£Œ '{ing}'ì— ì•Œë ˆë¥´ê¸° ì¬ë£Œ í¬í•¨)")
                                print(f"        ğŸ” ë§¤ì¹­ëœ ì¬ë£Œ: '{ing}' in ì•Œë ˆë¥´ê¸° ëª©ë¡")
                                filtered_count += 1
                                should_skip = True
                                break
                
                # ë¹„ì„ í˜¸ ì²´í¬ (í† í° ë§¤ì¹­ + ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­)
                if expanded_dislikes and not should_skip:
                    # ì œëª© ì²´í¬ (í† í° ë§¤ì¹­)
                    if self._exact_match_filter(title, expanded_dislikes):
                        # print(f"    âš ï¸ ë¹„ì„ í˜¸ ì œì™¸: '{title}' (ì œëª©ì— ë¹„ì„ í˜¸ ì¬ë£Œ í¬í•¨)")  # ì„ì‹œ ë¹„í™œì„±í™”
                        filtered_count += 1
                        should_skip = True
                    
                    # ì œëª© ì²´í¬ (ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ - "ê³„ë€ìƒëŸ¬ë“œ" ê°™ì€ ê²½ìš°)
                    if not should_skip:
                        title_lower = title.lower()
                        for dislike in expanded_dislikes:
                            if dislike in title_lower:
                                # print(f"    âš ï¸ ë¹„ì„ í˜¸ ì œì™¸: '{title}' (ì œëª©ì— '{dislike}' í¬í•¨)")  # ì„ì‹œ ë¹„í™œì„±í™”
                                filtered_count += 1
                                should_skip = True
                                break
                    
                    # ì¬ë£Œ ì²´í¬
                    if not should_skip:
                        for ing in ingredients:
                            if self._exact_match_filter(ing, expanded_dislikes):
                                # print(f"    âš ï¸ ë¹„ì„ í˜¸ ì œì™¸: '{title}' (ì¬ë£Œ '{ing}'ì— ë¹„ì„ í˜¸ ì¬ë£Œ í¬í•¨)")  # ì„ì‹œ ë¹„í™œì„±í™”
                                filtered_count += 1
                                should_skip = True
                                break
                
                if should_skip:
                    continue
                
                # í†µê³¼!
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': result.get('search_score', result.get('similarity_score', 0.0)),
                    'search_type': 'vector',
                    'url': result.get('url'),  # URL ì¶”ê°€
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'similarity_score', 'allergens', 'ingredients', 'url']}
                })
            
            if filtered_count > 0:
                print(f"    ğŸ” Python í•„í„°ë§: {filtered_count}ê°œ ì œì™¸ë¨")
            
            print(f"    âœ… ìµœì¢… ê²°ê³¼: {len(formatted_results)}ê°œ (ê²€ìƒ‰ {len(results.data or [])}ê°œ â†’ í•„í„°ë§ í›„ {len(formatted_results)}ê°œ)")
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” DEBUG: ë²¡í„° ê²€ìƒ‰ ì˜ˆì™¸ ìƒì„¸ ì •ë³´:")
            traceback.print_exc()
            return []
    
    async def _fallback_ilike_search(self, query: str, k: int) -> List[Dict]:
        """í´ë°± ILIKE ê²€ìƒ‰ (ê¸°ì¡´)"""
        try:
            if isinstance(self.supabase, type(None)) or hasattr(self.supabase, '__class__') and 'DummySupabase' in str(self.supabase.__class__):
                return []
            
            keywords = self._extract_korean_keywords(query)
            if not keywords:
                return []
            
            all_results = []
            
            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                try:
                    # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš© (JSONB ê²€ìƒ‰ ì œê±°)
                    title_results = self.supabase.table('recipe_blob_emb').select('*').ilike('title', f'%{keyword}%').limit(k).execute()
                    
                    all_results.extend(title_results.data or [])
                    
                except Exception as e:
                    print(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜ for '{keyword}': {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            seen_ids = set()
            unique_results = []
            for result in all_results:
                result_id = result.get('id')
                if result_id and result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in unique_results:
                formatted_results.append({
                    'id': str(result.get('id', '')),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'search_score': 0.5,  # ILIKE ê²€ìƒ‰ ê¸°ë³¸ ì ìˆ˜
                    'search_type': 'ilike',
                    'url': result.get('url'),  # URL ì¶”ê°€
                    'metadata': {k: v for k, v in result.items() if k not in ['id', 'title', 'content', 'allergens', 'ingredients', 'url']}
                })
            
            return formatted_results[:k]
            
        except Exception as e:
            print(f"í´ë°± ILIKE ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def korean_hybrid_search(self, query: str, k: int = 5, user_id: Optional[str] = None, meal_type: Optional[str] = None, 
                                   allergies: Optional[List[str]] = None, dislikes: Optional[List[str]] = None) -> List[Dict]:
        """í•œê¸€ ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë³‘ë ¬ ì‹¤í–‰ ë°©ì‹ + ê²°ê³¼ ìºì‹±)"""
        try:
            print(f"ğŸ” í•œê¸€ ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ: ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ëœë¤ ìš”ì†Œ ì¶”ê°€
            import random
            import time
            
            # ê¸°ë³¸ ìºì‹œ í‚¤
            base_cache_key = f"search_{hash(query)}_{k}_{user_id}_{meal_type}_{hash(tuple(sorted(allergies or [])))}_{hash(tuple(sorted(dislikes or [])))}"
            
            # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ëœë¤ ìš”ì†Œ (ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ìœ„í•´ ìºì‹œ ë¹„í™œì„±í™”)
            # random_seed = int(time.time() / 300)  # 5ë¶„ë§ˆë‹¤ ë³€ê²½
            # random_factor = random.randint(1, 10)
            # cache_key = f"{base_cache_key}_{random_seed}_{random_factor}"
            
            # ğŸš€ ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ ìºì‹œ ë¹„í™œì„±í™” (ë§¤ë²ˆ ìƒˆë¡œìš´ ê²°ê³¼)
            cache_key = f"{base_cache_key}_{int(time.time())}_{random.randint(1, 1000)}"
            
            print(f"  ğŸ² ìŠ¤ë§ˆíŠ¸ ìºì‹œ í‚¤: {cache_key}")
            
            # Redis ìºì‹œ í™•ì¸
            cached_result = redis_cache.get(cache_key)
            if cached_result:
                print(f"    ğŸ“Š Redis ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                return cached_result
            
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (í´ë°±)
            if cache_key in self._search_results_cache:
                print(f"    ğŸ“Š ë©”ëª¨ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                return self._search_results_cache[cache_key]
            
            all_results = []
            search_strategy = "hybrid"
            search_message = "ì¢…í•© ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."
            
            # ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            print("  ğŸš€ ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ ë³‘ë ¬ ì‹¤í–‰...")
            
            # 1. ë²¡í„° ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ 40% - ê°€ì¥ ë†’ìŒ)
            print("    ğŸ“Š ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰...")
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìºì‹± (Redis ìš°ì„ , ë©”ëª¨ë¦¬ í´ë°±)
            query_cache_key = f"query_{hash(query)}"
            
            # Redisì—ì„œ ì¿¼ë¦¬ ì„ë² ë”© í™•ì¸
            cached_embedding = redis_cache.get(query_cache_key)
            if cached_embedding:
                print(f"    ğŸ“Š Redis ì¿¼ë¦¬ ì„ë² ë”© ìºì‹œ íˆíŠ¸: {query[:30]}...")
                query_embedding = cached_embedding
            elif query_cache_key in self._query_embedding_cache:
                print(f"    ğŸ“Š ì¿¼ë¦¬ ì„ë² ë”© ìºì‹œ íˆíŠ¸: {query[:30]}...")
                query_embedding = self._query_embedding_cache[query_cache_key]
            else:
                query_embedding = await self._create_embedding(query)
                if query_embedding:
                    # Redisì— ì¿¼ë¦¬ ì„ë² ë”© ì €ì¥ (TTL: 1ì‹œê°„)
                    redis_cache.set(query_cache_key, query_embedding, ttl=3600)
                    # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥ (í´ë°±ìš©)
                    self._query_embedding_cache[query_cache_key] = query_embedding
                    self._manage_cache_size(self._query_embedding_cache)
                    print(f"    ğŸ“Š ì¿¼ë¦¬ ì„ë² ë”© ìºì‹œ ì €ì¥: {query[:30]}...")
            
            vector_results = []
            if query_embedding:
                vector_results = await self._vector_search(query, query_embedding, k, user_id, meal_type, allergies, dislikes)
                for result in vector_results:
                    result['final_score'] = result['search_score'] * 0.4
                    result['search_type'] = 'vector'
                all_results.extend(vector_results)
                # print(f"    âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(vector_results)}ê°œ")  # ì„ì‹œ ë¹„í™œì„±í™”
            else:
                print("    âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ë²¡í„° ê²€ìƒ‰ ê±´ë„ˆëœ€")
            
            # ğŸš¨ ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ í•„í„°ë§ì´ ìˆì–´ë„ ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ ì‚¬ìš© (ê²°ê³¼ í™•ë³´ ìš°ì„ )
            has_filters = (allergies and len(allergies) > 0) or (dislikes and len(dislikes) > 0) or user_id
            if has_filters:
                print("    âš ï¸ ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ í•„í„°ë§ ì ìš© - ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ ì‚¬ìš© (ê²°ê³¼ í™•ë³´ ìš°ì„ )")
            
            # ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ ì‹¤í–‰ (í•„í„°ë§ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            # 2. ì •í™•í•œ ILIKE ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 35%)
            print("    ğŸ” ILIKE ì •í™• ë§¤ì¹­ ê²€ìƒ‰...")
            ilike_exact = await self._exact_ilike_search(query, k)
            for result in ilike_exact:
                result['final_score'] = result['search_score'] * 0.35
                result['search_type'] = 'exact_ilike'
            all_results.extend(ilike_exact)
            # print(f"    âœ… ILIKE ì •í™• ë§¤ì¹­ ì™„ë£Œ: {len(ilike_exact)}ê°œ")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            # 3. Full-Text Search (ê°€ì¤‘ì¹˜ 30%)
            print("    ğŸ“ Full-Text Search ì‹¤í–‰...")
            fts_results = await self._full_text_search(query, k)
            for result in fts_results:
                result['final_score'] = result['search_score'] * 0.3
                result['search_type'] = 'fts'
            all_results.extend(fts_results)
            # print(f"    âœ… FTS ê²€ìƒ‰ ì™„ë£Œ: {len(fts_results)}ê°œ")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            # 4. Trigram ìœ ì‚¬ë„ ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ 20%)
            print("    ğŸ”¤ Trigram ê²€ìƒ‰ ì‹¤í–‰...")
            trigram_results = await self._trigram_similarity_search(query, k)
            for result in trigram_results:
                result['final_score'] = result['search_score'] * 0.2
                result['search_type'] = 'trigram'
            all_results.extend(trigram_results)
            # print(f"    âœ… Trigram ê²€ìƒ‰ ì™„ë£Œ: {len(trigram_results)}ê°œ")  # ì„ì‹œ ë¹„í™œì„±í™”
            
            # ê²€ìƒ‰ ì „ëµ ê²°ì • (ê²°ê³¼ ì¢…ë¥˜ì— ë”°ë¼)
            if vector_results and len(vector_results) >= 2:
                search_strategy = "vector_strong"
                search_message = "AI ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            elif ilike_exact and len(ilike_exact) >= 2:
                search_strategy = "exact"
                search_message = "ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            elif fts_results and len(fts_results) >= 2:
                search_strategy = "fts_strong"
                search_message = "ì „ë¬¸ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            elif any([vector_results, ilike_exact, fts_results, trigram_results]):
                search_strategy = "partial"
                search_message = "ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
            
            # ê²°ê³¼ í†µí•© ë° ì •ë ¬
            if not all_results:
                print("    âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
            seen_ids = set()
            unique_results = []
            for result in all_results:
                result_id = result.get('id')
                if result_id and result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
                elif result_id in seen_ids:
                    # ì¤‘ë³µëœ ê²½ìš° ë” ë†’ì€ ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                    for i, existing in enumerate(unique_results):
                        if existing.get('id') == result_id and result['final_score'] > existing['final_score']:
                            unique_results[i] = result
                            break
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬ + ë‹¤ì–‘ì„±ì„ ìœ„í•œ ëœë¤ ìš”ì†Œ ì¶”ê°€
            unique_results.sort(key=lambda x: x['final_score'], reverse=True)
            
            # ğŸ¯ ì•„ì¹¨ ì‹ì‚¬ì—ë§Œ íŠ¹ë³„ ë¡œì§ ì ìš©: ê³„ë€ í¬í•¨/ì œì™¸ ë¶„ë¦¬ í›„ ëœë¤ ì„ íƒ
            # ì•„ì¹¨ í‚¤ì›Œë“œ ì²´í¬
            breakfast_keywords = ['ì•„ì¹¨', 'ë¸Œë ‰í¼ìŠ¤íŠ¸', 'ëª¨ë‹', 'breakfast', 'morning']
            is_breakfast_query = any(keyword in query.lower() for keyword in breakfast_keywords)
            
            if is_breakfast_query:
                print(f"    ğŸŒ… ì•„ì¹¨ ì‹ì‚¬ ê°ì§€ - íŠ¹ë³„ ë‹¤ì–‘ì„± ë¡œì§ ì ìš©")
                
                egg_recipes = []
                non_egg_recipes = []
                
                # ê³„ë€ ê´€ë ¨ í‚¤ì›Œë“œ (ë™ì˜ì–´ í¬í•¨)
                egg_keywords = ['ê³„ë€', 'egg', 'ë‹¬ê±€', 'ê³„ë€í”„ë¼ì´', 'ìŠ¤í¬ë¨ë¸”', 'ì˜¤ë¯ˆë ›', 'ì—ê·¸']
                
                for result in unique_results:
                    title = result.get('title', '').lower()
                    content = result.get('content', '').lower()
                    
                    # ê³„ë€ í¬í•¨ ì—¬ë¶€ ì²´í¬
                    is_egg = any(keyword in title or keyword in content for keyword in egg_keywords)
                    
                    if is_egg:
                        egg_recipes.append(result)
                    else:
                        non_egg_recipes.append(result)
                
                print(f"    ğŸ” ê³„ë€ í¬í•¨ ë ˆì‹œí”¼: {len(egg_recipes)}ê°œ")
                print(f"    ğŸ” ê³„ë€ ì œì™¸ ë ˆì‹œí”¼: {len(non_egg_recipes)}ê°œ")
                
                # ë‹¤ì–‘ì„± í™•ë³´: ê³„ë€ 1ê°œ + ë¹„ê³„ë€ 2ê°œ (ì´ 3ê°œ)
                import random
                selected_results = []
                
                # ê³„ë€ ë ˆì‹œí”¼ 1ê°œ ì„ íƒ (ìˆìœ¼ë©´)
                if egg_recipes:
                    selected_egg = random.choice(egg_recipes)
                    selected_results.append(selected_egg)
                    print(f"    âœ… ê³„ë€ ë ˆì‹œí”¼ ì„ íƒ: {selected_egg.get('title')}")
                
                # ë¹„ê³„ë€ ë ˆì‹œí”¼ 2ê°œ ì„ íƒ (ë¶€ì¡±í•˜ë©´ ê°€ëŠ¥í•œ ë§Œí¼)
                non_egg_count = min(2, len(non_egg_recipes))
                if non_egg_count > 0:
                    selected_non_egg = random.sample(non_egg_recipes, non_egg_count)
                    selected_results.extend(selected_non_egg)
                    print(f"    âœ… ë¹„ê³„ë€ ë ˆì‹œí”¼ ì„ íƒ: {[r.get('title') for r in selected_non_egg]}")
                
                # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë‚˜ë¨¸ì§€ ì¶”ê°€
                if len(selected_results) < 3 and len(unique_results) > len(selected_results):
                    remaining = [r for r in unique_results if r not in selected_results]
                    needed = 3 - len(selected_results)
                    selected_results.extend(remaining[:needed])
                    print(f"    âœ… ì¶”ê°€ ë ˆì‹œí”¼ ì„ íƒ: {[r.get('title') for r in remaining[:needed]]}")
                
                print(f"    âœ… ìµœì¢… ì„ íƒëœ ë ˆì‹œí”¼: {len(selected_results)}ê°œ")
                
                filtered_results = selected_results
            else:
                print(f"    ğŸ½ï¸ ì¼ë°˜ ì‹ì‚¬ - ê¸°ì¡´ ë‹¤ì–‘ì„± ë¡œì§ ì ìš©")
                
                # ê¸°ì¡´ ë‹¤ì–‘ì„± í•„í„°ë§ ë¡œì§ (ì•„ì¹¨ì´ ì•„ë‹Œ ê²½ìš°)
                filtered_results = []
                seen_ingredients = set()
                seen_categories = set()
                seen_proteins = set()
                
                for result in unique_results:
                    title = result.get('title', '').lower()
                    content = result.get('content', '').lower()
                    
                    # ë°°ì¶”ë¥˜ ì¤‘ë³µ ì²´í¬
                    cabbage_keywords = ['ì–‘ë°°ì¶”', 'ì•Œë°°ì¶”', 'ë°°ì¶”', 'cabbage']
                    is_cabbage = any(keyword in title or keyword in content for keyword in cabbage_keywords)
                    if is_cabbage and 'ë°°ì¶”ë¥˜' in seen_ingredients:
                        print(f"    âš ï¸ ë°°ì¶”ë¥˜ ì¤‘ë³µ ì œì™¸: {result.get('title')}")
                        continue
                    if is_cabbage:
                        seen_ingredients.add('ë°°ì¶”ë¥˜')
                    
                    # ê³„ë€ ì¤‘ë³µ ì²´í¬ (ì¼ë°˜ì ì¸ ê²½ìš°)
                    egg_keywords = ['ê³„ë€', 'egg', 'ë‹¬ê±€', 'ê³„ë€í”„ë¼ì´', 'ìŠ¤í¬ë¨ë¸”', 'ì˜¤ë¯ˆë ›', 'ì—ê·¸']
                    is_egg = any(keyword in title or keyword in content for keyword in egg_keywords)
                    if is_egg and 'ê³„ë€' in seen_ingredients:
                        print(f"    âš ï¸ ê³„ë€ ì¤‘ë³µ ì œì™¸: {result.get('title')}")
                        continue
                    if is_egg:
                        seen_ingredients.add('ê³„ë€')
                    
                    # ê¹€ë°¥ ì¤‘ë³µ ì²´í¬
                    if 'ê¹€ë°¥' in title or 'gimbap' in title:
                        if 'ê¹€ë°¥' in seen_categories:
                            print(f"    âš ï¸ ê¹€ë°¥ ì¤‘ë³µ ì œì™¸: {result.get('title')}")
                            continue
                        seen_categories.add('ê¹€ë°¥')
                    
                    # ë‹¨ë°±ì§ˆì› ì¤‘ë³µ ì²´í¬
                    protein_keywords = ['ë‹­ê³ ê¸°', 'ì†Œê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ì—°ì–´', 'ìƒˆìš°', 'ì°¸ì¹˜', 'ë² ì´ì»¨', 'ì¹˜ì¦ˆ']
                    for protein in protein_keywords:
                        if protein in title or protein in content:
                            if protein in seen_proteins:
                                print(f"    âš ï¸ ë‹¨ë°±ì§ˆì› ì¤‘ë³µ ì œì™¸: {result.get('title')} (ë‹¨ë°±ì§ˆì›: {protein})")
                                continue
                            seen_proteins.add(protein)
                            break
                    
                    filtered_results.append(result)
                    
                    # ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´ ìµœëŒ€ 3ê°œë¡œ ì œí•œ
                    if len(filtered_results) >= 3:
                        print(f"    âœ… ë‹¤ì–‘ì„± í™•ë³´: {len(filtered_results)}ê°œ ê²°ê³¼ë¡œ ì œí•œ")
                        break
            
            # ë‹¤ì–‘ì„± í™•ë³´: ìƒìœ„ ê²°ê³¼ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒ
            if len(filtered_results) > k:
                # ìƒìœ„ 70%ì—ì„œ ëœë¤ ì„ íƒ
                top_count = max(k, int(len(filtered_results) * 0.7))
                top_results = filtered_results[:top_count]
                final_results = random.sample(top_results, k)
                print(f"  ğŸ² ë‹¤ì–‘ì„± í™•ë³´: ìƒìœ„ {top_count}ê°œì—ì„œ {k}ê°œ ëœë¤ ì„ íƒ")
            else:
                final_results = filtered_results[:k]
            
            # URL ë³´ì™„: RPC í•¨ìˆ˜ê°€ urlì„ ë°˜í™˜í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì§ì ‘ ì¡°íšŒ
            for result in final_results:
                recipe_id = result.get('id')
                if not result.get('url') and recipe_id:
                    try:
                        recipe_info = self.supabase.table('recipe_blob_emb').select('url').eq('id', recipe_id).execute()
                        if recipe_info.data and len(recipe_info.data) > 0:
                            result['url'] = recipe_info.data[0].get('url')
                            if result.get('url'):
                                print(f"  ğŸ“ {result.get('title')} URL ë³´ì™„: {result['url']}")
                    except Exception as e:
                        print(f"  âš ï¸ URL ì¡°íšŒ ì‹¤íŒ¨ ({recipe_id}): {e}")
            
            # ê²€ìƒ‰ ì „ëµê³¼ ë©”ì‹œì§€ ì¶”ê°€
            for result in final_results:
                result['search_strategy'] = search_strategy
                result['search_message'] = search_message
            
            print(f"  âœ… ìµœì¢… ê²°ê³¼: {len(final_results)}ê°œ (ì „ëµ: {search_strategy})")
            print(f"  ğŸ’¬ {search_message}")
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            for i, result in enumerate(final_results[:3], 1):
                print(f"    {i}. {result['title']} (ì ìˆ˜: {result['final_score']:.3f}, íƒ€ì…: {result['search_type']})")
            
            # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥ (Redis ìš°ì„ , ë©”ëª¨ë¦¬ í´ë°±)
            # Redisì— ì €ì¥ (TTL: 1ì‹œê°„)
            redis_cache.set(cache_key, final_results, ttl=3600)
            
            # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥ (í´ë°±ìš©)
            self._search_results_cache[cache_key] = final_results
            self._manage_cache_size(self._search_results_cache)
            print(f"    ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥ (Redis + ë©”ëª¨ë¦¬): {query[:30]}...")
            
            return final_results
            
        except Exception as e:
            print(f"âŒ í•œê¸€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def search(self, query: str, profile: str = "", max_results: int = 5) -> List[Dict]:
        """ê°„ë‹¨í•œ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ (í•œê¸€ ìµœì í™” + ìŠ¤ë§ˆíŠ¸ ê°œì„ )"""
        try:
            # í”„ë¡œí•„ì—ì„œ í•„í„° ì¶”ì¶œ
            filters = {}
            if profile:
                if "ì•„ì¹¨" in profile or "morning" in profile.lower():
                    filters['category'] = 'ì•„ì¹¨'
                if "ì‰¬ìš´" in profile or "easy" in profile.lower():
                    filters['difficulty'] = 'ì‰¬ì›€'
            
            # ë©”ì‹œì§€ì—ì„œ ì‹ì‚¬-ì‹œê°„ í‚¤ì›Œë“œ ê°ì§€ â†’ ë³´ì¡° í‚¤ì›Œë“œë¡œ ê°•í™”
            adjusted_query = query
            meal_hint = None
            if any(k in query for k in ["ì•„ì¹¨", "ë¸Œë ‰í¼ìŠ¤íŠ¸", "ì•„ì¹¨ì‹ì‚¬", "morning", "breakfast"]):
                meal_hint = 'ì•„ì¹¨'
                # ì•„ì¹¨ í‚¤ì›Œë“œë³„ë¡œ ëœë¤í•˜ê²Œ í•˜ë‚˜ì”© ì„ íƒ
                import random
                breakfast_keywords = ["ì˜¤ë¯ˆë ›", "ìƒëŸ¬ë“œ", "ìš”ê±°íŠ¸", "ë² ì´ì»¨", "ì•„ë³´ì¹´ë„", "ì—°ì–´", "ë‹­ê°€ìŠ´ì‚´", "ì†Œê³ ê¸°"]
                selected_keywords = random.sample(breakfast_keywords, min(3, len(breakfast_keywords)))
                adjusted_query = f"{query} {' '.join(selected_keywords)}"
                print(f"  ğŸ² ì•„ì¹¨ í‚¤ì›Œë“œ ëœë¤ ì„ íƒ: {selected_keywords}")
            elif any(k in query for k in ["ì ì‹¬", "ëŸ°ì¹˜", "lunch"]):
                meal_hint = 'ì ì‹¬'
                adjusted_query = f"{query} ìƒëŸ¬ë“œ ìŠ¤í…Œì´í¬ ë³¶ìŒ êµ¬ì´"
            elif any(k in query for k in ["ì €ë…", "ë””ë„ˆ", "dinner"]):
                meal_hint = 'ì €ë…'
                adjusted_query = f"{query} ìŠ¤í…Œì´í¬ êµ¬ì´ ì°œ ë³¶ìŒ"

            # ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰(ê°•í™” ì¿¼ë¦¬ ìš°ì„ )
            print(f"  ğŸ” í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰: '{adjusted_query}'")
            results = await self.korean_hybrid_search(adjusted_query, max_results)
            if not results and adjusted_query != query:
                print(f"  ğŸ” ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰: '{query}'")
                results = await self.korean_hybrid_search(query, max_results)
            
            # ê²°ê³¼ í¬ë§·íŒ… (ê²€ìƒ‰ ì „ëµê³¼ ë©”ì‹œì§€ í¬í•¨)
            formatted_results = []
            search_message = ""
            search_strategy = "unknown"
            
            for result in results:
                # ì²« ë²ˆì§¸ ê²°ê³¼ì—ì„œ ê²€ìƒ‰ ì „ëµê³¼ ë©”ì‹œì§€ ì¶”ì¶œ
                if not search_message:
                    search_message = result.get('search_message', '')
                    search_strategy = result.get('search_strategy', 'unknown')
                    if meal_hint and not search_message:
                        search_message = f"'{meal_hint}' í‚¤ì›Œë“œë¥¼ ë°˜ì˜í•´ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤."
                
                # blob ë°ì´í„° ë””ë²„ê¹…
                blob_data = result.get('blob', '')
                print(f"    ğŸ” ê²€ìƒ‰ ê²°ê³¼ blob í™•ì¸: {result.get('title', 'ì œëª©ì—†ìŒ')}")
                print(f"    ğŸ” blob ì¡´ì¬: {bool(blob_data)}")
                print(f"    ğŸ” blob ê¸¸ì´: {len(str(blob_data))}")
                if blob_data:
                    print(f"    ğŸ” blob ë‚´ìš©: {str(blob_data)[:100]}...")
                
                formatted_results.append({
                    'id': result.get('id', ''),
                    'title': result.get('title', 'ì œëª© ì—†ìŒ'),
                    'content': result.get('content', ''),
                    'blob': result.get('blob', ''),  # blob ë°ì´í„° ì¶”ê°€
                    'allergens': result.get('allergens', []),
                    'ingredients': result.get('ingredients', []),
                    'similarity': result.get('final_score', 0.0),
                    'metadata': result.get('metadata', {}),
                    'search_types': [result.get('search_type', 'hybrid')],
                    'search_strategy': search_strategy
                })
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ì¶”ê°€
            if not formatted_results:
                formatted_results.append({
                    'title': 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ',
                    'content': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.',
                    'similarity': 0.0,
                    'metadata': {'search_message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'},
                    'search_types': ['none'],
                    'search_strategy': 'none'
                })
            
            # ê²€ìƒ‰ ë©”ì‹œì§€ ì¶œë ¥
            if search_message:
                print(f"ğŸ’¬ ì‚¬ìš©ì ì•ˆë‚´: {search_message}")
            
            return formatted_results
            
        except Exception as e:
            print(f"Search error: {e}")
            return [{
                'title': 'ê²€ìƒ‰ ì˜¤ë¥˜',
                'content': f'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'similarity': 0.0,
                'metadata': {'error': str(e)},
                'search_types': ['error'],
                'search_strategy': 'error'
            }]

    async def smart_search(self, query: str, k: int = 5) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ (ì‚¬ìš©ì í”¼ë“œë°± í¬í•¨)"""
        try:
            print(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
            print("  ğŸ¯ 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ê²€ìƒ‰...")
            fts_results = await self._full_text_search(query, k)
            
            if fts_results and any(result['search_score'] > 0.1 for result in fts_results):
                print(f"    âœ… ì •í™•í•œ ë§¤ì¹­ ë°œê²¬: {len(fts_results)}ê°œ")
                return {
                    'results': fts_results,
                    'search_strategy': 'exact',
                    'message': 'ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.',
                    'total_count': len(fts_results)
                }
            
            # 2ë‹¨ê³„: ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            print("  ğŸ” 2ë‹¨ê³„: ë¶€ë¶„ ë§¤ì¹­ ê²€ìƒ‰...")
            trigram_results = await self._trigram_similarity_search(query, k)
            ilike_results = await self._fallback_ilike_search(query, k)
            
            if trigram_results or ilike_results:
                print(f"    âœ… ë¶€ë¶„ ë§¤ì¹­ ë°œê²¬: Trigram {len(trigram_results)}ê°œ, ILIKE {len(ilike_results)}ê°œ")
                
                # ê²°ê³¼ í†µí•©
                all_partial_results = []
                all_partial_results.extend(trigram_results)
                all_partial_results.extend(ilike_results)
                
                # ì¤‘ë³µ ì œê±°
                seen_ids = set()
                unique_results = []
                for result in all_partial_results:
                    result_id = result.get('id')
                    if result_id and result_id not in seen_ids:
                        seen_ids.add(result_id)
                        unique_results.append(result)
                
                return {
                    'results': unique_results[:k],
                    'search_strategy': 'partial',
                    'message': 'ì •í™•í•œ ê²€ìƒ‰ì–´ê°€ ì—†ì–´ì„œ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.',
                    'total_count': len(unique_results)
                }
            
            # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            print("  ğŸ”„ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰...")
            hybrid_results = await self.korean_hybrid_search(query, k)
            
            if hybrid_results:
                return {
                    'results': hybrid_results,
                    'search_strategy': 'hybrid',
                    'message': 'ì¢…í•© ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'total_count': len(hybrid_results)
                }
            
            # 4ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
            print("    âŒ ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ì—ì„œ ê²°ê³¼ ì—†ìŒ")
            return {
                'results': [],
                'search_strategy': 'none',
                'message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.',
                'total_count': 0
            }
            
        except Exception as e:
            print(f"âŒ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'results': [],
                'search_strategy': 'error',
                'message': f'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'total_count': 0
            }

# ì „ì—­ í•œê¸€ ê²€ìƒ‰ ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤
korean_search_tool = KoreanSearchTool()
