#!/usr/bin/env python3
"""
ì¿¼ë¦¬ ì „ì²˜ë¦¬ ëª¨ë“ˆ
ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì¿¼ë¦¬ ì •ì œ ë° í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import re
from typing import List, Set
from synonym_dictionary import expand_query_with_synonyms, normalize_query

# ë¶ˆìš©ì–´ ì‚¬ì „ (ë³´ìˆ˜ì ìœ¼ë¡œ ì¶•ì†Œ)
STOP_WORDS = {
    # ê¸°ë³¸ ì¡°ì‚¬ë§Œ (í•µì‹¬ í‚¤ì›Œë“œ ë³´í˜¸)
    "ê°€", "ì´", "ì„", "ë¥¼", "ì˜", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "ë„", "ë§Œ",
    
    # ê¸°ë³¸ ì–´ë¯¸ë§Œ
    "ëŠ”", "ì€", "í•œ", "ì¸", "ëœ",
    
    # ëª…í™•í•œ ë¶ˆìš©ì–´ë§Œ
    "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë˜ëŠ”", "ê·¸ëŸ°ë°", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë”°ë¼ì„œ", "ê·¸ë˜ì„œ",
    "ì´ê²ƒ", "ì €ê²ƒ", "ê·¸ê²ƒ", "ì´ëŸ°", "ì €ëŸ°", "ê·¸ëŸ°", "ì´ë ‡ê²Œ", "ì €ë ‡ê²Œ", "ê·¸ë ‡ê²Œ",
    "ì •ë§", "ì§„ì§œ", "ì •ë§ë¡œ", "ì§„ì§œë¡œ", "ì•„ì£¼", "ë§¤ìš°", "ë„ˆë¬´", "êµ‰ì¥íˆ"
}

# í•µì‹¬ í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ (ë†’ì€ ì ìˆ˜)
PRIORITY_KEYWORDS = {
    # ì‹ì¬ë£Œ (ìµœê³  ìš°ì„ ìˆœìœ„)
    "ì•„ë³´ì¹´ë„", "ì˜¬ë¦¬ë¸Œìœ ", "ì˜¬ë¦¬ë¸Œì˜¤ì¼", "ë§ˆëŠ˜", "ì–‘íŒŒ", "ê³„ë€", "ë‹¬ê±€", "í† ë§ˆí† ", 
    "ì¹˜ì¦ˆ", "ë² ì´ì»¨", "ë²„í„°", "ìš°ìœ ", "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì†Œê³ ê¸°", "ìƒì„ ",
    
    # ì¡°ë¦¬ë²• (ë†’ì€ ìš°ì„ ìˆœìœ„)
    "ë³¶ìŒ", "êµ¬ìš´", "êµ¬ì´", "ì°œ", "íŠ€ê¹€", "ìƒëŸ¬ë“œ", "ìŠ¤í”„", "ìŠ¤í…Œì´í¬", "ìŠ¤í¬ë¨ë¸”",
    "íŒŒìŠ¤íƒ€", "í”¼ì", "ìƒŒë“œìœ„ì¹˜", "ìŠ¤ë¬´ë””", "ì¼€ì´í¬", "ì¿ í‚¤", "ë¹µ", "ê¹€ë°¥",
    
    # ë§›/íŠ¹ì„± (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)
    "ë§¤ìš´", "ë‹¬ì½¤í•œ", "ë‹¨ë§›", "ì§ ", "ì‹ ", "ê³ ì†Œí•œ", "ì§„í•œ", "ë¶€ë“œëŸ¬ìš´", "ë°”ì‚­í•œ",
    
    # ê±´ê°•/ë‹¤ì´ì–´íŠ¸ (ì¤‘ê°„ ìš°ì„ ìˆœìœ„)
    "keto", "ì¼€í† ", "í‚¤í† ", "ì €íƒ„ìˆ˜í™”ë¬¼", "ì €íƒ„", "ê³ ë‹¨ë°±", "ë‹¤ì´ì–´íŠ¸", "ê±´ê°•", "ë¬´ì„¤íƒ•"
}

def remove_stop_words(query: str) -> str:
    """
    ë¶ˆìš©ì–´ ì œê±°
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        
    Returns:
        ë¶ˆìš©ì–´ê°€ ì œê±°ëœ ì¿¼ë¦¬
    """
    words = query.split()
    filtered_words = [word for word in words if word not in STOP_WORDS]
    return " ".join(filtered_words)

def remove_particles_and_endings(query: str) -> str:
    """
    í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ ì œê±° (ë³´ìˆ˜ì  ì ‘ê·¼)
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        
    Returns:
        ì¡°ì‚¬/ì–´ë¯¸ê°€ ì œê±°ëœ ì¿¼ë¦¬
    """
    # ë³´ìˆ˜ì ì¸ ì¡°ì‚¬/ì–´ë¯¸ ì œê±° íŒ¨í„´ (í•µì‹¬ í‚¤ì›Œë“œ ë³´í˜¸)
    particle_patterns = [
        r'\b[ê°€ì´ì„ë¥¼ì˜ì—ì—ì„œë¡œìœ¼ë¡œì™€ê³¼ë„ë§Œ]\b',  # ë‹¨ì–´ ê²½ê³„ê°€ ìˆëŠ” ì¡°ì‚¬ë§Œ
        r'\b[ëŠ”ì€í•œì¸ëœ]\b',  # ë‹¨ì–´ ê²½ê³„ê°€ ìˆëŠ” ì–´ë¯¸ë§Œ
    ]
    
    cleaned_query = query
    for pattern in particle_patterns:
        cleaned_query = re.sub(pattern, '', cleaned_query)
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()
    
    return cleaned_query

def extract_core_keywords(query: str) -> List[str]:
    """
    í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        
    Returns:
        í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    # 1. ë¶ˆìš©ì–´ ì œê±°
    cleaned_query = remove_stop_words(query)
    
    # 2. ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
    cleaned_query = remove_particles_and_endings(cleaned_query)
    
    # 3. ë‹¨ì–´ ë¶„ë¦¬
    words = cleaned_query.split()
    
    # 4. ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ í•„í„°ë§
    core_keywords = []
    for word in words:
        if word in PRIORITY_KEYWORDS:
            core_keywords.append(word)
        elif len(word) >= 2:  # 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ í¬í•¨
            core_keywords.append(word)
    
    return core_keywords

def preprocess_query(query: str, use_synonyms: bool = True) -> str:
    """
    ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        use_synonyms: ë™ì˜ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬
    """
    # 1. ê¸°ë³¸ ì •ì œ
    cleaned_query = query.strip()
    
    # 2. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    core_keywords = extract_core_keywords(cleaned_query)
    
    if not core_keywords:
        # í•µì‹¬ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ë°˜í™˜
        return query
    
    # 3. ë™ì˜ì–´ í™•ì¥ (ì„ íƒì )
    if use_synonyms:
        expanded_keywords = []
        for keyword in core_keywords:
            expanded_keywords.append(keyword)
            # ë™ì˜ì–´ í™•ì¥
            expanded_query = expand_query_with_synonyms(keyword)
            expanded_terms = expanded_query.split()
            expanded_keywords.extend(expanded_terms)
        
        # ì¤‘ë³µ ì œê±°
        unique_keywords = list(dict.fromkeys(expanded_keywords))
        return " ".join(unique_keywords)
    else:
        return " ".join(core_keywords)

def preprocess_for_vector_search(query: str) -> str:
    """
    ë²¡í„° ê²€ìƒ‰ìš© ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ìµœì†Œí•œì˜ ì „ì²˜ë¦¬)
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        
    Returns:
        ë²¡í„° ê²€ìƒ‰ìš© ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬
    """
    # ë²¡í„° ê²€ìƒ‰ì€ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ìµœëŒ€í•œ ë³´ì¡´
    # OpenAI ì„ë² ë”© ëª¨ë¸ì´ ìì—°ì–´ë¥¼ ì˜ ì²˜ë¦¬í•˜ë¯€ë¡œ ìµœì†Œí•œì˜ ì „ì²˜ë¦¬ë§Œ
    
    # 1. ê¸°ë³¸ì ì¸ ë¶ˆìš©ì–´ë§Œ ì œê±°
    cleaned_query = remove_stop_words(query)
    
    # 2. ì¡°ì‚¬/ì–´ë¯¸ëŠ” ì œê±°í•˜ì§€ ì•ŠìŒ (ì˜ë¯¸ ë³´ì¡´)
    
    # 3. ë¹ˆ ì¿¼ë¦¬ì¸ ê²½ìš° ì›ë³¸ ë°˜í™˜
    if not cleaned_query.strip():
        return query
    
    return cleaned_query

def preprocess_for_keyword_search(query: str) -> str:
    """
    í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ë™ì˜ì–´ í™•ì¥ í¬í•¨)
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        
    Returns:
        í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬
    """
    return preprocess_query(query, use_synonyms=True)

def test_query_preprocessing():
    """ì¿¼ë¦¬ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    test_queries = [
        "ì•„ë³´ì¹´ë„ê°€ ë“¤ì–´ê°„ ê±´ê°•í•œ ìŒì‹",
        "ì˜¬ë¦¬ë¸Œìœ ë¡œ ë§Œë“œëŠ” ì´íƒˆë¦¬ì•ˆ ìš”ë¦¬", 
        "ë§ˆëŠ˜ í–¥ì´ ì§„í•œ í•œêµ­ ìš”ë¦¬",
        "ê³„ë€ì„ ì£¼ì¬ë£Œë¡œ í•˜ëŠ” ì•„ì¹¨ ì‹ì‚¬",
        "ì–‘íŒŒ ì—†ì´ ë§Œë“œëŠ” ìš”ë¦¬",
        "keto ë‹¤ì´ì–´íŠ¸ ìš”ë¦¬",
        "ì €íƒ„ìˆ˜í™”ë¬¼ ê±´ê°•ì‹",
        "ë§¤ìš´ ìŒì‹ ë§Œë“¤ê¸°",
        "ë‹¬ì½¤í•œ ë””ì €íŠ¸ ë ˆì‹œí”¼"
    ]
    
    print("ğŸ§ª ì¿¼ë¦¬ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    for query in test_queries:
        print(f"\nì›ë³¸ ì¿¼ë¦¬: '{query}'")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì „ì²˜ë¦¬
        keyword_processed = preprocess_for_keyword_search(query)
        print(f"í‚¤ì›Œë“œìš©:   '{keyword_processed}'")
        
        # ë²¡í„° ê²€ìƒ‰ìš© ì „ì²˜ë¦¬
        vector_processed = preprocess_for_vector_search(query)
        print(f"ë²¡í„°ìš©:     '{vector_processed}'")
        
        # í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        core_keywords = extract_core_keywords(query)
        print(f"í•µì‹¬ í‚¤ì›Œë“œ: {core_keywords}")

if __name__ == "__main__":
    test_query_preprocessing()
