"""
ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‚¬ìš©í•œ ë©”ë‰´ ì„ë² ë”© ìƒì„± ìŠ¤í¬ë¦½íŠ¸
- ìŒì‹ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ ì‚¬ì „ ì‚¬ìš©
- keto_scores.score >= 5ì¸ ë©”ë‰´ë§Œ ëŒ€ìƒ
- content_blob: {"name": "ë©”ë‰´ëª…", "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]}
- ì„ë² ë”©: "ë©”ë‰´ëª… í‚¤ì›Œë“œ1 í‚¤ì›Œë“œ2"
"""

import os
import sys
import asyncio
import hashlib
import json
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

import argparse
from infrastructure.database.connection import db_pool

# OpenAI ì„ë² ë”©
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None


@dataclass
class EmbeddingConfig:
    model_name: str = "text-embedding-3-small"
    algorithm_version: str = "RAG-v1.0"
    dimension: int = 1536
    batch_size: int = 100
    limit: int = 1000
    offset: int = 0


# ëŒ€í­ í™•ì¥ëœ ìŒì‹ ë„ë©”ì¸ í‚¤ì›Œë“œ ì‚¬ì „
KETO_FRIENDLY_KEYWORDS = {
    # ìƒì„ /í•´ì‚°ë¬¼ (í™•ì¥)
    'íšŒ': {'weight': 25, 'confidence': 0.9},
    'ìƒì„ íšŒ': {'weight': 30, 'confidence': 0.95},
    'ì—°ì–´': {'weight': 25, 'confidence': 0.9},
    'ì°¸ì¹˜': {'weight': 22, 'confidence': 0.85},
    'êµ´ë¹„': {'weight': 20, 'confidence': 0.8},
    'ê³ ë“±ì–´': {'weight': 20, 'confidence': 0.8},
    'ì‚¼ì¹˜': {'weight': 20, 'confidence': 0.8},
    'ê°ˆì¹˜': {'weight': 20, 'confidence': 0.8},
    'ì¡°ê¸°': {'weight': 18, 'confidence': 0.8},
    'ë¶ì–´': {'weight': 18, 'confidence': 0.8},
    'ìƒì„ ': {'weight': 18, 'confidence': 0.75},
    'ìƒˆìš°': {'weight': 18, 'confidence': 0.8},
    # 'ê²Œ': {'weight': 18, 'confidence': 0.8},  # ì œê±° (ì¹´ë¼ì•„ê²Œì—ì„œ ì˜ëª» ë§¤ì¹­)
    'ë‚™ì§€': {'weight': 15, 'confidence': 0.75},
    'ì˜¤ì§•ì–´': {'weight': 15, 'confidence': 0.75},
    'ì „ë³µ': {'weight': 20, 'confidence': 0.8},
    'ì¡°ê°œ': {'weight': 15, 'confidence': 0.75},
    # 'êµ´': {'weight': 15, 'confidence': 0.8},  # ì œê±° (êµ´ë¹„ì™€ ì¤‘ë³µ)
    'í™í•©': {'weight': 15, 'confidence': 0.75},
    'í•´ë¬¼': {'weight': 15, 'confidence': 0.7},
    'ê¼¼ì¥ì–´': {'weight': 18, 'confidence': 0.8},
    
    # ìœ¡ë¥˜ (ëŒ€í­ í™•ì¥)
    'ë‹­': {'weight': 18, 'confidence': 0.8},
    'ì¹˜í‚¨': {'weight': 18, 'confidence': 0.8},
    'ìŠ¤í…Œì´í¬': {'weight': 25, 'confidence': 0.9},
    'ë² ì´ì»¨': {'weight': 20, 'confidence': 0.85},
    'ì‚¼ê²¹ì‚´': {'weight': 22, 'confidence': 0.85},
    'ê°ˆë¹„': {'weight': 20, 'confidence': 0.8},
    'ì†Œê³ ê¸°': {'weight': 22, 'confidence': 0.85},
    'ë¼ì§€ê³ ê¸°': {'weight': 20, 'confidence': 0.8},
    'ë“±ì‹¬': {'weight': 22, 'confidence': 0.85},
    'ì•ˆì‹¬': {'weight': 22, 'confidence': 0.85},
    'ì±„ë': {'weight': 20, 'confidence': 0.8},
    'ëª©ì‚´': {'weight': 20, 'confidence': 0.8},
    'í•­ì •ì‚´': {'weight': 22, 'confidence': 0.8},
    'ì†Œì‹œì§€': {'weight': 18, 'confidence': 0.75},
    'í–„': {'weight': 15, 'confidence': 0.7},
    'ì œìœ¡': {'weight': 18, 'confidence': 0.8},
    'ìœ¡íšŒ': {'weight': 25, 'confidence': 0.9},
    'êµ¬ì´': {'weight': 15, 'confidence': 0.7},
    'ì°œ': {'weight': 12, 'confidence': 0.65},
    'ë³¶ìŒ': {'weight': 10, 'confidence': 0.6},
    
    # ê³„ë€/ìœ ì œí’ˆ
    'ê³„ë€': {'weight': 18, 'confidence': 0.8},
    'ë‹¬ê±€': {'weight': 18, 'confidence': 0.8},
    'ì¹˜ì¦ˆ': {'weight': 15, 'confidence': 0.8},
    'ëª¨ì§œë ë¼': {'weight': 15, 'confidence': 0.8},
    'ë²„í„°': {'weight': 20, 'confidence': 0.8},
    
    # ì•¼ì±„/ê¸°íƒ€ (í™•ì¥)
    'ìƒëŸ¬ë“œ': {'weight': 20, 'confidence': 0.85},
    'ì–‘ë°°ì¶”': {'weight': 15, 'confidence': 0.8},
    'ë¸Œë¡œì½œë¦¬': {'weight': 15, 'confidence': 0.8},
    'ì‹œê¸ˆì¹˜': {'weight': 12, 'confidence': 0.75},
    'ë²„ì„¯': {'weight': 12, 'confidence': 0.75},
    'í‘œê³ ë²„ì„¯': {'weight': 12, 'confidence': 0.75},
    'ëŠíƒ€ë¦¬ë²„ì„¯': {'weight': 12, 'confidence': 0.75},
    'ì•„ë³´ì¹´ë„': {'weight': 25, 'confidence': 0.9},
    'ê²¬ê³¼ë¥˜': {'weight': 18, 'confidence': 0.8},
    'ì˜¬ë¦¬ë¸Œ': {'weight': 15, 'confidence': 0.8},
    'ê¹€ì¹˜': {'weight': 10, 'confidence': 0.7},
    'ë‚˜ë¬¼': {'weight': 8, 'confidence': 0.65},
    'ì½©ë‚˜ë¬¼': {'weight': 8, 'confidence': 0.65},
    
    # ë‘ë¶€ë¥˜ (ì¶”ê°€)
    'ë‘ë¶€': {'weight': 12, 'confidence': 0.75},
    'ìˆœë‘ë¶€': {'weight': 12, 'confidence': 0.75},
    'ì†ë‘ë¶€': {'weight': 12, 'confidence': 0.75},
    
    # í‚¤í†  íŠ¹í™”
    'í‚¤í† ': {'weight': 30, 'confidence': 0.95},
    'ì €íƒ„ìˆ˜': {'weight': 25, 'confidence': 0.9},
    'ë¬´íƒ„ìˆ˜': {'weight': 30, 'confidence': 0.95},
}

HIGH_CARB_KEYWORDS = {
    # ì£¼ì‹ë¥˜
    'ë°¥': {'weight': -60, 'confidence': 0.9},
    'ìŒ€': {'weight': -55, 'confidence': 0.85},
    'í˜„ë¯¸': {'weight': -50, 'confidence': 0.8},
    'ì¡ê³¡': {'weight': -45, 'confidence': 0.8},
    
    # ë©´ë¥˜
    'ë©´': {'weight': -55, 'confidence': 0.85},
    'êµ­ìˆ˜': {'weight': -60, 'confidence': 0.9},
    'ë¼ë©´': {'weight': -65, 'confidence': 0.9},
    'ëƒ‰ë©´': {'weight': -60, 'confidence': 0.9},
    'ë¹„ë¹”ëƒ‰ë©´': {'weight': -60, 'confidence': 0.9},
    'íŒŒìŠ¤íƒ€': {'weight': -60, 'confidence': 0.9},
    'ìš°ë™': {'weight': -60, 'confidence': 0.9},
    'ì†Œë°”': {'weight': -55, 'confidence': 0.85},
    
    # ë¹µ/ë–¡ë¥˜
    'ë¹µ': {'weight': -55, 'confidence': 0.85},
    'ìƒŒë“œìœ„ì¹˜': {'weight': -55, 'confidence': 0.85},
    'ë–¡': {'weight': -65, 'confidence': 0.9},
    'í† ìŠ¤íŠ¸': {'weight': -50, 'confidence': 0.8},
    'ë² ì´ê¸€': {'weight': -55, 'confidence': 0.85},
    
    # ì´ˆë°¥/ê¹€ë°¥
    'ë‹ˆê¸°ë¦¬': {'weight': -70, 'confidence': 0.9},
    'ì´ˆë°¥': {'weight': -65, 'confidence': 0.9},
    'ê¹€ë°¥': {'weight': -60, 'confidence': 0.9},
    
    # ê¸°íƒ€ ê³ íƒ„ìˆ˜í™”ë¬¼
    'í”¼ì': {'weight': -65, 'confidence': 0.9},
    'ê°ì': {'weight': -45, 'confidence': 0.8},
    'ê³ êµ¬ë§ˆ': {'weight': -40, 'confidence': 0.8},
}


def extract_keywords_improved(text: str) -> List[Dict[str, Any]]:
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ - ê¸´ í‚¤ì›Œë“œ ìš°ì„ , ìœ„ì¹˜ ê²¹ì¹¨ ë°©ì§€"""
    text = text.lower().strip()
    found_keywords = []
    
    # ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ í•©ì¹˜ê³  ê¸¸ì´ìˆœ ì •ë ¬
    all_keywords = {}
    for keyword, info in KETO_FRIENDLY_KEYWORDS.items():
        all_keywords[keyword] = {'type': 'keto_friendly', 'weight': info['weight'], 'confidence': info['confidence']}
    for keyword, info in HIGH_CARB_KEYWORDS.items():
        all_keywords[keyword] = {'type': 'high_carb', 'weight': info['weight'], 'confidence': info['confidence']}
    
    # í‚¤ì›Œë“œë¥¼ ê¸¸ì´ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ ê²ƒë¶€í„°)
    sorted_keywords = sorted(all_keywords.keys(), key=len, reverse=True)
    
    used_positions = set()  # ì´ë¯¸ ë§¤ì¹­ëœ ìœ„ì¹˜ë“¤
    
    for keyword in sorted_keywords:
        start_pos = 0
        while True:
            pos = text.find(keyword, start_pos)
            if pos == -1:
                break
            
            # ì´ë¯¸ ì‚¬ìš©ëœ ìœ„ì¹˜ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            keyword_positions = set(range(pos, pos + len(keyword)))
            if not keyword_positions.intersection(used_positions):
                info = all_keywords[keyword]
                found_keywords.append({
                    'keyword': keyword,
                    'type': info['type'],
                    'weight': info['weight'],
                    'confidence': info['confidence']
                })
                used_positions.update(keyword_positions)
                break  # ê°™ì€ í‚¤ì›Œë“œëŠ” í•œ ë²ˆë§Œ
            
            start_pos = pos + 1
    
    return found_keywords


def sha256_hex(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def create_content_blob(name: str, keywords: List[str]) -> str:
    """ë©”ë‰´ëª… + í‚¤ì›Œë“œë¡œ content_blob JSON ìƒì„± (êµ¬ì¡°í™”ëœ ì •ë³´ + ì„ë² ë”© í…ìŠ¤íŠ¸)"""
    unique_keywords = []
    seen = set()
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            unique_keywords.append(k)
    
    # ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±
    if unique_keywords:
        embedding_text = f"{name} {' '.join(unique_keywords)}"
    else:
        embedding_text = name
    
    blob_obj = {
        "name": name,
        "keywords": unique_keywords,
        "embedding_text": embedding_text
    }
    
    return json.dumps(blob_obj, ensure_ascii=False, separators=(',', ':'))


def create_embedding_text(name: str, keywords: List[str]) -> str:
    """ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±"""
    unique_keywords = []
    seen = set()
    for k in keywords:
        if k and k not in seen:
            seen.add(k)
            unique_keywords.append(k)
    
    if not unique_keywords:
        return name.strip()
    
    return f"{name} {' '.join(unique_keywords)}".strip()


async def fetch_menus_with_keto_filter(supa, limit: int, offset: int, min_score: int = 5) -> List[Dict[str, Any]]:
    """keto_scores.score >= min_scoreì¸ ë©”ë‰´ë“¤ ì¡°íšŒ"""
    # 1) keto_scoresì—ì„œ score >= min_scoreì¸ menu_id ì¡°íšŒ
    keto_res = supa.client.table('keto_scores').select('menu_id,score').gte('score', min_score).range(offset, offset + limit - 1).execute()
    keto_data = keto_res.data or []
    
    if not keto_data:
        return []
    
    menu_ids = [row['menu_id'] for row in keto_data]
    
    # 2) menu í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ë©”ë‰´ë“¤ ì¡°íšŒ
    menu_res = supa.client.table('menu').select('id,name,description').in_('id', menu_ids).execute()
    menu_data = menu_res.data or []
    
    # 3) ì¡°ì¸
    menu_map = {m['id']: m for m in menu_data}
    result = []
    for keto in keto_data:
        menu_id = keto['menu_id']
        if menu_id in menu_map:
            menu = menu_map[menu_id]
            result.append({
                'id': menu_id,
                'name': menu.get('name', ''),
                'description': menu.get('description'),
                'keto_score': keto.get('score', 0)
            })
    
    return result


def ensure_openai_client() -> OpenAI:
    if OpenAI is None:
        raise RuntimeError("openai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install openai í•„ìš”")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    return OpenAI(api_key=api_key)


def embed_texts(client: OpenAI, texts: List[str], model_name: str) -> List[List[float]]:
    if not texts:
        return []
    
    print(f"ğŸ”„ OpenAI API í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {model_name}, í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(texts)})")
    import time
    start_time = time.time()
    
    resp = client.embeddings.create(model=model_name, input=texts)
    
    end_time = time.time()
    print(f"âœ… OpenAI API ì‘ë‹µ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
    print(f"ğŸ“Š ì‘ë‹µ ë°ì´í„°: {len(resp.data)}ê°œ ì„ë² ë”©, ì°¨ì›: {len(resp.data[0].embedding) if resp.data else 0}")
    
    return [d.embedding for d in resp.data]


async def upsert_embeddings(supa, rows: List[Dict[str, Any]]):
    if not rows:
        return
    supa.client.table('menu_embedding').upsert(rows, on_conflict="menu_id,model_name,algorithm_version").execute()


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-name", default="text-embedding-3-small")
    parser.add_argument("--algorithm-version", default="RAG-v1.0")
    parser.add_argument("--dimension", type=int, default=1536)
    parser.add_argument("--batch-size", type=int, default=100)
    parser.add_argument("--limit", type=int, default=1000)
    parser.add_argument("--offset", type=int, default=0)
    parser.add_argument("--min-keto-score", type=int, default=20)
    parser.add_argument("--recompute-all", action="store_true")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì„ë² ë”© ì—†ì´ í‚¤ì›Œë“œ ì¶”ì¶œë§Œ í…ŒìŠ¤íŠ¸")
    args = parser.parse_args()

    cfg = EmbeddingConfig(
        model_name=args.model_name,
        algorithm_version=args.algorithm_version,
        dimension=args.dimension,
        batch_size=args.batch_size,
        limit=args.limit,
        offset=args.offset,
    )

    # DB ì´ˆê¸°í™”
    await db_pool.initialize()
    supa = db_pool.supabase

    # OpenAI í´ë¼ì´ì–¸íŠ¸ (dry-runì´ ì•„ë‹ ë•Œë§Œ)
    openai_client = None
    if not args.dry_run:
        openai_client = ensure_openai_client()

    print(f"ğŸ” keto_scores.score >= {args.min_keto_score}ì¸ ë©”ë‰´ ì¡°íšŒ ì¤‘...")
    
    # ë©”ë‰´ ì¡°íšŒ
    menus = await fetch_menus_with_keto_filter(supa, cfg.limit, cfg.offset, args.min_keto_score)
    
    if not menus:
        print("âŒ ì¡°ê±´ì— ë§ëŠ” ë©”ë‰´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        await db_pool.close()
        return

    print(f"âœ… {len(menus)}ê°œ ë©”ë‰´ ë°œê²¬")

    # ê¸°ì¡´ ë ˆì½”ë“œ í•´ì‹œ ì¡°íšŒ (ì¤‘ë³µ ë°©ì§€ìš©)
    menu_ids = [m['id'] for m in menus]
    existing_map: Dict[Tuple[str, str, str], str] = {}
    res_existing = supa.client.table('menu_embedding').select('menu_id,model_name,algorithm_version,content_hash').in_('menu_id', menu_ids).eq('model_name', cfg.model_name).eq('algorithm_version', cfg.algorithm_version).execute()
    for row in res_existing.data or []:
        existing_map[(row['menu_id'], row['model_name'], row['algorithm_version'])] = row.get('content_hash') or ""

    texts: List[str] = []
    meta: List[Tuple[str, str]] = []  # (menu_id, content_hash)
    blobs: List[str] = []

    print("\n" + "=" * 80)
    print("í‚¤ì›Œë“œ ì¶”ì¶œ ë° blob ìƒì„±")
    print("=" * 80)

    for i, menu in enumerate(menus, 1):
        menu_id = menu['id']
        name = menu['name']
        
        # ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword_matches = extract_keywords_improved(name)
        keywords = [match['keyword'] for match in keyword_matches]
        
        if args.dry_run or i <= 10:  # ì²˜ìŒ 10ê°œëŠ” ìƒì„¸ ì¶œë ¥
            print(f"\n[{i}] {name} (í‚¤í† ì ìˆ˜: {menu['keto_score']})")
            if keyword_matches:
                for match in keyword_matches:
                    print(f"  âœ“ {match['keyword']} ({match['type']}, ê°€ì¤‘ì¹˜: {match['weight']})")
            else:
                print("  - í‚¤ì›Œë“œ ì—†ìŒ")

        # content_blob ìƒì„±
        content_blob = create_content_blob(name, keywords)
        content_hash = sha256_hex(content_blob)
        
        # ê¸°ì¡´ í•´ì‹œì™€ ë¹„êµ
        existed_hash = existing_map.get((menu_id, cfg.model_name, cfg.algorithm_version))
        if existed_hash and existed_hash == content_hash and not args.recompute_all:
            if args.dry_run or i <= 10:
                print(f"  â†’ ìŠ¤í‚µ (ê¸°ì¡´ê³¼ ë™ì¼)")
            continue

        # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸
        embedding_text = create_embedding_text(name, keywords)
        
        if args.dry_run or i <= 10:
            print(f"  â†’ blob: {content_blob}")
            print(f"  â†’ ì„ë² ë”© í…ìŠ¤íŠ¸: '{embedding_text}'")

        texts.append(embedding_text)
        meta.append((menu_id, content_hash))
        blobs.append(content_blob)

    if args.dry_run:
        print(f"\nâœ… Dry-run ì™„ë£Œ. ì´ {len(texts)}ê°œ ë©”ë‰´ê°€ ì„ë² ë”© ëŒ€ìƒì…ë‹ˆë‹¤.")
        await db_pool.close()
        return

    if not texts:
        print("âŒ ì²˜ë¦¬í•  ë©”ë‰´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        await db_pool.close()
        return

    # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
    print(f"\nğŸš€ {len(texts)}ê°œ ë©”ë‰´ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = embed_texts(openai_client, texts, cfg.model_name)
    
    # DBì— upsert
    rows = []
    for (menu_id, content_hash), emb, content_blob in zip(meta, embeddings, blobs):
        rows.append({
            'menu_id': menu_id,
            'model_name': cfg.model_name,
            'dimension': cfg.dimension,
            'algorithm_version': cfg.algorithm_version,
            'embedding': emb,
            'content_hash': content_hash,
            'content_blob': content_blob,
        })
    
    await upsert_embeddings(supa, rows)
    print(f"âœ… {len(rows)}ê°œ ì„ë² ë”© upsert ì™„ë£Œ")

    await db_pool.close()


if __name__ == "__main__":
    asyncio.run(main())
