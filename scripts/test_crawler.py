#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import settings
from config.crawler_config import register_crawlers
from services.crawler.crawler_factory import crawler_factory
from services.processor.data_processor import DataProcessor
from services.processor.geocoding_service import MockGeocodingService
from services.processor.deduplication_service import DeduplicationService
from services.cache.cache_manager import CacheManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_crawler_basic():
    """ê¸°ë³¸ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    logger.info("=== í¬ë¡¤ëŸ¬ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

    try:
        # í¬ë¡¤ëŸ¬ ìƒì„±
        crawler = crawler_factory.create('diningcode')

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        logger.info("ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        connection_ok = await crawler.test_connection()
        logger.info(f"ì—°ê²° í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if connection_ok else 'ì‹¤íŒ¨'}")

        # ê²€ìƒ‰ URL í…ŒìŠ¤íŠ¸
        search_url = crawler.get_search_url("ê°•ë‚¨ì—­ ë§›ì§‘", 1)
        logger.info(f"ê²€ìƒ‰ URL: {search_url}")

        # URL íŒ¨í„´ í…ŒìŠ¤íŠ¸
        test_urls = [
            "https://www.diningcode.com/profile.php?rid=12345",
            "https://www.diningcode.com/list?keyword=test",
            "https://www.diningcode.com/profile.php?rid=abc"
        ]

        for url in test_urls:
            is_restaurant = crawler.is_restaurant_url(url)
            logger.info(f"URL: {url} -> ì‹ë‹¹ URL: {is_restaurant}")

        return True

    except Exception as e:
        logger.error(f"í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

    finally:
        if 'crawler' in locals():
            await crawler.cleanup()

async def test_data_processing():
    """ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    logger.info("=== ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

    try:
        # ì„œë¹„ìŠ¤ ìƒì„±
        geocoding_service = MockGeocodingService()
        dedup_service = DeduplicationService()
        cache_manager = CacheManager()
        await cache_manager.initialize()

        processor = DataProcessor(
            geocoding_service=geocoding_service,
            deduplication_service=dedup_service
        )

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_restaurant_data = {
            'name': '  ê°•ë‚¨ í…ŒìŠ¤íŠ¸ ì‹ë‹¹  ',
            'address': 'ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123',
            'phone': '02-1234-5678',
            'rating': '4.5',
            'review_count': 100,
            'cuisine_types': ['í•œì‹', 'ì¼ì‹'],
            'price_range': 'medium',
            'source_url': 'https://test.com/restaurant/123',
            'source_name': 'diningcode'
        }

        test_menu_data = [
            {'name': 'ê¹€ì¹˜ì°Œê°œ', 'price': '8000ì›', 'description': 'ë§¤ì½¤í•œ ê¹€ì¹˜ì°Œê°œ'},
            {'name': 'ëœì¥ì°Œê°œ', 'price': 7000},
            {'name': '   ', 'price': '0'},  # ì˜ëª»ëœ ë°ì´í„°
        ]

        # ì‹ë‹¹ ë°ì´í„° ì²˜ë¦¬
        restaurant = await processor.process_restaurant_data(test_restaurant_data)
        logger.info(f"ì²˜ë¦¬ëœ ì‹ë‹¹: {restaurant.name}")
        logger.info(f"ì£¼ì†Œ: {restaurant.address.display_address if restaurant.address else 'None'}")
        logger.info(f"ìœ„ì¹˜: {restaurant.has_location}")

        # ë©”ë‰´ ë°ì´í„° ì²˜ë¦¬
        menus = []
        for menu_data in test_menu_data:
            menu = await processor.process_menu_data(menu_data, restaurant.id)
            if menu:
                menus.append(menu)

        logger.info(f"ì²˜ë¦¬ëœ ë©”ë‰´ ìˆ˜: {len(menus)}")
        for menu in menus:
            logger.info(f"  - {menu.name}: {menu.price}ì›")

        return True

    except Exception as e:
        logger.error(f"ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

async def test_cache_system():
    """ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("=== ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

    try:
        cache_manager = CacheManager()
        await cache_manager.initialize()

        # í¬ë¡¤ë§ ë°ì´í„° ìºì‹œ í…ŒìŠ¤íŠ¸
        test_data = [
            {'restaurant': {'name': 'í…ŒìŠ¤íŠ¸ ì‹ë‹¹ 1'}, 'menus': []},
            {'restaurant': {'name': 'í…ŒìŠ¤íŠ¸ ì‹ë‹¹ 2'}, 'menus': []}
        ]

        await cache_manager.store_crawl_data('diningcode', test_data)
        cached_data = await cache_manager.get_recent_crawl_data('diningcode', 1)

        logger.info(f"ìºì‹œ ì €ì¥/ì¡°íšŒ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if cached_data else 'ì‹¤íŒ¨'}")

        # ì§€ì˜¤ì½”ë”© ìºì‹œ í…ŒìŠ¤íŠ¸
        test_address = "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123"
        test_geocode_result = {
            'lat': 37.4979,
            'lng': 127.0276,
            'formatted_address': 'ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123'
        }

        await cache_manager.store_geocoding_result(test_address, test_geocode_result)
        cached_geocode = await cache_manager.get_geocoding_result(test_address)

        logger.info(f"ì§€ì˜¤ì½”ë”© ìºì‹œ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if cached_geocode else 'ì‹¤íŒ¨'}")

        # ìºì‹œ í†µê³„
        stats = await cache_manager.get_cache_statistics()
        logger.info(f"ìºì‹œ í†µê³„: {stats}")

        return True

    except Exception as e:
        logger.error(f"ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

async def test_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger.info("=== ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

    try:
        # í¬ë¡¤ëŸ¬ ìƒì„±
        crawler = crawler_factory.create('diningcode')

        # ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤ ìƒì„±
        geocoding_service = MockGeocodingService()
        dedup_service = DeduplicationService()
        cache_manager = CacheManager()
        await cache_manager.initialize()

        processor = DataProcessor(
            geocoding_service=geocoding_service,
            deduplication_service=dedup_service
        )

        # ìºì‹œ í™•ì¸
        cached_data = await cache_manager.get_recent_crawl_data('diningcode', 24)
        if cached_data:
            logger.info("ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")
            crawl_results = cached_data
        else:
            logger.info("ìƒˆë¡œìš´ í¬ë¡¤ë§ í•„ìš” (ì‹œë®¬ë ˆì´ì…˜)")
            # ì‹¤ì œë¡œëŠ” crawler.crawl_restaurant_list() í˜¸ì¶œ
            crawl_results = [
                {
                    'restaurant': {
                        'name': 'ê°•ë‚¨ ë§›ì§‘',
                        'address': 'ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123',
                        'phone': '02-1234-5678',
                        'source_url': 'https://test.com/123',
                        'source_name': 'diningcode'
                    },
                    'menus': [
                        {'name': 'ê¹€ì¹˜ì°Œê°œ', 'price': 8000},
                        {'name': 'ëœì¥ì°Œê°œ', 'price': 7000}
                    ]
                }
            ]
            await cache_manager.store_crawl_data('diningcode', crawl_results)

        # ë°ì´í„° ì²˜ë¦¬
        processed_results = await processor.process_batch(crawl_results)

        logger.info(f"íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(processed_results)}ê°œ ì‹ë‹¹ ì²˜ë¦¬")

        for restaurant, menus in processed_results:
            logger.info(f"  - {restaurant.name}: {len(menus)}ê°œ ë©”ë‰´")

        return True

    except Exception as e:
        logger.error(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

    finally:
        if 'crawler' in locals():
            await crawler.cleanup()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ í¬ë¡¤ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # í¬ë¡¤ëŸ¬ ë“±ë¡ í™•ì¸
    register_crawlers()
    available_sources = crawler_factory.list_sources()
    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬: {available_sources}")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("í¬ë¡¤ëŸ¬ ê¸°ë³¸ ê¸°ëŠ¥", test_crawler_basic),
        ("ë°ì´í„° ì²˜ë¦¬", test_data_processing),
        ("ìºì‹œ ì‹œìŠ¤í…œ", test_cache_system),
        ("ì „ì²´ íŒŒì´í”„ë¼ì¸", test_full_pipeline)
    ]

    results = []
    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"í…ŒìŠ¤íŠ¸: {test_name}")
        logger.info(f"{'='*50}")

        try:
            result = await test_func()
            results.append((test_name, result))
            logger.info(f"âœ… {test_name}: {'ì„±ê³µ' if result else 'ì‹¤íŒ¨'}")
        except Exception as e:
            logger.error(f"âŒ {test_name}: ì˜ˆì™¸ ë°œìƒ - {e}")
            results.append((test_name, False))

    # ê²°ê³¼ ìš”ì•½
    logger.info(f"\n{'='*50}")
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info(f"{'='*50}")

    success_count = sum(1 for _, result in results if result)
    total_count = len(results)

    for test_name, result in results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"{test_name}: {status}")

    logger.info(f"\nì „ì²´ ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")

    return success_count == total_count

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)