#!/usr/bin/env python3
"""
ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.settings import settings
from config.crawler_config import register_crawlers
from services.crawler.crawler_factory import crawler_factory
from services.processor.data_processor import DataProcessor
from services.processor.geocoding_service import MockGeocodingService
from services.processor.deduplication_service import DeduplicationService
from services.cache.cache_manager import CacheManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CrawlingManager:
    """í¬ë¡¤ë§ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.crawler = None
        self.processor = None
        self.cache_manager = None
        self.results = []
        
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        logger.info("í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # í¬ë¡¤ëŸ¬ ìƒì„±
        self.crawler = crawler_factory.create('diningcode')
        
        # ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤ ìƒì„±
        geocoding_service = MockGeocodingService()
        dedup_service = DeduplicationService()
        self.cache_manager = CacheManager()
        await self.cache_manager.initialize()
        
        self.processor = DataProcessor(
            geocoding_service=geocoding_service,
            deduplication_service=dedup_service
        )
        
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def crawl_keyword(self, keyword: str, max_pages: int = 3) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§"""
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        all_restaurants = []
        
        for page in range(1, max_pages + 1):
            logger.info(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
            
            try:
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‹ë‹¹ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                search_url = self.crawler.get_search_url(keyword, page)
                logger.info(f"ê²€ìƒ‰ URL: {search_url}")
                
                # ê²€ìƒ‰ í˜ì´ì§€ í¬ë¡¤ë§
                restaurant_urls = await self.crawler._crawl_search_page(search_url)
                
                logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {len(restaurant_urls)}ê°œ ì‹ë‹¹ URL ë°œê²¬")
                
                # ê° ì‹ë‹¹ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
                for i, url in enumerate(restaurant_urls):
                    try:
                        logger.info(f"ì‹ë‹¹ {i+1}/{len(restaurant_urls)} í¬ë¡¤ë§: {url}")
                        
                        # ì‹ë‹¹ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
                        crawl_result = await self.crawler.crawl_restaurant_detail(url)
                        
                        if crawl_result.success:
                            all_restaurants.append(crawl_result.data)
                            logger.info(f"ì‹ë‹¹ í¬ë¡¤ë§ ì„±ê³µ: {crawl_result.data.get('restaurant', {}).get('name', 'Unknown')}")
                        else:
                            logger.error(f"ì‹ë‹¹ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {crawl_result.error}")
                        
                        # Rate limiting
                        await asyncio.sleep(1.0)
                        
                    except Exception as e:
                        logger.error(f"ì‹ë‹¹ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
                        continue
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                await asyncio.sleep(2.0)
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_restaurants)}ê°œ ì‹ë‹¹")
        return all_restaurants
    
    async def process_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ì²˜ë¦¬"""
        logger.info("ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            processed_results = await self.processor.process_batch(raw_data)
            
            # ê²°ê³¼ ì •ë¦¬
            processed_data = []
            for restaurant, menus in processed_results:
                processed_data.append({
                    'restaurant': {
                        'id': restaurant.id,
                        'name': restaurant.name,
                        'address': restaurant.address.display_address if restaurant.address else None,
                        'phone': restaurant.phone,
                        'rating': restaurant.rating,
                        'review_count': restaurant.review_count,
                        'cuisine_types': restaurant.cuisine_types,
                        'price_range': restaurant.price_range,
                        'source_url': restaurant.source_url,
                        'source_name': restaurant.source_name,
                        'has_location': restaurant.has_location
                    },
                    'menus': [
                        {
                            'id': menu.id,
                            'name': menu.name,
                            'price': menu.price,
                            'description': menu.description,
                            'keto_score': menu.keto_score.score if menu.keto_score else None
                        }
                        for menu in menus
                    ]
                })
            
            logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data)}ê°œ ì‹ë‹¹")
            return processed_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    async def save_results(self, data: List[Dict[str, Any]], filename: str = None):
        """ê²°ê³¼ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_results_{timestamp}.json"
        
        file_path = settings.get_data_path("reports", filename)
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            
            # í†µê³„ ì¶œë ¥
            total_restaurants = len(data)
            total_menus = sum(len(item['menus']) for item in data)
            
            logger.info(f"í¬ë¡¤ë§ í†µê³„:")
            logger.info(f"  - ì‹ë‹¹ ìˆ˜: {total_restaurants}")
            logger.info(f"  - ë©”ë‰´ ìˆ˜: {total_menus}")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ì •ë¦¬"""
        if self.crawler:
            await self.crawler.cleanup()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘")
    
    # í¬ë¡¤ëŸ¬ ë“±ë¡
    register_crawlers()
    
    # í¬ë¡¤ë§ ê´€ë¦¬ì ìƒì„±
    manager = CrawlingManager()
    
    try:
        # ì´ˆê¸°í™”
        await manager.initialize()
        
        # í¬ë¡¤ë§í•  í‚¤ì›Œë“œë“¤
        keywords = [
            "ê°•ë‚¨ì—­ ë§›ì§‘",
            "í™ëŒ€ ë§›ì§‘", 
            "ì´íƒœì› ë§›ì§‘",
            "ê±´ëŒ€ ë§›ì§‘"
        ]
        
        all_results = []
        
        for keyword in keywords:
            logger.info(f"\n{'='*50}")
            logger.info(f"í‚¤ì›Œë“œ: {keyword}")
            logger.info(f"{'='*50}")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            raw_data = await manager.crawl_keyword(keyword, max_pages=2)
            
            if raw_data:
                # ë°ì´í„° ì²˜ë¦¬
                processed_data = await manager.process_data(raw_data)
                all_results.extend(processed_data)
                
                # ê°œë³„ í‚¤ì›Œë“œ ê²°ê³¼ ì €ì¥
                await manager.save_results(processed_data, f"crawl_{keyword.replace(' ', '_')}.json")
            else:
                logger.warning(f"í‚¤ì›Œë“œ '{keyword}'ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        if all_results:
            await manager.save_results(all_results, "crawl_all_results.json")
            logger.info(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ì‹ë‹¹ ìˆ˜ì§‘")
        else:
            logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return False
    
    finally:
        await manager.cleanup()
    
    return True

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
