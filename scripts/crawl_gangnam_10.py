#!/usr/bin/env python3
"""
ê°•ë‚¨ í‚¤ì›Œë“œë¡œ 10ê°œ ì‹ë‹¹ë§Œ í¬ë¡¤ë§
"""

import asyncio
import sys
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.crawler_config import register_crawlers
from services.crawler.crawler_factory import crawler_factory

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def crawl_gangnam_10_restaurants():
    """ê°•ë‚¨ í‚¤ì›Œë“œë¡œ 10ê°œ ì‹ë‹¹ í¬ë¡¤ë§"""
    logger.info("ğŸš€ ê°•ë‚¨ í‚¤ì›Œë“œ 10ê°œ ì‹ë‹¹ í¬ë¡¤ë§ ì‹œì‘")
    
    # í¬ë¡¤ëŸ¬ ë“±ë¡
    register_crawlers()
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = crawler_factory.create('diningcode')
    
    try:
        await crawler.initialize()
        
        keyword = "ê°•ë‚¨ì—­ ë§›ì§‘"
        target_count = 10
        results = []
        
        logger.info(f"í‚¤ì›Œë“œ: '{keyword}' - ëª©í‘œ: {target_count}ê°œ ì‹ë‹¹")
        
        # ê²€ìƒ‰ URL ìƒì„± (ì²« ë²ˆì§¸ í˜ì´ì§€ë§Œ)
        search_url = crawler.get_search_url(keyword, 1)
        
        # HTML ê°€ì ¸ì˜¤ê¸°
        response = await crawler._client.get(search_url)
        html_content = response.text
        
        # ì‹ë‹¹ URL ì¶”ì¶œ
        restaurant_urls = crawler._extract_restaurant_urls_from_search(html_content)
        logger.info(f"ì´ {len(restaurant_urls)}ê°œ ì‹ë‹¹ URL ë°œê²¬")
        
        # 10ê°œë§Œ ì„ íƒ
        selected_urls = restaurant_urls[:target_count]
        logger.info(f"ì„ íƒëœ {len(selected_urls)}ê°œ ì‹ë‹¹ í¬ë¡¤ë§ ì‹œì‘")
        
        # ê° ì‹ë‹¹ í¬ë¡¤ë§
        for i, url in enumerate(selected_urls, 1):
            try:
                logger.info(f"[{i}/{len(selected_urls)}] í¬ë¡¤ë§ ì¤‘...")
                
                crawl_result = await crawler.crawl_restaurant_detail(url)
                
                if crawl_result.success:
                    restaurant_data = crawl_result.data
                    restaurant_name = restaurant_data.get('restaurant', {}).get('name', 'Unknown')
                    menu_count = len(restaurant_data.get('menus', []))
                    
                    logger.info(f"âœ… [{i}] {restaurant_name}: {menu_count}ê°œ ë©”ë‰´")
                    results.append(restaurant_data)
                else:
                    logger.error(f"âŒ [{i}] í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_result.error}")
                
                # Rate limiting
                await asyncio.sleep(1.0)
                
            except Exception as e:
                logger.error(f"âŒ [{i}] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²°ê³¼ ì €ì¥
        if results:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"gangnam_10_restaurants_{timestamp}.json"
            file_path = Path(f"data/reports/{filename}")
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # í†µê³„ ì¶œë ¥
            total_menus = sum(len(result.get('menus', [])) for result in results)
            
            logger.info(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ê²°ê³¼ í†µê³„:")
            logger.info(f"  - ìˆ˜ì§‘ëœ ì‹ë‹¹: {len(results)}ê°œ")
            logger.info(f"  - ìˆ˜ì§‘ëœ ë©”ë‰´: {total_menus}ê°œ")
            logger.info(f"  - ì €ì¥ íŒŒì¼: {file_path}")
            
            # ì‹ë‹¹ ëª©ë¡ ì¶œë ¥
            logger.info(f"\nğŸ“‹ ìˆ˜ì§‘ëœ ì‹ë‹¹ ëª©ë¡:")
            for i, result in enumerate(results, 1):
                restaurant_name = result.get('restaurant', {}).get('name', 'Unknown')
                menu_count = len(result.get('menus', []))
                logger.info(f"  {i}. {restaurant_name} ({menu_count}ê°œ ë©”ë‰´)")
            
        else:
            logger.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await crawler.cleanup()

if __name__ == "__main__":
    asyncio.run(crawl_gangnam_10_restaurants())
