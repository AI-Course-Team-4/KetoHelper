#!/usr/bin/env python3
"""
ìµœì¢… í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ - ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ëŒ€ëŸ‰ ìˆ˜ì§‘
"""

import asyncio
import sys
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from config.crawler_config import register_crawlers
from services.crawler.crawler_factory import crawler_factory

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def crawl_keyword_restaurants(crawler, keyword: str, max_pages: int = 2) -> List[Dict[str, Any]]:
    """í‚¤ì›Œë“œë³„ ì‹ë‹¹ í¬ë¡¤ë§ - ì¤‘ë³µ ì œê±° í¬í•¨"""
    logger.info(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
    
    all_results = []
    crawled_urls = set()  # ì´ë¯¸ í¬ë¡¤ë§í•œ URL ì¶”ì 
    
    for page in range(1, max_pages + 1):
        try:
            logger.info(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
            
            # ê²€ìƒ‰ URL ìƒì„±
            search_url = crawler.get_search_url(keyword, page)
            
            # HTML ê°€ì ¸ì˜¤ê¸°
            response = await crawler._client.get(search_url)
            html_content = response.text
            
            # ì‹ë‹¹ URL ì¶”ì¶œ
            restaurant_urls = crawler._extract_restaurant_urls_from_search(html_content)
            logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {len(restaurant_urls)}ê°œ ì‹ë‹¹ URL ë°œê²¬")
            
            if not restaurant_urls:
                logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ì‹ë‹¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                break
            
            # ìƒˆë¡œìš´ URLë§Œ í•„í„°ë§
            new_urls = [url for url in restaurant_urls if url not in crawled_urls]
            logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ìƒˆë¡œìš´ ì‹ë‹¹ {len(new_urls)}ê°œ (ì¤‘ë³µ ì œê±°ë¨: {len(restaurant_urls) - len(new_urls)}ê°œ)")
            
            if not new_urls:
                logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ìƒˆë¡œìš´ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¤‘ë‹¨")
                break
            
            # ìƒˆë¡œìš´ ì‹ë‹¹ë“¤ë§Œ í¬ë¡¤ë§
            for i, url in enumerate(new_urls):
                try:
                    logger.info(f"ì‹ ê·œ ì‹ë‹¹ {i+1}/{len(new_urls)} í¬ë¡¤ë§: {url}")
                    
                    crawl_result = await crawler.crawl_restaurant_detail(url)
                    
                    if crawl_result.success:
                        restaurant_data = crawl_result.data
                        restaurant_name = restaurant_data.get('restaurant', {}).get('name', 'Unknown')
                        menu_count = len(restaurant_data.get('menus', []))
                        
                        logger.info(f"âœ… {restaurant_name}: {menu_count}ê°œ ë©”ë‰´")
                        all_results.append(restaurant_data)
                        crawled_urls.add(url)  # í¬ë¡¤ë§ ì™„ë£Œëœ URL ê¸°ë¡
                    else:
                        logger.error(f"âŒ ì‹ë‹¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_result.error}")
                        crawled_urls.add(url)  # ì‹¤íŒ¨í•œ URLë„ ê¸°ë¡í•´ì„œ ì¬ì‹œë„ ë°©ì§€
                    
                    # Rate limiting
                    await asyncio.sleep(1.0)
                    
                except Exception as e:
                    logger.error(f"ì‹ë‹¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ {url}: {e}")
                    crawled_urls.add(url)  # ì˜¤ë¥˜ URLë„ ê¸°ë¡
                    continue
            
            # í˜ì´ì§€ ê°„ ëŒ€ê¸°
            await asyncio.sleep(2.0)
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì™„ë£Œ: {len(all_results)}ê°œ ì‹ë‹¹ (ì´ {len(crawled_urls)}ê°œ URL ì²˜ë¦¬)")
    return all_results

async def main():
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìµœì¢… í¬ë¡¤ë§ ì‹œì‘")
    
    # í¬ë¡¤ëŸ¬ ë“±ë¡
    register_crawlers()
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = crawler_factory.create('diningcode')
    
    try:
        await crawler.initialize()
        
        # í¬ë¡¤ë§í•  í‚¤ì›Œë“œë“¤
        keywords = [
            "ê°•ë‚¨ì—­ ë§›ì§‘",
            "í™ëŒ€ ë§›ì§‘", 
            "ì´íƒœì› ë§›ì§‘",
            "ê±´ëŒ€ ë§›ì§‘",
            "ëª…ë™ ë§›ì§‘"
        ]
        
        all_results = []
        keyword_results = {}
        global_crawled_urls = set()  # ì „ì²´ í‚¤ì›Œë“œì—ì„œ ì´ë¯¸ í¬ë¡¤ë§í•œ URL ì¶”ì 
        
        for keyword in keywords:
            logger.info(f"\n{'='*60}")
            logger.info(f"í‚¤ì›Œë“œ: {keyword}")
            logger.info(f"{'='*60}")
            
            try:
                # í‚¤ì›Œë“œë³„ í¬ë¡¤ë§ ì‹¤í–‰
                results = await crawl_keyword_restaurants(crawler, keyword, max_pages=2)
                
                if results:
                    all_results.extend(results)
                    keyword_results[keyword] = results
                    
                    logger.info(f"âœ… '{keyword}': {len(results)}ê°œ ì‹ë‹¹ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.warning(f"âš ï¸ '{keyword}': ë°ì´í„° ì—†ìŒ")
                
            except Exception as e:
                logger.error(f"âŒ '{keyword}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        # ê²°ê³¼ ì €ì¥
        if all_results:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ì „ì²´ ê²°ê³¼ ì €ì¥
            all_results_file = Path(f"data/reports/crawl_all_results_{timestamp}.json")
            all_results_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(all_results_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ì „ì²´ ê²°ê³¼ ì €ì¥: {all_results_file}")
            
            # í‚¤ì›Œë“œë³„ ê²°ê³¼ ì €ì¥
            for keyword, results in keyword_results.items():
                if results:
                    keyword_safe = keyword.replace(' ', '_').replace('/', '_')
                    keyword_file = Path(f"data/reports/crawl_{keyword_safe}_{timestamp}.json")
                    
                    with open(keyword_file, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²°ê³¼ ì €ì¥: {keyword_file}")
            
            # í†µê³„ ì¶œë ¥
            logger.info(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ í†µê³„:")
            logger.info(f"  - ì „ì²´ ì‹ë‹¹ ìˆ˜: {len(all_results)}")
            
            total_menus = sum(len(result.get('menus', [])) for result in all_results)
            logger.info(f"  - ì „ì²´ ë©”ë‰´ ìˆ˜: {total_menus}")
            
            for keyword, results in keyword_results.items():
                if results:
                    menu_count = sum(len(result.get('menus', [])) for result in results)
                    logger.info(f"  - {keyword}: {len(results)}ê°œ ì‹ë‹¹, {menu_count}ê°œ ë©”ë‰´")
            
        else:
            logger.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await crawler.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
